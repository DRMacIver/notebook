# Virtues as game theoretic strategies

I was writing up something about meta-ethics and in the course of explaining it, two interesting things came together for me that felt worthy of writing up on their own.
So, guys, it's time for some game theory.

First, a side note: I don't regard game theory as particularly good as a descriptive tool for describing human behaviour. It is however a great source of *toy models* for reasoning about human behaviour. That is, behaviours that occur in the game theoretic models are suggestive of behaviours that might occur in real life. They are also good for testing hypotheses - e.g. if you posit that some sort of precondition is necessary for some behaviour, but the behaviour occurs in models without that precondition, that suggests a flaw in your reasoning.

I will be using game theory in this descriptive/illustrative sense. I'm not claiming that it has strong predictive value, but I do think it illuminates something important about the problem.

Also, I'm still working this all out. I don't have a tidy conclusion, and details (or indeed major features) may be wrong. That's why it's no the notebook.

Anyway, now for some theory of positive sum games.

Consider some interaction between you and me. This might be a trade, it might be collaborating on some project, it might be travelling together because there's safety in numbers... Anything really. The key feature is that if the interaction goes well, it's better for both of us than not having it. A trade is probably the simplest version of this, so I'll use that as my working example, but it's pretty general.

Now, the core problem with this interaction is that there is the possibility of *betrayal*. I might cheat you at business, you might take credit for my work, I might steal your stuff and run off into the night. Or it doesn't even have to be quite so overt - e.g. you might just not do your share of the work, I might fall asleep on watch, you might make bad decisions and lose all our money, etc.

(Some of these don't seem good for the betrayer, but I think in those cases they are *still better for the betrayer than not working together*, which is the crucial thing, even if they are not especially good for them).

When this happens it is significantly better for the betrayer than us working together equally, but worse for the betrayed than not working together.

Note that this is not the prisoner's dilemma, although it's relatively close. The important difference between it and the prisoner's dilemma is that we have the *opportunity not to participate*.

The best options for me, in order, are:

1. We trade, I betray you.
2. We trade, neither of us betrays the other.
3. We don't trade.
4. We trade, we both betray the other.
4. We trade, I behave honestly and you betray me.

So with this interaction I have three choices:

1. We trade, I am honest.
2. We trade, I betray you.
3. We don't trade.

And you similarly have three options. If either us pick "Don't trade", we get the neutral outcome of nothing happening, otherwise the trade happens and questions of betrayal come in.

Here's the important point which makes this a positive sum game: The world is *much* better for both of us if we can reliably pick the "We trade, neither of us betrays the other" option. This is a strictly preferable outcome to not trading. People are not good at functioning solo, and need to be able to rely on others.

But, in any given interaction, which option I choose is very dependent on how likely you are to betray me.

I'm not actually going to write out full pay off matrices, because it's complicated and mostly uninformative (and because I'm bad at game theory), but lets consider some simplified versions.

Suppose I am scrupulously honest and have no inclination to betray you. Let's say we each put in one unit of value, and get 10 units of value out. In the honest trade scenario, we split it evenly, if you betray me you take it all. So if \(p\) is your probability of betrayal, my expected return on trade is \(4 (1 - p) - p = 4 - 5p\). This is worth it if \(4 - 5p > 0\) i.e. if \(p < \frac{4}{5}\).

So in this case, because the cost of betrayal is relatively low and the profit from successful trade is relatively high, so unless you're very likely to betray me it's probably worth it. But of course, because the profit for you is also high, your incentive to betray me is quite high...

In fact, this intrinsically ends up eating all the possible gains from cooperation. If you look at the Nash equilibria, whenever you are sufficiently unlikely to betray me that it's worth my while to to trade, we are at a non-equilibrium state and you increase your expected profit by increasing your betrayal probability until I am indifferent between trading and not trading.

(Do people play at Nash equilibria in reality? No. But! There is an evolutionary game theory thing going on where each time people interact they modify their strategy based on what works, so gradually approach Nash equilibria over time. This is one of the reasons why game theory can be a useful modelling tool even if at any given point people are not behaving like perfectly rational strategic actors)

One classic solution to this in game theory is punishment. If I can credibly signal that if you betray me there will be terrible consequences for you, and vice versa, then the pay-off matrix changes to make betrayal worse than cooperation for the betraying party, and as a result we both cooperate. Thus the presence of punishment allows us to both have a better outcome, and punishment needs be applied rarely or never in order to achieve this - all that is required is that punishment *would* be reliably applied in the event of betrayal. Given such a setup, the equilibrium behaviour is that nobody ever betrays and nobody ever punishes.

There are a couple of problems with this strategy though.

1. I can only punish to the extent of my own capabilities, which if the incentive to screw me over is great enough may not be enough to change your behaviour (especially if you can screw me over badly enough to prevent the possibility of punishment).
2. Once I have been betrayed, punishment is typically purely a cost to me - I don't get any personal benefit from it, and I have to put effort into doing it. Thus I also have an incentive to defect on promised punishment.
3. I have to invest in the ability to detect betrayal in a timely manner and punish for it, which raises the cost of doing business.

An alternative strategy is to have the punishment be enforced by some external party - contracts, laws, reputation, etc. This improves the first point (because you can have a more powerful party mete out the punishment, which increases the disincentive to betray), sortof helps with the second (in that the external party can also provide incentives to enforce), and replaces the third one with a much more expensive one, because now you have to be able to demonstrate the betrayal to the third party, which requires a level of explicitness that you would not previously have needed, and creates its own risks of a he-said/she-said.

On top of this, it's just stressful and unpleasant to work with someone who you think would betray you at the drop of a hat if they thought they could get away with it, reducing the benefits from cooperation too.

The result is that this punishment strategy works, in that it unlocks cooperation, but it essentially creates a "transaction cost" - many situations where the gains from cooperation are marginal but positive aren't worth it under this regime, because it results in higher costs and lower benefits, and even the ones that are worth it go less well in contrast with the ideal version where betrayal isn't on the table at all.

What you ideally want, both informally and also in the game theoretic sense, is someone who doesn't betray you because they have no desire to betray you.

One way to think of this is as the punishment being internal. e.g. part of why I don't tend to cheat people on business deals is that I would feel *terrible*. This effectively creates a negative utility for me on the betrayal version which makes me less likely to betray. If that negative utility is larger than the gains for betrayal (i.e. the difference in profit between the option where I betray and the option where I don't), betrayal is no longer the best option for me, and thus you can count on me not to betray.

The same holds for other types of betrayal that aren't literally betrayal - e.g. letting someone down.

Side note: In a literal treachery example the ideal is that you want someone to have basically infinite guilt for stabbing you in the back, because this takes betrayal off the table entirely no matter how profitable the results are. In other examples where betrayal is more like letting the other party down it's mostly OK. In real examples often there's some degree of letting down where the correct response is actually "OK no, fair enough" because you tried your best and failed, or something important came up, where actually it's mostly correct to let me down. I don't have a good way of fitting it into the game theory without it getting messy, but I wanted to note its existence.

So, suppose you've learned virtue and have this new utility function that means that you don't betray me. What happens?

Well, all else being equal you get screwed over (sortof). If I've learned no such virtue, 
