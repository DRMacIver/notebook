# Thinking Outside Your Head

As per [notes on reduced output](https://notebook.drmaciver.com/posts/2020-07-26-10:40.html) I'm trying to do more book post prompts. Right now I think this will be *very* rough and ready stuff where I'm just gathering together some related thoughts.

From [Behind Human Error](https://amzn.to/2LZNHSF) by Woods et al., page 173:

> flexibility tends to create and proliferate modes of operation which create new cognitive demands on practitioners. Practitioners must know more - both about how the system works in each different mode and about how to manage the new set of options in different operational contexts.

Long ago I wrote [The No Genies Conjecture](https://notebook.drmaciver.com/posts/2018-10-31-16:54.html), in which I proposed that all systems have at most two of the following properties:

* Powerful
* Flexible
* Easy to use

This prompted a good conversation with [Chris Martens](https://twitter.com/chrisamaphone/status/1252591089900572673) about what "easy to use" meant, in that most experts presumably find their tools easy to use.
I think this is a good example of why that is not the case: As flexibility goes up, the amount of state that the expert has to manage in their head will usually exceed the capacity of their week fleshy game.

This is also the limitation of what I suggested in [learning to use the system](https://notebook.drmaciver.com/posts/2020-07-10-16:52.html), where I talked about how forming mental models is critical to using a system effectively.
In response to this pozorvlak pointed out how one of the features of good UI is to enable the formation of mental models.

In particular I thought [this point was particularly good](https://twitter.com/pozorvlak/status/1282994011440021505):

> good UIs foster a good mental model of the system, *split over the mind of the user and the information available in the world*.

The problem is that this split is rarely well facilitated, especially in systems designed for experts, and this gets only worse as the flexibility of the system grows.

One of the big problems that comes up a lot in "Behind Human Error" is the problem of *modality*. Modal systems are ones where you need different mental models for different states of the system. People commit mode errors when they use a mental model for a different mode.

This, I think, points to one of the difficulties of externalising parts of your mental model: what you're externalising it as is, itself, something you need a model for.

I don't know what to do about this, but evidence suggests neither do the people who work on such systems unfortunately.
