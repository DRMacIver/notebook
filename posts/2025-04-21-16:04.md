# What is probabilistic programming?

Friends and family often ask me what probabilistic programming is.^[This isn't weird, as I theoretically work in it, although that's a recent development and I work a bit on the fringes of it so what I do is not necessarily a very good example of probabilistic programming] I've not given very good answers, so here's an attempt to write a better one.

In the spirit of my previous "[How do LLMs work?](https://notebook.drmaciver.com/posts/2025-02-08-09:26.html)" this is a mostly nontechnical introduction, but that's a bit harder to do because a probabilistic programming language is inherently quite a technical object. I'm going to try to give you an idea of the problems they solve, and doing so is going to require writing a bit of pseudocode.

It might be helpful to read [Probably enough probability for you](https://notebook.drmaciver.com/posts/2021-10-29-09:43.html), although it's not strictly necessary.

OK. Let's start.

*Probability* is the practice of putting numbers on uncertainty. When you say something like "I'm 80% confident that the problem is in the engine" or "There is a 30% chance of rain this afternoon", that's probability. There is a lot of mathematics that goes on in trying to do probability well, and indeed to even define what it means to do probability well.

Probabilistic programming is an attempt to do probability well in cases where it's currently hard to work out a probability.

In order to understand how it does that, you'll need to understand a couple of core probability concepts:

Given some type of object, a *probability distribution* over those objects is anything that takes some subset of those objects and lets you answer probability questions about them. The easiest version of this is if you've got some finite collection and define the *uniform distribution* that just means every object is equally likely. So for example if you've got some listing of properties, you could say that a randomly picked property has probability 25% of having at least two bathrooms.

Not all distributions are uniform. For example, if you consider the distribution over the numbers 2 through 12 that you get when you roll two dice and add the numbers together, you'll assign more probability to rolling a total of 6 than a total of 1 because there are more ways to achieve that.

You can also have distributions over collections which are infinite or logically infinite. For example, you might have some distribution over train delays, and say that 95% of train arrivals are under 5 minutes late. Even though you only have finitely many trains, the set of possible train delays includes any number, and thus is logically infinite.^[And you want to represent it that way to avoid *overfitting*. If you assume that all future trains have a delay exactly equal to some train delay you've previously seen, you'll get very silly results.]

The basic problem we want to be able to answer when making predictions is how to get a probability distribution that:

1. Represents the real world predictions we want to make well.
2. We can efficiently make calculations with.

Both parts of this are hard.

Probabilistic programming starts from the observation that often what we want is just to be be able to simulate draws from the distribution with a *random sampler*. A random sampler is any process that you can run and get some arbitrary object from the collection back. For example, you can always pick the same object, or you can flip a coin and return its result, roll dice, etc.

A random sampler for a particular distribution is one where your best best on some property of the random sampler is to predict a probability according to that distribution. If the distribution says that a coin should turn up heads 70% of the time, a random sampler for heads or tails has that distribution if it produces heads about 70% of the time.^[This can be made precise, but I'm not gonna here.]

One important observation that underpins probabilistic programming (and many other areas) is that you can use samplers to implement what are called *Monte Carlo* methods. If you've got a sampler for some distribution, and you want to know the probability of some event, you just sample from it a large number of times and count how often it occurs. If you sample ten thousand examples from a population and have of them have some condition, you don't know the true probability exactly but you do know it's pretty close to 50%. If the event never happens at all, you can guess the ture probability is somewhere under one in ten thousand.

This means that if you can construct fast samplers reliably, you can get very good *approximate* answers to probabilistic questions about the distribution.

I think this is how weather forecasting works for example:^[I'm not actually at all sure of this, but it's a reasonable illustration of how it *could* work if it's not literally true.] You simulate out the next week or so according to your best weather models, and then if you want to know what's happening on Tuesday you look at how often your model had it rain on Tuesday.

The interesting thing about samplers for this purpose is that they are very easy to construct. A lot of my work in software testing is based on this - it's easy to construct random samplers that are pretty good at doing what you want.

For example, here's some pseudocode for constructing a sampler for the sum of two dice:

```
def roll_two_dice():
    return dice() + dice()
```

You can take primitive samplers, and then just combine values from them in arbitrary ways, just by writing normal functions in programming languages.

Unfortunately, what is *not* easy in general is constructing random samplers with specific distributions. If we've got some real world problem you want to represent, you can't just write a random sampler and expect it to work.^[Except by taking the data and just randomly picking from it, but that has a variety of problems.]

In particular, there's a really important case where there's no obvious way to construct a sampler for a given class of distribution, and *also* it's hard to get an exact distribution. This is the problem of *Bayesian inference*.

The basic idea of Bayesian inference is that you  
