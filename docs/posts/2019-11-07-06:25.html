<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>
DRMacIver's Notebook: Speeding Up Conditional Random Sampling
    </title>

    <meta property="og:title" content="Speeding Up Conditional Random Sampling">

    <meta property="og:url" content="https://notebook.drmaciver.com/posts/2019-11-07-06:25.html" />
    <link rel="canonical" href="https://notebook.drmaciver.com/posts/2019-11-07-06:25.html" />
    <script src="https://hypothes.is/embed.js" async></script>


    <meta name="twitter:card" content="summary" />

    <meta property="og:creator" content="@DRMacIver">

    <link rel="stylesheet" href="/pandoc.css"/>
    <link rel="stylesheet" href="/pygments.css"/>
    <link rel="stylesheet" href="/tufte.css"/>
    <link rel="stylesheet" href="/latex.css"/>
    <link rel="stylesheet" href="/drmnotes.css"/>
    <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/feed.xml" />

    <meta name="viewport" content="width=device-width, initial-scale=1">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['\\(', '\\)']]},
  multiLine: true,
  "HTML-CSS": { 
       linebreaks: { automatic: true }
  },
  SVG: { 
       linebreaks: { automatic: true } 
  }
});

MathJax.Hub.Register.MessageHook("Math Processing Error", function(message) {
  console.log(message)
});

</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-169185204-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-169185204-1');
</script>

  </head>

  <body>
    <article>
        <h1><a href="/">DRMacIver's Notebook</a></h1>
        <p class=subtitle>Speeding Up Conditional Random Sampling</p>

        

<section id="the-post">
<p class=subtitle>Speeding Up Conditional Random Sampling</p>
<dl class=metadata>
<dt>Published</dt>
<dd class="post-date">2019-11-07</dd>
</dl>

<p>This is an elaboration of an algorithm I described in an email to
John Regehr, when talking to him about <a href="https://blog.regehr.org/archives/1700">his recent blog post about
generative fuzzers</a> and its connection to Boltzmann sampling. It
currently has no implementation and so I can’t make any promises about
how well it works, but it’s interesting and I don’t currently have time
to work on it, so I wanted to write it down before the idea gets away
from me.</p>
<p>The basic problem I’m interested in here is this: Suppose you have
some random variable <span class="math inline">\(X\)</span> which is
implemented by making a series of non-deterministic binary choices (coin
flips), and some condition <span class="math inline">\(f(X)\)</span>,
and you want to sample from the conditional distribution <span class="math inline">\(X | f\)</span> - i.e. the subset of values of
<span class="math inline">\(X\)</span> for which <span class="math inline">\(f\)</span> holds, with probabilities proportional
to their usual probabilities for <span class="math inline">\(X\)</span>.</p>
<p>The connection to John’s problem of interest is that samplers based
on binary choices already have the property that the probability of
drawing any given sequence of choices <span class="math inline">\(s\)</span> depends only on its length, and is
proportional to <span class="math inline">\(2^{-|s|}\)</span>. Such
random variables are called <a href="http://algo.inria.fr/flajolet/Publications/DuFlLoSc04.pdf">Boltzmann
Samplers</a> with parameter <span class="math inline">\(\frac{1}{2}\)</span>. In particular this means
that the conditional distribution on any fixed size is already uniform,
so if you pick <span class="math inline">\(f\)</span> to be a size
constraint then it will naturally get something approximating what John
wants for the constrained generator.</p>
<p>The easy and normal way to do this is to do rejection sampling.
Generate independent copies <span class="math inline">\(X_1, \ldots,
X_i\)</span> until you find some <span class="math inline">\(i\)</span>
with <span class="math inline">\(f(X_i)\)</span> and return that. The
problem with this being that this takes on average <span class="math inline">\(\frac{1}{P(f(X)}\)</span> generator calls, which
if <span class="math inline">\(f\)</span> is hard to satisfy can be
<em>very</em> slow. I’ve been interested and idly thinking for a while
about how we can use the structure of random generation to speed it up.
I think the following algorithm does it:</p>
<p>In order to do this it helps to think of <span class="math inline">\(X\)</span> as a function <span class="math inline">\(c: \{0, 1\}^{\mathbb{n}} \to T\)</span> taking a
series of binary choices and turning them into a test case, with the
property that <span class="math inline">\(c(u)\)</span> only depends on
some finite initial prefix of <span class="math inline">\(u\)</span>
(and we can observe what that prefix is by instrumenting the random
generator). We can simulate <span class="math inline">\(X\)</span> by
lazily generating a uniform-at-random infinite binary sequence, but we
can also just as easily try <span class="math inline">\(c\)</span> on
any other binary sequence (this observation is also how test-case
reduction and a bunch of other things work in Hypothesis).</p>
<p>The algorithm works as follows: We maintain a list of <em>starts</em>
<span class="math inline">\(S = \{s_1, \ldots, s_n\}\)</span>. This list
is a set of finite bit strings in <span class="math inline">\(\{0,
1\}^{&lt;\omega}\)</span> with the following properties:</p>
<ol type="1">
<li>It is <em>prefix free</em> - that is if <span class="math inline">\(i \neq j\)</span> then <span class="math inline">\(s_i\)</span> does not start with <span class="math inline">\(s_j\)</span>.</li>
<li>If <span class="math inline">\(f(c(u))\)</span> for some <span class="math inline">\(u\)</span> then <span class="math inline">\(u\)</span> starts with some (necessarily unique)
<span class="math inline">\(s_i\)</span>.</li>
</ol>
<p>We will update this set as we learn more about the shape of the
problem. Initially we start with <span class="math inline">\(S\)</span>
containing just the empty string.</p>
<p>Suppose we have any such set <span class="math inline">\(S\)</span>.
The following algorithm perfectly simulates the conditional distribution
<span class="math inline">\(X|f\)</span>:</p>
<ol type="1">
<li>Pick <span class="math inline">\(s_i \in S\)</span> with probability
proportional to <span class="math inline">\(2^{-|s_i|}\)</span>.</li>
<li>Evaluate <span class="math inline">\(x = c(s_i u)\)</span> for some
uniformly chosen <span class="math inline">\(u \in \{0,
1\}^{\mathbb{n}}\)</span>.</li>
<li>If <span class="math inline">\(f(x)\)</span> then return <span class="math inline">\(x\)</span>, else go back to <span class="math inline">\(1\)</span> and try again.</li>
</ol>
<p>This is certainly true if we have the property that every <span class="math inline">\(u \in \{0, 1\}^{\mathbb{n}}\)</span> starts with
some <span class="math inline">\(s_i\)</span>, because we’re picking
<span class="math inline">\(s_i\)</span> with the probability that a
uniform at random string starts with it, and then we’re drawing the rest
uniformly at random, so our input sequence is a pure uniform at random
series of choices.</p>
<p>If <span class="math inline">\(S\)</span> does <em>not</em> have this
property, imagine enlarging it by adding strings until it does. Every
<span class="math inline">\(s\)</span> added has the property that no
extension of it will cause <span class="math inline">\(f(su)\)</span> to
be true (because by assumption such an <span class="math inline">\(su\)</span> would have to start with one of the
existing <span class="math inline">\(s_i\)</span>, which by
prefix-freeness is incompatible with starting with <span class="math inline">\(s\)</span>$. This means step 3 will always fail,
so the distribution is the same as the conditional distribution on
always choosing from one of our original <span class="math inline">\(s_i\)</span>.</p>
<p>This algorithm is a speedup only if the chosen set <span class="math inline">\(S\)</span> can prune a lot of possible prefixes.
This actually <em>does</em> sometimes help because if the tree has a lot
of shallow branches then we can evaluate them in full, determine whether
they lead to a desired value, and if not discard them, but this will
mostly not be the case. It does how motivate the next algorithm:</p>
<ol type="1">
<li>Pick <span class="math inline">\(s_i \in S\)</span> with probability
proportional to <span class="math inline">\(2^{-|s_i|} P(f(sU))\)</span>
where <span class="math inline">\(U\)</span> is a uniform at random
infinite bit sequence..</li>
<li>Evaluate the sequence <span class="math inline">\(x_i = c(s_i
u_i)\)</span> for some uniformly chosen <span class="math inline">\(u_i
\in \{0, 1\}^{\mathbb{n}}\)</span> until you find some <span class="math inline">\(x_i\)</span> with <span class="math inline">\(f(x_i)\)</span> being true, and return that.</li>
</ol>
<p>This is essentially the same algorithm as before, but with the
rejection sampling moved inside: We pick a prefix with probability
weighted to how likely it is to work, and then we run rejection sampling
until it works.</p>
<p>The easiest way for me to see that this works was to note that this
is a disjoint union of Boltzmann generators weighted in the right way,
but it’s probably easy to check it from elementary principles.</p>
<p>There are two big problems with this algorithm:</p>
<ol type="1">
<li>For many sets <span class="math inline">\(S\)</span> it’s no speedup
at all. In particular when <span class="math inline">\(S =
\{''\}\)</span> it’s literally just rejection sampling.</li>
<li>We have no way to calculate those probabilities.</li>
</ol>
<p>Fixing this is where the details of the algorithm get a bit fuzzy and
would require me to actually have an implementation to test and see how
it performs, as there are a bunch of tuning parameters and choices made
in the design. The basic idea is that we maintain a distribution of
beliefs about these probabilities, Bayesian style, and then use a <a href="https://en.wikipedia.org/wiki/Thompson_sampling">Thompson
Sampling</a> style approach to build our sampler with a suitable
explore/exploit tradeoff.</p>
<p>For each <span class="math inline">\(s_i\)</span> we maintain counts
of the number of times we’ve tried a sequence starting with it, <span class="math inline">\(n_i\)</span>, and the number of times that has
resulted in satisfying <span class="math inline">\(f\)</span>, <span class="math inline">\(g_i\)</span>. This gives us a posterior
distribution for <span class="math inline">\(P(f(sU))\)</span> of <span class="math inline">\(\beta(g_i + 1, n_i - g_i + 1)\)</span> (this would
not be true if our observations were not drawn uniformly at random but
the algorithm will proceed in a way that guarantees that’s always the
case). We can record all draws we perform in a <a href="https://en.wikipedia.org/wiki/Radix_tree">Patricia Trie</a> so
that these are easy to recalculate when we modify <span class="math inline">\(S\)</span>.</p>
<p>The algorithm now proceeds as follows:</p>
<ol type="1">
<li>For each <span class="math inline">\(s_i\)</span> draw a value <span class="math inline">\(q_i\)</span> from its posterior distribution.</li>
<li>Pick <span class="math inline">\(s_i\)</span> with probability
proportional to <span class="math inline">\(2^{-|s_i|}
q_i\)</span>.</li>
<li>Perform “some amount of” rejection sampling on <span class="math inline">\(s_i U\)</span> to attempt to find an <span class="math inline">\(X\)</span> satisfying <span class="math inline">\(X\)</span> (updating the statistics for <span class="math inline">\(s_i\)</span> as we do).</li>
<li>If the rejection sampling succeeds, return that. If not, possibly
<em>split</em> <span class="math inline">\(s_i\)</span>, setting <span class="math inline">\(S' = S \cup \{s_i 0, s_i 1\} \setminus
\{s_i\}\)</span> (possibly not adding any of these which define a
complete prefix for <span class="math inline">\(c\)</span> that doesn’t
satisfy <span class="math inline">\(f\)</span>), then go back to <span class="math inline">\(1\)</span>.</li>
</ol>
<p>The idea being that we sample according to what we would in some
configuration of our estimate of the true probabilities. As the
algorithm runs, we get more and more information about those
probabilities, so this algorithm should converge on our exact algorithm
above.</p>
<p>This leaves two questions:</p>
<ol type="1">
<li>How much rejection sampling to perform? Our estimate of the
probability might be wildly off, so we can’t rejection sample
indefinitely: We might even be at a prefix where the true probability is
zero!</li>
<li>When should we split a node?</li>
</ol>
<p>This is something that I think needs careful experimentation and
tuning and can’t be figured out from first principles, but my guess
is:</p>
<ol type="1">
<li>The number of rejection sampling loops should be roughly tuned to
how certain our estimate of the probability is. Something alone the
lines of updating the stats after drawing af ailure, drawing a new <span class="math inline">\(p\)</span> from the posterior, and terminating the
loop with probability <span class="math inline">\(1 - \max(1,
\frac{p}{q_i})\)</span> where <span class="math inline">\(q_i\)</span>
is our initially drawn estimate of the probability is likely to work
pretty well: If we start to think that it’s <em>really</em> unlikely
that we’re going to succeed because we’ve got a large number of failures
in a row, we’ll bail out, but if we knew we were an unlikely scenario
early on with fairly high certainty and thought it worth picking
anyway.</li>
<li>We should split fairly aggressively once we’ve seen at least one
success under a node, and fairly conservatively if we haven’t. There
might be something clever we can do with estimating the benefits of
splitting based on observed data, but my suspicion is that a dumb
heuristic to the tune of “Split a node once you’ve seen at least <span class="math inline">\(10\)</span> data points under it, at least one of
which has succeeded and one of which has failed” is probably likely to
be pretty good.</li>
</ol>
<p>Alternatively, here’s another interesting possibility for
<em>really</em> aggressive splitting: Every time we observe a success
<span class="math inline">\(f(c(u))\)</span>, find the <span class="math inline">\(s_i \in S\)</span> that starts <span class="math inline">\(u\)</span> and split it.</p>
<p>The reason why these heuristics are a bit uncertain is that they’re
essentially a quality/performance tradeoff: splitting will almost always
improve the quality of our generator (the “almost” qualifier is because
each child node will have less data gathered for it, so we will tend to
spend a little more time exploring them due to our uncertainty, which
may be useless), so the goal of the splitting heuristic is mostly going
to be to keep memory usage under control, and similarly running more
rejection samples will almost always give us a higher quality result,
but it will also cause us to be slower.</p>
<p>I don’t know if this idea will work well. It has the nice property
that it can’t really work <em>badly</em>, in that its failure modes are
essentially no worse than the status quo, but it may well be that
they’re also no better. My plan is to at some point (hopefully in the
next few weeks but realistically probably not) and see if I can use this
in Hypothesis. It’s particularly interesting for solving a problem that
has long been a bit of a sore point in Hypothesis, which is <em>size
control</em>.</p>
<p>QuickCheck has some special functionality for dealing with sizes of
examples. Examples in QuickCheck are generated with both a random seed
and a particular value of a size parameter. This is used to explore
small examples first and then gradually work up to big ones, which also
helps it with bounding the size of recursive subexamples and preventing
cases where they don’t terminate.</p>
<p>Hypothesis doesn’t do this. Instead it bounds the maximum length of
the choice sequence, effectively performing rejection sampling to keep
the size under control, and biases the distribution in ways that make it
more likely to fit within that cap. This has historically worked out
pretty OK, but it would be nice to be able to explicitly fuzz in smaller
sized regions first - it improves test performance and makes the
shrinker’s life easier.</p>

</section>

    </article>
<footer>
Copyright David R. MacIver.

CSS mostly due to <a href="https://edwardtufte.github.io/tufte-css/">Tufte CSS</a> by Dave Liepmann.
</footer>
  </body>
</html>
