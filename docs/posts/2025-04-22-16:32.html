<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>
DRMacIver's Notebook: Algebra and insight
    </title>

    <meta property="og:title" content="Algebra and insight">

    <meta property="og:url" content="https://notebook.drmaciver.com/posts/2025-04-22-16:32.html" />
    <link rel="canonical" href="https://notebook.drmaciver.com/posts/2025-04-22-16:32.html" />
    <script src="https://hypothes.is/embed.js" async></script>


    <meta name="twitter:card" content="summary" />

    <meta property="og:creator" content="@DRMacIver">

    <link rel="stylesheet" href="/pandoc.css"/>
    <link rel="stylesheet" href="/pygments.css"/>
    <link rel="stylesheet" href="/tufte.css"/>
    <link rel="stylesheet" href="/latex.css"/>
    <link rel="stylesheet" href="/drmnotes.css"/>
    <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/feed.xml" />

    <meta name="viewport" content="width=device-width, initial-scale=1">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['\\(', '\\)']]},
  multiLine: true,
  "HTML-CSS": { 
       linebreaks: { automatic: true }
  },
  SVG: { 
       linebreaks: { automatic: true } 
  }
});

MathJax.Hub.Register.MessageHook("Math Processing Error", function(message) {
  console.log(message)
});

</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-169185204-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-169185204-1');
</script>

  </head>

  <body>
    <article>
        <h1><a href="/">DRMacIver's Notebook</a></h1>
        <p class=subtitle>Algebra and insight</p>

        

<section id="the-post">
<p class=subtitle>Algebra and insight</p>
<dl class=metadata>
<dt>Published</dt>
<dd class="post-date">2025-04-22</dd>
</dl>

<p>I was reading up on Markov Chains today, as one does, partially
reminding myself about them and partly filling in some gaps in my
knowledge.</p>
<p>One bit I was reading about was reversible distributions. If you’ve
got some transition matrix <span class="math inline">\(P_{ij}\)</span>,
a reversible distribution for it is a probability distribution <span class="math inline">\(q\)</span> over the states such that <span class="math inline">\(q_i P_{ij} = q_j P_{ji}\)</span>.</p>
<p>It’s a theorem that every reversible distribution is stationary. That
is, if <span class="math inline">\(q\)</span> is reversible then <span class="math inline">\(q_i = \sum\limits_j P_{ji} q_j\)</span> for all
<span class="math inline">\(i\)</span> - the distribution is not changed
by running the Markov chain forwards.</p>
<p>The proof of this presented in the text I was reading is as
follows:</p>
<p>We can write <span class="math inline">\(q_i = q_i \sum\limits_j
P_{ij}\)</span> (because it’s a standard property of transition matrices
that this sum is 1), so <span class="math inline">\(q_i = \sum\limits_j
q_i P_{ij} = \sum\limits_j q_j P_{ji}\)</span>, so <span class="math inline">\(q\)</span> is stationary.</p>
<p>I can follow this proof of course. It’s very simple algebra. But
there’s something about it that feels like a magic trick. I don’t feel
like I’ve got any actual insight into the nature of reversible
distributions or have any sense of why they must be stationary from
this. I’m sure there are plenty of mathematicians who could, and I’m not
even sure that I <em>need</em> that insight, but it bugged me.</p>
<p>Thinking about it a bit more lead me to come up with the following
alternative proof<label class="margin-toggle sidenote-number" for="fn1"></label><input class="margin-toggle" id="fn1" type="checkbox"/><span class="sidenote">Which I suspect I have cribbed in some sort of
half-remembered manner from Geoffrey Grimmett’s “Probability on Graphs”,
but if so it was useful to recreate it from memory anyway.</span></p>
<p>One way to think about distributions on markov chains is that you’ve
got a certain amount of “stuff” (probability mass) floating around, and
at each step that you run the Markov chain, this probability mass flows
around the graph.</p>
<p>When you’ve got a probability distribution <span class="math inline">\(q\)</span>, all of <span class="math inline">\(q_i\)</span> flows out of node <span class="math inline">\(i\)</span>, and <span class="math inline">\(\sum\limits_j q_j P_{ji}\)</span> flows into it.
That is, for each node <span class="math inline">\(j\)</span>, the mass
in it flows out to every other node, and the fraction of it that goes to
<span class="math inline">\(i\)</span> is <span class="math inline">\(P_{ji}\)</span>.</p>
<p>A distribution is stationary if for all <span class="math inline">\(i\)</span>, <span class="math inline">\(q_i =
\sum\limits_j P_{ji} q_j\)</span>, but equally we can write this as
<span class="math inline">\(q_i - \sum\limits_j P_{ji} q_j  =
0\)</span>. The net flow of mass out of <span class="math inline">\(i\)</span> is 0.</p>
<p>We can write this flow of mass out of <span class="math inline">\(q_i\)</span> as <span class="math inline">\(q_i =
\sum\limits_j q_i P_{ij}\)</span> - the sum of the mass flowing to each
<span class="math inline">\(j\)</span>. So the net flow of mass out of
<span class="math inline">\(i\)</span> is <span class="math inline">\(\sum\limits_j P_{ij} q_i - P_{ji} q_j\)</span>.
That is, there is a net flow along each edge, let’s say <span class="math inline">\(f_{ij} = P_{ij} q_i - P_{ji} q_j\)</span>. The
probability of <span class="math inline">\(i\)</span> remains unchanged
if <span class="math inline">\(\sum\limits_j f_{ij} = 0\)</span>.</p>
<p>However, the reversability condition is precisely that all of the
<span class="math inline">\(f_{ij} = 0\)</span>. That is, there is no
net flow of mass along each edge. As a result, a reversible distribution
is necessarily stationary, because a much stronger condition applies:
Not only is the total net flow across the edges from <span class="math inline">\(i\)</span> equal to 0, it’s 0 along each edge.</p>
<p>This proof is in some sense much worse. It’s longer and involves more
algebra rather than less. But after reading this proof I felt like I
understood why reversibility obviously implied a distribution was
stationary, and also why it was a much stronger condition than being
stationary.</p>
<p>I think mathematics - and many other things - benefits hugely from
this sort of chewing on a problem and seeing it from different angles
until it actually makes sense. If you just know that a result is true,
you often find yourself unable to really use it unless you understand at
an intuitive level why it’s true. Proof without developing insight
alongside it is certainly better than no proof, but it’s in some sense
fundamentally fragile and untrustworthy, and it’s often better to mull
it over until you find the right way to break up the problem to actually
allow it to make sense at an emotional level.</p>


</section>

    </article>
<footer>
Copyright David R. MacIver.

CSS mostly due to <a href="https://edwardtufte.github.io/tufte-css/">Tufte CSS</a> by Dave Liepmann.
</footer>
  </body>
</html>
