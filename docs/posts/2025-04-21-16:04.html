<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>
DRMacIver's Notebook: What is probabilistic programming?
    </title>

    <meta property="og:title" content="What is probabilistic programming?">

    <meta property="og:url" content="https://notebook.drmaciver.com/posts/2025-04-21-16:04.html" />
    <link rel="canonical" href="https://notebook.drmaciver.com/posts/2025-04-21-16:04.html" />
    <script src="https://hypothes.is/embed.js" async></script>


    <meta name="twitter:card" content="summary" />

    <meta property="og:creator" content="@DRMacIver">

    <link rel="stylesheet" href="/pandoc.css"/>
    <link rel="stylesheet" href="/pygments.css"/>
    <link rel="stylesheet" href="/tufte.css"/>
    <link rel="stylesheet" href="/latex.css"/>
    <link rel="stylesheet" href="/drmnotes.css"/>
    <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/feed.xml" />

    <meta name="viewport" content="width=device-width, initial-scale=1">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['\\(', '\\)']]},
  multiLine: true,
  "HTML-CSS": { 
       linebreaks: { automatic: true }
  },
  SVG: { 
       linebreaks: { automatic: true } 
  }
});

MathJax.Hub.Register.MessageHook("Math Processing Error", function(message) {
  console.log(message)
});

</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-169185204-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-169185204-1');
</script>

  </head>

  <body>
    <article>
        <h1><a href="/">DRMacIver's Notebook</a></h1>
        <p class=subtitle>What is probabilistic programming?</p>

        

<section id="the-post">
<p class=subtitle>What is probabilistic programming?</p>
<dl class=metadata>
<dt>Published</dt>
<dd class="post-date">2025-04-21</dd>
</dl>

<p>Friends and family often ask me what probabilistic programming is.<label class="margin-toggle sidenote-number" for="fn1"></label><input class="margin-toggle" id="fn1" type="checkbox"/><span class="sidenote">This isn’t weird, as I theoretically work in it,
although that’s a recent development and I work a bit on the fringes of
it so what I do is not necessarily a very good example of probabilistic
programming</span> I’ve not given very good answers, so
here’s an attempt to write a better one.</p>
<p>In the spirit of my previous “<a href="https://notebook.drmaciver.com/posts/2025-02-08-09:26.html">How do
LLMs work?</a>” this is a mostly nontechnical introduction, but that’s a
bit harder to do because a probabilistic programming language is
inherently quite a technical object. I’m going to try to give you an
idea of the problems they solve, and doing so is going to require
writing a bit of pseudocode.</p>
<p>It might be helpful to read <a href="https://notebook.drmaciver.com/posts/2021-10-29-09:43.html">Probably
enough probability for you</a>, although it’s not strictly
necessary.</p>
<p>OK. Let’s start.</p>
<p><em>Probability</em> is the practice of putting numbers on
uncertainty. When you say something like “I’m 80% confident that the
problem is in the engine” or “There is a 30% chance of rain this
afternoon”, that’s probability. There is a lot of mathematics that goes
on in trying to do probability well, and indeed to even define what it
means to do probability well.</p>
<p>Probabilistic programming is an attempt to do probability well in
cases where it’s currently hard to work out a probability.</p>
<p>In order to understand how it does that, you’ll need to understand a
couple of core probability concepts:</p>
<p>Given some type of object, a <em>probability distribution</em> over
those objects is anything that takes some subset of those objects and
lets you answer probability questions about them. The easiest version of
this is if you’ve got some finite collection and define the <em>uniform
distribution</em> that just means every object is equally likely. So for
example if you’ve got some listing of properties, you could say that a
randomly picked property has probability 25% of having at least two
bathrooms.</p>
<p>Not all distributions are uniform. For example, if you consider the
distribution over the numbers 2 through 12 that you get when you roll
two dice and add the numbers together, you’ll assign more probability to
rolling a total of 6 than a total of 1 because there are more ways to
achieve that.</p>
<p>You can also have distributions over collections which are infinite
or logically infinite. For example, you might have some distribution
over train delays, and say that 95% of train arrivals are under 5
minutes late. Even though you only have finitely many trains, the set of
possible train delays includes any number, and thus is logically
infinite.<label class="margin-toggle sidenote-number" for="fn2"></label><input class="margin-toggle" id="fn2" type="checkbox"/><span class="sidenote">And you want to represent it that way to avoid
<em>overfitting</em>. If you assume that all future trains have a delay
exactly equal to some train delay you’ve previously seen, you’ll get
very silly results.</span></p>
<p>The basic problem we want to be able to answer when making
predictions is how to get a probability distribution that:</p>
<ol type="1">
<li>Represents the real world predictions we want to make well.</li>
<li>We can efficiently make calculations with.</li>
</ol>
<p>Both parts of this are hard.</p>
<p>Probabilistic programming starts from the observation that often what
we want is just to be be able to simulate draws from the distribution
with a <em>random sampler</em>. A random sampler is any process that you
can run and get some arbitrary object from the collection back. For
example, you can always pick the same object, or you can flip a coin and
return its result, roll dice, etc.</p>
<p>A random sampler for a particular distribution is one where your best
best on some property of the random sampler is to predict a probability
according to that distribution. If the distribution says that a coin
should turn up heads 70% of the time, a random sampler for heads or
tails has that distribution if it produces heads about 70% of the
time.<label class="margin-toggle sidenote-number" for="fn3"></label><input class="margin-toggle" id="fn3" type="checkbox"/><span class="sidenote">This can be made precise, but I’m not gonna here.</span></p>
<p>One important observation that underpins probabilistic programming
(and many other areas) is that you can use samplers to implement what
are called <em>Monte Carlo</em> methods. If you’ve got a sampler for
some distribution, and you want to know the probability of some event,
you just sample from it a large number of times and count how often it
occurs. If you sample ten thousand examples from a population and have
of them have some condition, you don’t know the true probability exactly
but you do know it’s pretty close to 50%. If the event never happens at
all, you can guess the ture probability is somewhere under one in ten
thousand.</p>
<p>This means that if you can construct fast samplers reliably, you can
get very good <em>approximate</em> answers to probabilistic questions
about the distribution.</p>
<p>I think this is how weather forecasting works for example:<label class="margin-toggle sidenote-number" for="fn4"></label><input class="margin-toggle" id="fn4" type="checkbox"/><span class="sidenote">I’m not actually at all sure of this, but it’s a
reasonable illustration of how it <em>could</em> work if it’s not
literally true.</span> You simulate out the next week or so
according to your best weather models, and then if you want to know
what’s happening on Tuesday you look at how often your model had it rain
on Tuesday.</p>
<p>The interesting thing about samplers for this purpose is that they
are very easy to construct. A lot of my work in software testing is
based on this - it’s easy to construct random samplers that are pretty
good at doing what you want.</p>
<p>For example, here’s some pseudocode for constructing a sampler for
the sum of two dice:</p>
<pre><code>def roll_two_dice():
    return dice() + dice()</code></pre>
<p>You can take primitive samplers, and then just combine values from
them in arbitrary ways, just by writing normal functions in programming
languages.</p>
<p>Unfortunately, what is <em>not</em> easy in general is constructing
random samplers with specific distributions. If we’ve got some real
world problem you want to represent, you can’t just write a random
sampler and expect it to work.<label class="margin-toggle sidenote-number" for="fn5"></label><input class="margin-toggle" id="fn5" type="checkbox"/><span class="sidenote">Except by taking the data and just randomly picking from
it, but that has a variety of problems.</span></p>
<p>In particular, there’s a really important case where there’s no
obvious way to construct a sampler for a given class of distribution,
and <em>also</em> it’s hard to get an exact distribution. This is the
problem of <em>Bayesian inference</em>.</p>
<p>The basic idea of Bayesian inference is that you</p>


</section>

    </article>
<footer>
Copyright David R. MacIver.

CSS mostly due to <a href="https://edwardtufte.github.io/tufte-css/">Tufte CSS</a> by Dave Liepmann.
</footer>
  </body>
</html>
