<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://notebook.drmaciver.com/</id>
  <title>DRMacIver's notebook</title>
  <updated>2021-06-07T08:42:30+01:00</updated>
  <author>
    <name>David R. MacIver</name>
    <email>david@drmaciver.com</email>
  </author>
  <link href="https://notebook.drmaciver.com" rel="alternate"/>
  <link href="https://notebook.drmaciver.com/feed.xml" rel="self"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-31-13:14.html</id>
    <title>Standardised Facts</title>
    <updated>2020-07-31T13:19:12+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Standardised Facts&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-31&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/2ZB2wmE"&gt;Seeing Like a State&lt;/a&gt; by James C. Scott, page 80:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;State simplifications have at least five characteristics that deserve emphasis. Most obviously, state simplifications are observations of only those aspects of social life that are of official interest. They are &lt;em&gt;interested&lt;/em&gt;, utilitarian facts. Second, they are also nearly always written (verbal or numerical) &lt;em&gt;documentary&lt;/em&gt; facts. Third, they are typically &lt;em&gt;static&lt;/em&gt; facts. Fourth, most stylized state facts are also &lt;em&gt;aggregate&lt;/em&gt; facts. Aggregate facts may be impersonal (the density of transportation networks) or simply a collection of facts about individuals (employment rates, literacy rates, residence patterns). Finally, for most purposes, stateofficials need to group citizens in ways that permit them to make a collective assessment. Facts that can be aggregated and presented as averages or distributions must therefore by &lt;em&gt;standardized&lt;/em&gt; facts. However unique the actual circumstances of the various individuals who make up the aggregate, it is their sameness or, more precisely, their differences along a standardized scale or continuum that are of interest.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;This is a very interesting passage that I think I probably just nodded and skimmed over when first reading this book. I find that often the random page approach is a good way of highlighting such passages.&lt;/p&gt;


&lt;p&gt;I don't have a huge amount to say about this passage right now (because I'm ill and it's hot and as a result I am quite stupid today) but I feel like it's especially relevant right now during the COVID-19 epidemic, where we're seeing a lot of this sort of thing. Death rates, hospitalisation rates, etc. are very much an example of standardised facts, and are eliding a lot of the weird and very unpleasant ways in which people's COVID-19 symptoms differ.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-31-13:14.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-08-01-15:39.html</id>
    <title>From a Certain Point of View</title>
    <updated>2020-08-01T16:03:44+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;From a Certain Point of View&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-08-01&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/3fiWsnC"&gt;Jonathan Strange &amp;amp; Mr Norrell&lt;/a&gt; by Susanna Clarke, page 480:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;"Artists are tricky fellows, sir, forever reshaping the world according to some design of their own," said Strange. "Indeed they are not unlike magicians in that."&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;A thing I often think about is the blurry line between true and useful, communication and performance.&lt;/p&gt;


&lt;p&gt;In the context of the story, this is about visual art, but the same happens with the written or spoken word: When we attempt to convey facts, we end up summarising, simplifying, focusing on what we think is important about it, and often when we do this we end up saying things that are not, in the strictest sense, true.&lt;/p&gt;


&lt;p&gt;(In context, Norrell is complaining about a painting that has a mirror where there is none).&lt;/p&gt;


&lt;p&gt;Sometimes this is because we elide crucial details. Sometimes it's a &lt;a href="https://en.wikipedia.org/wiki/Lie-to-children"&gt;lie-to-children&lt;/a&gt;. Sometimes it's to give the story the true aesthetic without the elaborate scaffolding needed to convey that aesthetic accurately (the mirror).
Sometimes, of course, it's just a lie.&lt;/p&gt;


&lt;p&gt;I think we end up doing this more than we think, because memory is more fallible than we like to treat it as, and we tend to think more in terms of aesthetic and narrative than detailed recollection of facts. In this sense we are all artists.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-08-01-15:39.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-08-07-12:27.html</id>
    <title>Ensuring Downward Paths</title>
    <updated>2020-08-07T15:41:49+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Ensuring Downward Paths&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-08-07&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I had some half-formed thoughts about test-case reduction that I wanted to work through, and this notebook is a place for working through half-formed thoughts, so here we go...&lt;/p&gt;


&lt;p&gt;The thing I've been working on on and off for about a month now is &lt;a href="https://notebook.drmaciver.com/posts/2020-07-05-11:29.html"&gt;using language inference to figure out new reduction passes&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;It can be thought of as a way of taking an idea that doesn't work (using L* for test case reduction) and combining it with an idea that works but doesn't generalise well (learning a table of small string substitutions to make),
and getting an idea that does work and does generalise.&lt;/p&gt;


&lt;p&gt;This has me thinking: What, exactly, does it mean for an approach to test-case reduction to generalise?&lt;/p&gt;


&lt;p&gt;Also &lt;em&gt;does&lt;/em&gt; this approach generalise well? The big question with it is basically whether it works well when you change the interestingness test. I think it will, but it probably depends a lot on the choice of interestingness test and how close your reducer is to normalising it already.&lt;/p&gt;


&lt;p&gt;In the approach to test-case reduction I favour, test-case reduction starts from set of test cases \(X\) and a total order \(\preceq\) over \(X\) called the &lt;em&gt;reduction order&lt;/em&gt;,
and a test-case reducer is a function \(r(U, x)\) where \(x \in U \subseteq X\), \(r(U, x) \in U\), and \(r(U, x) \preceq x\). For me usually \(X\) is a set of strings and \(\preceq\) is the shortlex order where \(x \preceq y\) if \(|x| &amp;lt; |y|\) or \(|x| = |y|\) and \(x \leq y\) lexicographically.&lt;/p&gt;


&lt;p&gt;Your prototypical test-case reducer starts from a bunch of small-step reductions, which gives you a function \(t: X \to X^{&amp;lt;\omega}\), where \(t(x)_i \prec x\),
and the reducer greedily tries each of \(t(x)_i\) in order. If none of them are in \(U\) it returns \(x\), otherwise it it recurses and returns \(r(U, t(x)_i)\) for the first \(i\) with \(t(x)_i \in U\).&lt;/p&gt;


&lt;p&gt;More generally given such a function we might say that a reducer is \(t\)-local if it satisfies the following conditions:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;\(t(U, x) = x\) if and only if \(t(x)_i \not\in U\) for all \(i\).&lt;/li&gt;
&lt;li&gt;For all \(U, x\) is some sequence \(x = x_0, \ldots, x_n = r(U, x)\) with \(x_{i + 1} \in t(x)_i\) (note that we don't necessarily require that \(x_i \in U\)).&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;That is, basically a \(t\)-local reducer is one that looks like the prototypical reducer but may have some optimisations and fiddly bits. e.g. delta debugging is a \(t\) local reducer where \(t\) is maps \(x\) to all test cases that can be formed by deleting a single element of \(x\). Delta debugging starts by trying large bulk combinations of these deletions and gradually narrows it down until it's only trying the small ones. Another approach which uses the same \(t\) but a different algorithm is what I've called &lt;a href="https://www.drmaciver.com/2017/06/adaptive-delta-debugging/"&gt;adaptive delta debugging&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;Most reducers implemented in practice are roughly \(t\)-local for some \(t\) (often they fail to be \(t\)-local in some cases because they hit some internal limit and stop when they could be making more progress, and rerunning the reducer may produce further redctions) and I think this is good because it allows you to reason in terms of what transformations they are capable of performing.&lt;/p&gt;


&lt;p&gt;So lets assume we have a \(t\)-local reducer. What makes it good?&lt;/p&gt;


&lt;p&gt;Basically the tradeoff is this:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;The larger \(|t(x)|\) tends to be the better you will reduce \(x\).&lt;/li&gt;
&lt;li&gt;The smaller \(|t(x)|\) is the better your reducer tends to perform.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The ideal seems to be that you want \(|t(x)| = O(|x|)\), as per delta debugging, and the trick is to tweak the set within that constraints to try to get good results.&lt;/p&gt;


&lt;p&gt;(Note that having \(|t(x)| = O(|x|)\) doesn't mean that the overall reducer runs in \(O(|x|)\) - even if you reduce the size at every step you can easily be made to make \(O(|x|^2)\) membership checks, and if you just make lexicographic transformations it's easy for performance to get exponentially bad)&lt;/p&gt;


&lt;p&gt;The problem that you run into is that basically what you really want is not just for \(|t(x)|\) to be large, but for it to contain values that are in some sense "likely to be in \(U\)". One way this can fail to happen is when there is some sort of precondition on the range of \(U\) we are interested in. The biggest example of this is when interesting test cases have to be syntactically valid according to some grammar. When this happens, effectively you are censoring \(t\) by replacing it with its syntactically valid subset. Often when something like delta debugging gets stuck it's not because it's not because of any intrinsic limitation related to the interestingness test, it's just ended up at a point where \(t(x)\) contains no syntactically valid test-cases.&lt;/p&gt;


&lt;p&gt;So, essentially what we want in order to get a good reducer for a format is to try to ensure that \(t(x)\) is always well populated with &lt;em&gt;syntactically valid&lt;/em&gt; reductions of \(x\). &lt;/p&gt;


&lt;p&gt;One way to do this might be to artificially construct interestingness tests that are basically designed to expose small reductions. I'm not quite sure what those would be. One thing I was thinking of is having interestingness tests of where \(x\) is interesting if \(y \preceq x\) for some fixed \(y\), then once those are normalised gradually "censor" the tests by removing all intermediate parts, stopping when you can't learn anything other than the direct jump to \(y\).&lt;/p&gt;


&lt;p&gt;This can be thought of as ensuring that whenever the final reduced result is \(y\) you have as rich a set of paths leading to it as the learning process is possibly able to find.&lt;/p&gt;


&lt;p&gt;I don't yet know how likely this is to work. I'm planning to do more experiments soon on getting this running on hypothesis-csmith and we'll see how well it can learn that, but I think there's some interesting stuff to investigate here.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-08-07-12:27.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-12-28-16:19.html</id>
    <title>Reducing Weird Tests</title>
    <updated>2020-12-30T02:43:45+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Reducing Weird Tests&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-12-28&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;Hillel recently wrote about &lt;a href="https://buttondown.email/hillelwayne/archive/fd1f0758-ae31-4e83-9138-33721cbd5ce3"&gt;cross-branch testing&lt;/a&gt; - comparing implementations of the same API in two different branches as a form of differential testing to see whether a change introduced any functional differences.
As part of his example he ended up with what was functionally the following interestingness test:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;He mentions in the article that this gives nondeterministic results for Hypothesis:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Running this gives us an erroneous input:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Falsifying example: test_f(a=5, b=6, c=5,)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is nondeterministic: Iâ€™ve also gotten &lt;code&gt;(0, 4, 0)&lt;/code&gt; and &lt;code&gt;(-114, -114, -26354)&lt;/code&gt; as error cases.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;&lt;code&gt;(0, 4, 0)&lt;/code&gt; is the (probably globally) correct answer of all of these according to Hypothesis's reduction logic,
but Hypothesis's test-case reduction is unable to reliably transform all of the failing examples to it.&lt;/p&gt;


&lt;p&gt;I reacted to this as I always do to such things: "Oh, I wonder if I can make the test-case reducer better". But the answer is no, no I cannot, and the reason is that this test contains what is basically a hash function (not a very good hash function, but good enough), and as a result the test cases are pretty much uniformly distributed throughout the set of test cases - if you generate test cases where the values are between &lt;code&gt;-10&lt;/code&gt; and &lt;code&gt;10&lt;/code&gt;, then of the &lt;code&gt;9261&lt;/code&gt; possible test cases, &lt;code&gt;129&lt;/code&gt; are interesting, i.e. about &lt;code&gt;1.4%&lt;/code&gt; which is about what you'd expect if the computation of &lt;code&gt;abs(out) % 100&lt;/code&gt; were roughly randomly distributed.&lt;/p&gt;


&lt;p&gt;For such functions we end up with the slightly awkward situation where:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;It's not that hard to find test cases (&lt;code&gt;1%&lt;/code&gt; isn't that infrequent! We can just exhaustively enumerate to find the globally minimal test case).&lt;/li&gt;
&lt;li&gt;It's hard enough to find test cases that most test-cases are locally minimal.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The reason for (2) is that if reductions are not any more likely to be interesting than any other test case (which seems roughly true for hillel's example and is true by construction for truly random examples), if you try \(N\) reductions you only expect \(0.01 N\) reductions to work. Suppose for a typical triple of integers we have, say, &lt;code&gt;10&lt;/code&gt; reductions we can perform, and those reductions are no more likely than chance to work, then &lt;code&gt;90%&lt;/code&gt; of interesting test cases are locally minimal, which means that we'll typically end the reduction process within a few steps.&lt;/p&gt;


&lt;p&gt;In order to explore this, lets consider the following toy model: Our space of interesting test cases are triples of integers between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;10&lt;/code&gt;, and we have a local reducer that makes the following transformations:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reduce_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# First try sorting the tuple&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;reduced&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Now try lowering each component&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;reduced&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# If we can raise the next component then try that&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(if this doesn't make sense to you don't worry about it too much, it's just the first local reducer that came to mind for triples of integers)&lt;/p&gt;


&lt;p&gt;Suppose we're reducing subject to an interesting test where the maximal value &lt;code&gt;(10, 10, 10)&lt;/code&gt; is always interesting and every other test case is interesting with probability &lt;code&gt;p&lt;/code&gt;. What is the probability that this local reducer finds the globally optimal test case?&lt;/p&gt;


&lt;p&gt;Based on some fairly crude simulations, the local reducer starts finding the globally optimal test-case more than half the time around &lt;code&gt;p = 0.93&lt;/code&gt;, which has the amusing property that the local reducer is reliably worse (in expectation) than the reducer that just always tries &lt;code&gt;(0, 0, 0)&lt;/code&gt; and gives up if that doesn't work (which finds the globally minimal test case with probability &lt;code&gt;p&lt;/code&gt;). Some equally crude simulation suggests that an approach that just enumerates the smallest test cases for a while returns at least as good a result in the same number of calls to the interestingness test in at least 80% of cases, regardless of the value of &lt;code&gt;p&lt;/code&gt;.&lt;/p&gt;


&lt;p&gt;I'm not sure if there's any sort of general lesson to derive here - most interestingness tests are not quite this adversarial - but I do feel like there is often something like this going on, where the reduction problem isn't that hard despite being in a local minimum, and I wonder if something loosely inspired by this that estimates the density of interesting test cases below the current interesting test case, but I'm not sure if that's entirely useful.&lt;/p&gt;


&lt;p&gt;If one &lt;em&gt;did&lt;/em&gt; have some sort of easy way to sample from test cases that are strictly smaller than the current interesting test case, it might be worth running that for longer than it would be intuitively obviously useful as a sort of crude but perhaps surprisingly effective reducer? It's certainly more robust to this sort of problem than any kind of local transformation based reducer would be. &lt;/p&gt;


&lt;p&gt;This also suggests that possibly test-case reducers should try a bit more bounded exploration of the small test cases than they currently do, but I think this is probably misleading due to the relatively small number of test cases in this example - for most formats of interest the number of test cases of size &lt;code&gt;n&lt;/code&gt; is exponential in &lt;code&gt;n&lt;/code&gt;, so any amount of enumeration other than trying the globally minimal test case (which &lt;em&gt;is&lt;/em&gt; usually worth doing) is probably not worth it.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-12-28-16:19.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-01-01-11:22.html</id>
    <title>Basic Reducer Design</title>
    <updated>2021-01-01T12:10:49+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Basic Reducer Design&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-01-01&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;There's a design of test-case reducer that I use and consider the natural way to do test-case reducers,
but seems to be mildly idiosyncratic to me (though most of the details of it are found in other test-case reducers),
which is basically this:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;Test cases are given some total order, the &lt;em&gt;reduction order&lt;/em&gt;, and the goal of test-case reduction is to (ideally) find the minimal interesting test case in that reduction order.&lt;/li&gt;
&lt;li&gt;Test-case reduction is managed by a stateful object that automatically simplifies the book keeping of doing that.&lt;/li&gt;
&lt;li&gt;Test-case reduction consists of a sequence of passes that are run until none of them lowers the current most interesting test case.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;(the fact that there is a total reduction order plus some of the book keeping are the bits that seem not totally standard)&lt;/p&gt;


&lt;p&gt;The reduction order lets us keep track of the current best test case and this significantly simplifies keeping track of the progress of the reducer.
This can all be encapsulated as the following Python code:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Reducer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;initial_test_case&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;reduction_order&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;reduction_passes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;initial_test_case&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__is_interesting&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;is_interesting&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduction_order&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reduction_order&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduction_passes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reduction_passes&lt;/span&gt;

        &lt;span class="c1"&gt;# We cache the result of the interestingness test - this is pretty&lt;/span&gt;
        &lt;span class="c1"&gt;# standard in test-case reduction.&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__cache&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial_test_case&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Cannot reduce uninteresting initial test case"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_case&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__cache&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_case&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;KeyError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;pass&lt;/span&gt;

        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__cache&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_case&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

        &lt;span class="c1"&gt;# If this is a smaller interesting test case than our current best&lt;/span&gt;
        &lt;span class="c1"&gt;# known one, automatically update.&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduction_order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduction_order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_case&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;prev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;prev&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;
            &lt;span class="c1"&gt;# Each reduction pass calls self.is_interesting in order to do&lt;/span&gt;
            &lt;span class="c1"&gt;# any work, so we do not need to keep track of whether they&lt;/span&gt;
            &lt;span class="c1"&gt;# succeeded or failed.&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduction_passes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;reducer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Reducer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;reducer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reducer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A reduction pass in this framework looks something like the following (this is a greedy algorithm for deleting elements from a list or other sequence, like delta debugging without the delta step):&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This design has a lot of flexibility to it. For example, it makes arbitrary reduction passes into an anytime algorithm, because you always know the current best test case,
and so you can design it work with arbitrary interruptions.
e.g. If you wanted to impose a limit on the maximum number of calls you could do it by modifying the reducer as follows:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;StopReducing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Reducer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;max_calls&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;
    &lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_calls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_calls&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_case&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call_count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call_count&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_calls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;StopReducing&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;StopReducing&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(People will, at this point, complain that I am using exceptions for control flow. Yes I am. What of it?)&lt;/p&gt;


&lt;p&gt;One downside to this design is that it doesn't play that well with multithreading. This isn't a major problem in Python, so I haven't spent much time working on it, but roughly there are two obvious things you can do with this design: You can make the best test case thread local and then merge by taking the minimum when you join threads, or you can make your reduction passes single-threaded but use speculative concurrency to speed them up. This latter approach is roughly what reducers like &lt;a href="https://github.com/googleprojectzero/halfempty"&gt;halfempty&lt;/a&gt;. I've experimented a bit with this in some reducer prototypes, but not in Hypothesis where it's not especially useful because Python.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-01-01-11:22.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-01-11-06:13.html</id>
    <title>Representing Steps in Test-Case Reduction</title>
    <updated>2021-01-11T07:50:20+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Representing Steps in Test-Case Reduction&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-01-11&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;In &lt;a href="https://notebook.drmaciver.com/posts/2021-01-01-11:22.html"&gt;my last post&lt;/a&gt; on reducer design I included the following deletion algorithm:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This iterates forward through a sequence and tries deleting each element of it in turn to see if it's still interesting.
It's a foundation for a lot of useful test-case reduction passes.&lt;/p&gt;


&lt;p&gt;It's also in the wrong order. Really for most problems you probably want to write it as follows:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# We're only calling this for the side effect of updating self.best_test_case&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reason for this order is that later elements often depend on early elements (e.g. if a variable declared earlier is used later),
so this often does less wasted work because it takes fewer passes to get to a fixed point - going forward, you need to rerun the entire pass to delete the earlier bits that have been enabled by the later bits.&lt;/p&gt;


&lt;p&gt;One problem with either of these is that they can lead to long stalls, where there's a large undeletable region and you're slowly going through all of it.
This is particularly annoying because often if you'd made the test case much smaller before checking all of those bits it would be much faster to go through them,
because the interestingness test is faster on smaller test cases.&lt;/p&gt;


&lt;p&gt;In order to avoid these stalls it might be better to write it like this:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pop_random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Might no longer be true because we've successfully reduced&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="c1"&gt;# We're only calling this for the side effect of updating self.best_test_case&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This iterates over the test case in a random order, avoiding stalls as a result - if a fraction \(p\) of the test case is deletable this will only stall for around \(\frac{1}{p}\) steps, while either deterministic version can potentially stall for \(O(n)\) calls.&lt;/p&gt;


&lt;p&gt;(this isn't a very good way of doing randomisation, and all of these are improved by &lt;a href="https://www.drmaciver.com/2017/06/adaptive-delta-debugging/"&gt;adaptive delta debugging&lt;/a&gt; of course, but we'll ignore that for now).&lt;/p&gt;


&lt;p&gt;The problem of course is that all of these are basically the same algorithm with a lot of code duplication between them, and we would like to be able to flexibly swap between modes based on what's working.&lt;/p&gt;


&lt;p&gt;Another related problem is that often we want to get an idea of how good a pass is before running it. If we were to e.g. run the random one for ten steps and see if it did anything before we decide whether to run the full thing, that would help a lot with the pass selection problem (how we know which reduction pass we should run right now).&lt;/p&gt;


&lt;p&gt;This example also makes it seem easier than it is. Consider the following reduction pass:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;put_in_order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This tries swapping all nearby out of order pairs in order to make the test case more sorted (which is a reduction lexicographically).
Here all the same considerations potentially apply, but there are two indices that vary instead of one.
The question of what order to try things in is also even more complicated, as potentially this one might need &lt;em&gt;many&lt;/em&gt; passes to reach convergence (there are \(n!\) possible permutations of a sequence so I think this has the potential to take \(O(n!)\) calls to complete given an adversarial interestingness test).&lt;/p&gt;


&lt;p&gt;All of this is problem set up as a preamble for telling you about how we solve the problem in Hypothesis, which is to consider a reduction pass as making a series of nondeterministic choices,
which it must exhaustively try all of before reduction is considered to have finished.
The API for making these choices looks as follows:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DeadBranch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Raised when a reduction pass is unable to continue from&lt;/span&gt;
&lt;span class="sd"&gt;    the current point."""&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Chooser&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""A source of nondeterminism for use in reduction passes."""&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;choose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;condition&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""Returns a value ``x`` in ``values`` such that&lt;/span&gt;
&lt;span class="sd"&gt;        ``condition(x)`` is True, possibly raising ``DeadBranch``&lt;/span&gt;
&lt;span class="sd"&gt;        if there are no such values, or if it turns out we have&lt;/span&gt;
&lt;span class="sd"&gt;        already exhausted all remaining values. """&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reduction passes above now look as follows:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;put_in_order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="c1"&gt;# NB: Possible no such j exists in which case this raises.&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reducer is now free to try these in any order it likes, without the reduction passes overspecifying it.&lt;/p&gt;


&lt;p&gt;Currently we try two different orders in different contexts:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;Uniformly at random.&lt;/li&gt;
&lt;li&gt;Iterating backwards in reverse lexicographic order.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Switching between the two according some frankly fairly arbitrary internal heuristics that seem to work pretty well but are clearly suboptimal.&lt;/p&gt;


&lt;p&gt;The thing that's worth noting however is the representation. You can see it in &lt;a href="https://github.com/HypothesisWorks/hypothesis/blob/master/hypothesis-python/src/hypothesis/internal/conjecture/choicetree.py"&gt;the relevant Python file&lt;/a&gt;, but the important features of it are basically:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;We maintain a rose tree, lazily built as we walk it, where each node has &lt;code&gt;len(values)&lt;/code&gt; children, &lt;code&gt;values&lt;/code&gt; being the collection of choices made at that point. Each node also maintains a count of how many "live" (that is, not fully explored) children it has.&lt;/li&gt;
&lt;li&gt;When we complete running a pass step, we mark the final node as dead, decrement the count on its parent and if that is zero also mark it as dead and do the same with its parent etc.&lt;/li&gt;
&lt;li&gt;Every time we successfully reduce the current target, we delete the tree and start over with a fresh one, but for deterministic iteration start from the same point in the tree (to the greatest degree possible)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Essentially we are representing a step in a reduction pass as a sequence of integers representing nondeterministic choices,
and stopping when the full tree has been explored for the given shrink target.
As well as turning out to be quite a convenient representation, this is pleasingly similar to &lt;a href="https://2020.ecoop.org/details/ecoop-2020-papers/13/Test-Case-Reduction-via-Test-Case-Generation-Insights-From-the-Hypothesis-Reducer"&gt;how Hypothesis represents test-case generation&lt;/a&gt;.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-01-11-06:13.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-01-14-14:00.html</id>
    <title>Rejection Sampling in Hypothesis</title>
    <updated>2021-01-14T14:48:06+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Rejection Sampling in Hypothesis&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-01-14&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;(This post assumes you know roughly how Hypothesis does test-case reduction and generation. See &lt;a href="https://2020.ecoop.org/details/ecoop-2020-papers/13/Test-Case-Reduction-via-Test-Case-Generation-Insights-From-the-Hypothesis-Reducer"&gt;our ECOOP paper&lt;/a&gt; for a high level explanation)&lt;/p&gt;


&lt;p&gt;Suppose you have some generator &lt;code&gt;base&lt;/code&gt; and you want to sample from it conditionally.
In Hypotheis you can do this with &lt;code&gt;base.filter(condition)&lt;/code&gt;.&lt;/p&gt;


&lt;p&gt;Conceptually, this does &lt;em&gt;rejection sampling&lt;/em&gt;, which works roughly like this:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Filtered&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;base&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;condition&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__base&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;base&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__condition&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;condition&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;do_draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__base&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__condition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is, we're attempting to draw from the conditional distribution of values that come from &lt;code&gt;base&lt;/code&gt; that satisfy &lt;code&gt;condition&lt;/code&gt; and we do this by repeatedly drawing values from it until one satisfies the condition.&lt;/p&gt;


&lt;p&gt;This has a couple of problems:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;The underlying choice sequence contains a lot of probably useless garbage - we made a bunch of choices in each iteration of the loop when drawing from the base (and possibly even when calling the condition! This is allowed), and these are probably all irrelevant.&lt;/li&gt;
&lt;li&gt;These loops are potentially very long, especially if the condition is hard or impossible to satisfy.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;We adopt the following heuristic approach to deal with the second problem:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Filtered&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;do_draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__base&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__condition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mark_invalid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is, we terminate all rejection sampling at three iterations - it may well be possible to proceed from there, but if so we have a chance at trying again at the next generation, and spending longer seems largely not worth it.&lt;/p&gt;


&lt;p&gt;(Why three you ask? Well, let me ask you a better question: Why not three?)&lt;/p&gt;


&lt;p&gt;(That is to say, no good reason. It's a fairly arbitrarily chosen number that seems to work OK)&lt;/p&gt;


&lt;p&gt;The first problem we have a slightly more principled way to deal with: We want to be able to reduce the choice sequence by removing all the redundant draws.&lt;/p&gt;


&lt;p&gt;(This is made &lt;em&gt;slightly&lt;/em&gt; more complicated by the fact that the draws might not be redundant - Hypothesis generators are allowed to have side effects, such as inserting rows into a database, so it's possible for a filtered generator to do important work in the initial loops).&lt;/p&gt;


&lt;p&gt;Assuming for the moment that &lt;code&gt;condition&lt;/code&gt; does not make any choices (which it mostly won't, and we're not that fussed about producing worse results if it does) this is actually relatively easy:
The boundaries of the &lt;code&gt;draw&lt;/code&gt; call are marked on the choice sequence, and the reducer can try deleting that region of the choice sequence. This allows it to delete the early iterations of the loop,
resulting in an effect where in a reduced choice sequence we just get implausibly lucky and always satisfy the condition first time.&lt;/p&gt;


&lt;p&gt;We can, and do (although &lt;a href="https://github.com/HypothesisWorks/hypothesis/pull/2757"&gt;it turns out this got silently broken at some point and I fixed it as a result of writing this blog post&lt;/a&gt;), speed this up further by adding special boundary markers that indicate that the region is probably irrelevant.
This looks like the following:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Filtered&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;do_draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start_example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILTER_LABEL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__base&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__condition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_example&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;
            &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;discard&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mark_invalid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reason this helps is that we can gather together all regions of the choice sequence that have been marked as discarded and attempt to delete them simultaneously.
This will, usually, work. In particular it allows us to make all filtered generators implausibly lucky simultaneously with one call.&lt;/p&gt;


&lt;p&gt;This kind of annotation of the choice sequence with useful information about common patterns in generation is often quite helpful for Hypothesis's test-case reduction. I think a lot of it is not that necessary any more, but this distinction between used and discarded regions seems to help a great deal and often allows for some significant performance improvements during reduction.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-01-14-14:00.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-01-19-11:41.html</id>
    <title>Some Theory of Test-Case Reduction</title>
    <updated>2021-01-19T14:39:06+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Some Theory of Test-Case Reduction&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-01-19&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;There's a bunch of test-case reduction theory that mostly lives in my head and currently-abandoned paper drafts.
I'm doing a week of PhD related writing so I thought it would be good to write up a dump of these ideas here.&lt;/p&gt;


&lt;p&gt;A test-case reducer is something that is designed to take an interesting test case and make it "simpler" in some sense while retaining the property of interest.&lt;/p&gt;


&lt;p&gt;Formally, a test-case reducer on some set of test-cases \(T\) is a function \(\{r : \{(U, x) : x \in U \subseteq T\} \to T\) such that \(r(U, x) \in U\). The set \(\{(U, x) : x \in U \subseteq T\}\) is called the set of reduction problems on \(T\).&lt;/p&gt;


&lt;p&gt;The first argument to the reducer is the interestingness test (the set of interesting test cases) and the second is some interesting test case that we want to reduce.&lt;/p&gt;


&lt;p&gt;A partial order \(\preceq\) is a reduction order for \(r\) (or \(r\) is a reducer for \(\preceq\)) if \(r(U, x) \preceq x\).
The reduction order captures what we mean by saying that a reducer makes the test case "simpler". A test case is simpler if it's smaller in the reduction order.&lt;/p&gt;


&lt;p&gt;(It's hopefully faily uncontroversial that the "simpler" relationship should be a partial order - it's basically to avoid cycles where sometimes \(x\) reduces to \(y\) and sometimes \(y\) reduces to \(x\) depending on the interestingness test. It's easy to construct reducers that do have these cycles, so not every reducer has a reduction order, but generally I think those reducers are badly designed)&lt;/p&gt;


&lt;p&gt;As I mentioned in &lt;a href="https://notebook.drmaciver.com/posts/2021-01-01-11:22.html"&gt;Basic Reducer Design&lt;/a&gt; I generally prefer the reduction order to be a total order.
There's no reason it has to be, but there's also no reason for it not to be - any partial order can be extended to a total order, and if a function is a reducer for some partial order then it's a reducer for any extension of it.&lt;/p&gt;


&lt;p&gt;One reason to prefer a total order is that if we want a test-case reducer to be normalizing (in the sense Groce et al. propose in &lt;a href="https://www.cefns.nau.edu/~adg326/issta17.pdf"&gt;One test to rule them all&lt;/a&gt;, that is its output does not depend on the initial test case) then it must be a total order because for any \(x, y\) we must have \(r(\{x, y\}, \cdot) = y\) or \(r(\{x, y\}, \cdot) = x\), so either \(x \preceq y\) or \(y \preceq x\).&lt;/p&gt;


&lt;p&gt;One nice thing about reduction orders is that they guarantee that reducers compose nicely without undoing each other's work. Given reducers \(r_1, r_2\) we can define a composite reducer straightforwardly as \((r_1 \cdot r_2)(U, x) = r_1(U, r_2(U, x))\) - we apply \(r_2\) then we apply \(r_1\).  If \(\preceq\) is a reduction order for both of \(r_1\) and \(r_2\) then it also is for \(r_1 \cdot r_2\).&lt;/p&gt;


&lt;p&gt;Without that common reduction order, you might end up in an annoying situation where \(r_2(U, x) = y\) and \(r_1(U, y) = x\). Note that you can have this even if \(r_1\) and \(r_2\) each have a reduction order but it's not the same one.&lt;/p&gt;


&lt;p&gt;One thing to consider is when a reducer is "done". We say a reduction problem \((U, x)\) is fully reduced for \(r\) if \(r(U, x) = x\),
and a reducer is idempotent if \((U, r(U, x))\) is fully reduced - i.e. running the reducer twice doesn't do any more than running the reducer once.&lt;/p&gt;


&lt;p&gt;Another nice feature of reduction orders is that they guarantee that you can always iterate a reducer to an idempotent fixed point (assuming the reduction order is reasonable. The technical condition we require here is &lt;a href="https://en.wikipedia.org/wiki/Well-founded_relation"&gt;well-foundedness&lt;/a&gt; but in basically every case of interest the order has the much stronger property that \(\{y: y \preceq x\}\) is finite for any \(x\)). Given a reducer \(r\) you can define the reducer \(r^+\) where \(r^+(U, x)\) is defined by iterating the sequence \(x_0 = x\), \(x_{n + 1} = r(U, x_n)\) until you reach a fixed point. Without a reduction order you can get cycles in this sequence so it may never terminate.&lt;/p&gt;


&lt;p&gt;You can also think of \(r^+\) as the fixed point of \(r\) under composition. i.e. \(r^+ = r \cdot r^+\), so we call \(r^+\) the &lt;em&gt;fixation&lt;/em&gt; of \(r\).&lt;/p&gt;


&lt;p&gt;By the way, these two constructs of reducer composition and fixation are not particularly abstract but are actually how reducers are built in practice.
You can think of your typical reducer as \((r_1 \cdot \ldots \cdot r_n\)^+) where the \(r_i\) are individual simpler reducers that are typically called &lt;em&gt;reduction passes&lt;/em&gt;.&lt;/p&gt;


&lt;p&gt;Generally speaking for a reducer we're most interested in the behaviour of \(r^+\) - what happens when you run the reducer to fixation - but it's often useful to consider reducers that are not necessarily run to fixation because of this pass structure. Anecdotally (and for solid heuristic reasons) it's generally better &lt;em&gt;not&lt;/em&gt; to run each pass to fixation before running the next one:
After running a reducer once it's typically "pretty close" to fixation, and running it again is likely to be wasted effort, while if other passes have run it may have been perturbed enough that more work is useful.&lt;/p&gt;


&lt;p&gt;Reasoning in terms of the fully reduced problems of a reducer also gives us another useful notion, which is that of reducer strength. We can say \(r_1\) is at least as strong as \(r_2\) if every reduction problem that is fully reduced for \(r_1\) is also fully reduced for \(r_2\) (and stronger if also there are some reduction problems that are fully reduced for \(r_2\) that are not for \(r_1\)).&lt;/p&gt;


&lt;p&gt;This is a slightly weaker notion of strength than some we might consider. It's possible for \(r_1\) to be stronger than \(r_2\) but to sometimes have \(r_2(U, x) \prec r_1(U, x)\), because \(r_1\) took a path that causes it to get stuck at a worse point.&lt;/p&gt;


&lt;p&gt;Under this notion of strength, \(r_1 \cdot r_2\) is at least as strong as each of \(r_1\) and \(r_2\), and \(r^+\) is equally strong to \(r\).&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-01-19-11:41.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-04-19-10:16.html</id>
    <title>Remastery Training</title>
    <updated>2021-04-19T11:10:12+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Remastery Training&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-04-19&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I play a lot of Slay the Spire.&lt;/p&gt;


&lt;p&gt;Slay the Spire is a game where you build up a deck of cards, which you use to fight a series of battles of varying difficulty.&lt;/p&gt;


&lt;p&gt;You ascend through the levels of the spire, until at the end of the game you attempt to defeat its heart.&lt;/p&gt;


&lt;p&gt;The game is divided into four acts, at the end of each of which you fight a boss.&lt;/p&gt;


&lt;p&gt;The heart is the boss of the fourth act.&lt;/p&gt;


&lt;p&gt;To get to Act 4, you must collect three keys before the end of Act 3.&lt;/p&gt;


&lt;p&gt;If you do not, the game ends early, with "Victory?" instead of "Victory"&lt;/p&gt;


&lt;p&gt;Slay the Spire has four characters, Ironclad, Silent, Defect, and Watcher.&lt;/p&gt;


&lt;p&gt;My preference goes Defect, Ironclad, Silent, Watcher, from favourite to least favourite.&lt;/p&gt;


&lt;p&gt;Slay the Spire has a series of difficulties called "Ascension levels".&lt;/p&gt;


&lt;p&gt;Ascension levels go from 0 to 20.&lt;/p&gt;


&lt;p&gt;You start at Ascension 0, and every time you complete Act 3 on an ascension level, you unlock the next one for that character.&lt;/p&gt;


&lt;p&gt;I would like to be able to defeat the heart on Ascension 20 with each character.&lt;/p&gt;


&lt;p&gt;So far I have managed this only with Defect.&lt;/p&gt;


&lt;p&gt;I am currently working on defeating the heart on Ascension 20 with Ironclad.&lt;/p&gt;


&lt;p&gt;My current, more modest, goal is to defeat Act 3, but so far I have not even been able to manage that.&lt;/p&gt;


&lt;p&gt;This is somewhat frustrating.&lt;/p&gt;


&lt;p&gt;I've tried quite a few times at this point.&lt;/p&gt;


&lt;p&gt;Probably not more than 10, but still.&lt;/p&gt;


&lt;p&gt;The problem is that I am no longer getting better at it, at least not very quickly.&lt;/p&gt;


&lt;p&gt;I have identified a strategy that would likely allow me to beat the heart, but I am very bad at it.&lt;/p&gt;


&lt;p&gt;To tell you about this strategy, I must first tell you about relics.&lt;/p&gt;


&lt;p&gt;Relics are items you acquire as you ascend the spire, which modify your abilities.&lt;/p&gt;


&lt;p&gt;Each character has a starter relic, unique to them.&lt;/p&gt;


&lt;p&gt;The Ironclad's starter relic is called "burning blood", and it heals you slightly at the end of each fight.&lt;/p&gt;


&lt;p&gt;One way to think about this is that it makes the game slightly easier, without making your character more powerful.&lt;/p&gt;


&lt;p&gt;This is not entirely right, but it's pretty close.&lt;/p&gt;


&lt;p&gt;At the beginning of the game, you encounter Neow, who offers you a blessing.&lt;/p&gt;


&lt;p&gt;One of the blessings you can get is to swap your starter relic for a random boss relic.&lt;/p&gt;


&lt;p&gt;Boss relics are very different from the Ironclad starter relic, in that most of them make the game harder while also making your character more powerful.&lt;/p&gt;


&lt;p&gt;Anyway, I noticed that I was never taking boss relic swap, because I liked the Ironclad starter relic too much.&lt;/p&gt;


&lt;p&gt;This suggested that my way through Ascension 20 might be to learn to boss relic swap on Ironclad.&lt;/p&gt;


&lt;p&gt;I tried this, and I died badly every time, because Ascension 20 is too hard for me to experiment on.&lt;/p&gt;


&lt;p&gt;So I went back down to Ascension 0.&lt;/p&gt;


&lt;p&gt;But even with the boss relic swap, Ascension 0 is very easy for me now.&lt;/p&gt;


&lt;p&gt;Slay the Spire is not an easy game, so a year or two ago this would have been inconceivable, but I've played more than 600 hours of it since then.&lt;/p&gt;


&lt;p&gt;I'm not &lt;em&gt;amazing&lt;/em&gt; at the game, but I'm quite good at it and, as a result, I can all but sleepwalk my way through Ascension 0, even with the harder tactics.&lt;/p&gt;


&lt;p&gt;So I decided that it wasn't enough for me to just beat act 3, I had to beat the heart to advance.&lt;/p&gt;


&lt;p&gt;The idea being that I have to learn to overshoot my goal.&lt;/p&gt;


&lt;p&gt;This starts to be at least a little difficult, even at the lower ascension levels.&lt;/p&gt;


&lt;p&gt;Enough that I have to pay attention, certainly.&lt;/p&gt;


&lt;p&gt;This approach lets me take advantage of the tighter feedback loops at lower ascension levels to focus on the parts of the game that I am still bad at.&lt;/p&gt;


&lt;p&gt;It removes slack. &lt;/p&gt;


&lt;p&gt;As a result I must be consistently good to progress, instead of just lucky.&lt;/p&gt;


&lt;p&gt;I will still progress by luck to some degree, because the game is hard enough that if you get unlucky you can die, even at lower ascension levels.&lt;/p&gt;


&lt;p&gt;But luck can only get you so far, and the changes I have made force me to develop more consistent skill.&lt;/p&gt;


&lt;p&gt;This is a bit like an idea called Mastery Training.&lt;/p&gt;


&lt;p&gt;Mastery Training is an approach where in order to learn something, you must master what comes before it.&lt;/p&gt;


&lt;p&gt;For example, requiring an A in all the prerequisite courses before you're allowed to take a course.&lt;/p&gt;


&lt;p&gt;This isn't always a good idea.&lt;/p&gt;


&lt;p&gt;I suspect it's often a bad idea.&lt;/p&gt;


&lt;p&gt;But what I'm doing here is an interesting variant of it.&lt;/p&gt;


&lt;p&gt;It is not mastery, but remastery.&lt;/p&gt;


&lt;p&gt;I have "completed the course" that is Ascensions 0 through 19.&lt;/p&gt;


&lt;p&gt;And now I'm retaking them, but with additional constraints, and trying to retake them at a higher level.&lt;/p&gt;


&lt;p&gt;I am relearning "familiar subjects" with the goal of getting an A in them this time.&lt;/p&gt;


&lt;p&gt;The theory being that what is holding me back from progressing past my current level might be a lack of consistency at the foundations.&lt;/p&gt;


&lt;p&gt;This seems like a good general lesson to apply.&lt;/p&gt;


&lt;p&gt;When we are stuck, start again from the beginning, but this time you have to do it &lt;em&gt;right&lt;/em&gt;.&lt;/p&gt;


&lt;p&gt;You have a clear path you walked before.&lt;/p&gt;


&lt;p&gt;You know the way.&lt;/p&gt;


&lt;p&gt;But now you must walk it again.&lt;/p&gt;


&lt;p&gt;This time carrying weights.&lt;/p&gt;


&lt;p&gt;Or paying careful attention to how you place your feet.&lt;/p&gt;


&lt;p&gt;Perhaps we should do this more generally?&lt;/p&gt;


&lt;p&gt;When we have something that is too hard, we replace it with something that is too easy, but done differently.&lt;/p&gt;


&lt;p&gt;For example, when we have reached a point where we feel our writing has plateaued, we start again with simpler writing, but done differently.&lt;/p&gt;


&lt;p&gt;Perhaps with a writing exercise where we change the cadence of our writing.&lt;/p&gt;


&lt;p&gt;And talk about mundane subjects.&lt;/p&gt;


&lt;p&gt;And use this to practice our voice, or our editing.&lt;/p&gt;


&lt;p&gt;You know, hypothetically.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-04-19-10:16.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-06-07-06:51.html</id>
    <title>A round-tripping problem with expected utility theory</title>
    <updated>2021-06-07T08:42:30+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;A round-tripping problem with expected utility theory&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-06-07&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;For reasons, I've been thinking about subjective expected utility theory and the &lt;a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem"&gt;Von Neumann-Morgenstern utility theorem&lt;/a&gt; ("the VNM theorem" for short) recently.
I have some significant philosophical objections to subjective expected utility and I'm going to explore a few of them in a series of notebook posts. This post is about an interesting thing I noticed while trying to articulate those objections.
I don't necessarily expect it to persuade anyone of anything.&lt;/p&gt;


&lt;p&gt;The problem is this: The VNM theorem loses information. You cannot actually reconstruct the original decision procedure from the output of the VNM theorem, because the VNM theorem's output isn't quite what it's usually framed as.&lt;/p&gt;


&lt;p&gt;The VNM theorem is as follows: Suppose you have a set of outcomes \(\mathcal{O} = \{O_1, \ldots, O_n\}\).
A lottery \(L\) is a random variable in \(\mathcal{O}\).&lt;/p&gt;


&lt;p&gt;An agent has preferences over lotteries, expressed by a preorder \(\preceq\) and an indifference relation \(\sim\).
That is \(L \preceq L'\) means that \(L'\) is at least as good as \(L\), and \(L \sim L'\) means the agent is indifferent between them.&lt;/p&gt;


&lt;p&gt;The VNM theorem is that under certain reasonableness conditions, this preorder must be expressible in terms of a utility function \(u: \mathcal{O} \to \mathbb{R}\), such that \(L \preceq L'\) if and only if \(E(u(L)) \leq E(u(L'))\). i.e. we prefer a lottery if it has a higher expected utility, and are indifferent between two lotteries with the same expected utility.&lt;/p&gt;


&lt;p&gt;It's instructive to sketch the proof of this theorem:&lt;/p&gt;


&lt;p&gt;Because we have a total order over outcomes, there is a best outcome and a worst outcome. Call them \(B\) and \(W\) respectively. If \(B \sim W\) then we're necessarily indifferent between all lotteries, so assume \(W \prec B\).
Now, consider the family of lotteries \(M_p\) such that \(M_p\) is \(B\) with probability \(p\) and \(W\) with probability \(1 - p\). So \(M_1 = B\) and \(M_0 = W\).&lt;/p&gt;


&lt;p&gt;Let \(L\) be some lottery. We must have \(M_0 = W \preceq L \preceq B = M_1\).
Therefore (by the hypotheses of the theorem) we can find some \(p\) in the middle such that \(M_p \sim L\).
Some computation shows that this number \(p\) behaves like an expected value.&lt;/p&gt;


&lt;p&gt;Thus the VNM theorem gives us the expected utility of an arbitrary lottery. Right?&lt;/p&gt;


&lt;p&gt;Except, it doesn't. It guarantees that this expected utility exists, but it does not actually give us a way to compute it.
The hypothesis of the VNM theorem is that there is some point at which the switchover to indifference happens, but we are not given an oracle which computes that probability, only a guarantee of its existence.&lt;/p&gt;


&lt;p&gt;Instead what we have to do is turn our oracle for preferences into a sort of mathematical root finding algorithm to find increasingly fine grained approximations to \(p\). We construct a sequence of approximations \(q_k \leq p \leq r_k\) by setting \(q_0 = 0\), \(r_0 = 1\), then to calculate \(q_{k+1}, r_{k+q}\) we set \(s_k = \frac{q_k + r_k}{2}\) and then asking whether \(M_{s_k} \preceq L\). If yes, we set \(q_{k + 1} = r_k, r_{k + 1} = s_k\), if no we set \(q_{k + 1} = s_k, r_{k + 1} = r_k\).&lt;/p&gt;


&lt;p&gt;There is no guarantee that this process will ever terminate - the numbers for which it terminates are precisely the dyadic rationals, the integers divided by some power of two, so for example if \(p = \frac{1}{3}\) it will go on forever, producing increasingly good approximations. You can design other divide and conquer schemes - there's nothing special about this particular one - but they will all have this problem of there being some utility values that this doesn't converge to.&lt;/p&gt;


&lt;p&gt;(Honesty compels me to admit that this problem goes away if you insist that probabilities and utilities have to be rational numbers, because you can design a divide and conquer scheme that just enumerates all of the rationals between the current bounds. For technical reasons that I will not go into here I find this a fundamentally unsatisfying conclusion, but the short version is that the real numbers are a better approximation to the problems of physical measurement than the rational numbers are)&lt;/p&gt;


&lt;p&gt;Which is fine, in practical terms you will very rarely need to know your utility to the last decimal place, but the interesting thing to note is this: Given access to a procedure that provides these increasingly fine grained approximations to the expected utility, you cannot actually recreate the original decision procedure with a process that is guaranteed to terminate.&lt;/p&gt;


&lt;p&gt;The problem is this: Suppose we have some procedure that calculates a relationship \(\preceq'\) based on such converging approximate utilities. We want \(\preceq'\) to be identical with \(\preceq\), so lets assume we've succeeded and see if this leads to a contradiction.&lt;/p&gt;


&lt;p&gt;Let \(p\) be some number where the process doesn't terminate and ask whether \(M_p \preceq' M_p\) (using the same \(M_p\) as above). If \(\preceq'\) agrees with \(\preceq\) we will get the correct answer that \(M_p \sim' M_p\).&lt;/p&gt;


&lt;p&gt;Now, look at how many steps of the approximation you had to compute in order to get to that point, say \(k\). This gives us an interval \(q_k \leq p \leq r_k\).&lt;/p&gt;


&lt;p&gt;Now at this point we decided that \(M_p \sim' M_p\) based on no information other than that \(p \in [q_k, r_k]\). Therefore for any other \(s \in [q_k, r_k]\) we would also decide that \(M_p \sim M_s\) (because we can't tell based on what we've computed that \(p \neq s\)). But in our original relationship we must have \(M_p \prec M_s\) or \(M_s \prec M_p\), because their utilities are \(p\) and \(s\) respectively. Therefore we have failed to recreate \(\preceq\) from the output of the VNM theorem.&lt;/p&gt;


&lt;p&gt;Why is this interesting?&lt;/p&gt;


&lt;p&gt;Well, mostly it's interesting because this looks exactly like my normal objections to the VNM theorem, but normally I justify it on practical grounds: You run into &lt;a href="https://en.m.wikipedia.org/wiki/Fredkin%27s_paradox"&gt;Fredkin's paradox&lt;/a&gt; (a term I've only recently learned - I've always thought of this as the &lt;a href="https://en.m.wikipedia.org/wiki/Buridan%27s_ass"&gt;Buridan's ass problem&lt;/a&gt;), where the amount of work you have to do to decide goes to infinity as you approach points of indifference.
From a practical viewpoint where you have all sorts of measurement and approximation problems,
this is (to me) obviously a problem with the theory, which is generally handwaved away by the setup:
The VNM theorem basically starts by saying "OK so first assume you're omniscient..." which is what allows it to start by assuming that you have this decision procedure.&lt;/p&gt;


&lt;p&gt;The interesting thing about this result is that the VNM theorem runs into this problem even on its own terms, without introducing any additional assumptions of physical reasonableness (unless you count "cannot run a literally infinite amount of computation"). The VNM theorem starts from an omniscient decision procedure and constructs a utility function, but you cannot run that process backwards from the utility function to such a decision procedure because of precisely the sorts of limitations that meant that you couldn't really have constructed that procedure in the first place. &lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-06-07-06:51.html" rel="alternate"/>
  </entry>
</feed>
