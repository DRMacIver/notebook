<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://notebook.drmaciver.com/</id>
  <title>DRMacIver's notebook</title>
  <updated>2025-03-26T19:06:00+00:00</updated>
  <author>
    <name>David R. MacIver</name>
    <email>david@drmaciver.com</email>
  </author>
  <link href="https://notebook.drmaciver.com" rel="alternate"/>
  <link href="https://notebook.drmaciver.com/feed.xml" rel="self"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2024-10-15-09:00.html</id>
    <title>“Fast” enumeration of partial assignments in SAT problems</title>
    <updated>2024-10-15T11:20:08+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;“Fast” enumeration of partial assignments in SAT problems&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2024-10-15&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;In &lt;a href="https://notebook.drmaciver.com/posts/2024-09-22-14:28.html"&gt;my
last post on SAT problems&lt;/a&gt; I noted that you can remove a single
variable from a SAT problem using the technique I outlined. That is, you
can build a SAT problem over the remaining variables that is solvable
iff there is some assignment of the removed variable that satisfies the
original problem.&lt;/p&gt;


&lt;p&gt;This will only sometimes run in reasonable time, because it runs in
one SAT solver call per clause you add, but most of the time&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;That is, for most but not all variables in a large SAT
problem.&lt;/span&gt; in my relatively limited experiments
that seems to be actually quite small.&lt;/p&gt;


&lt;p&gt;One open question I have is whether you can do better than removing
one variable at a time. In particular, if you have some very small
subset of the variables of a SAT problem, and you want to find the set
of clauses that those variables must satisfy to be extensible to an
assignment of the whole, removing one variable at a time ends up hugely
expensive.&lt;/p&gt;


&lt;p&gt;There is a fairly obvious brute force algorithm that you can use of
essentially enumerating all partial assignments up until the point where
you get a contradiction and adding the negations of the contradiction as
a clause. This will be worst-case exponential in the number of variables
in your subset.&lt;/p&gt;


&lt;p&gt;I suspect, but haven’t yet got an example that proves, that this is
actually essential and that it’s relatively easy to construct SAT
problems where exactly one assignment of the subset is not extensible to
the assignment of the whole, and this is in some way non-obvious. If so,
you might intrinsically need to check every possible assignment, which
is intrinsically exponential.&lt;/p&gt;


&lt;p&gt;Anyway this argument might be wrong, but I’ve still not figured out a
guaranteed better way to do this than brute force enumeration. But I
have figured out how to do the brute force enumeration
&lt;em&gt;faster&lt;/em&gt;.&lt;/p&gt;


&lt;p&gt;The core is the following function:&lt;/p&gt;


&lt;div class="sourceCode" id="cb1"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span id="cb1-1"&gt;&lt;a aria-hidden="true" href="#cb1-1" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="im"&gt;from&lt;/span&gt; pysat.solvers &lt;span class="im"&gt;import&lt;/span&gt; Glucose4&lt;/span&gt;
&lt;span id="cb1-2"&gt;&lt;a aria-hidden="true" href="#cb1-2" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="im"&gt;import&lt;/span&gt; numpy &lt;span class="im"&gt;as&lt;/span&gt; np&lt;/span&gt;
&lt;span id="cb1-3"&gt;&lt;a aria-hidden="true" href="#cb1-3" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-4"&gt;&lt;a aria-hidden="true" href="#cb1-4" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-5"&gt;&lt;a aria-hidden="true" href="#cb1-5" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="kw"&gt;def&lt;/span&gt; refine_assignment_table(clauses, variables, table):&lt;/span&gt;
&lt;span id="cb1-6"&gt;&lt;a aria-hidden="true" href="#cb1-6" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;"""Take a k x n table of assignments of n variables,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-7"&gt;&lt;a aria-hidden="true" href="#cb1-7" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    and refine it so that it contains only satisfying assignments&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-8"&gt;&lt;a aria-hidden="true" href="#cb1-8" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    of a set of SAT clauses, where a row in the table is interpreted&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-9"&gt;&lt;a aria-hidden="true" href="#cb1-9" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    such that row[i] is the assigned value of variables[i]."""&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-10"&gt;&lt;a aria-hidden="true" href="#cb1-10" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-11"&gt;&lt;a aria-hidden="true" href="#cb1-11" tabindex="-1"&gt;&lt;/a&gt;    n &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(variables)&lt;/span&gt;
&lt;span id="cb1-12"&gt;&lt;a aria-hidden="true" href="#cb1-12" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;assert&lt;/span&gt; table.shape[&lt;span class="dv"&gt;1&lt;/span&gt;] &lt;span class="op"&gt;==&lt;/span&gt; n&lt;/span&gt;
&lt;span id="cb1-13"&gt;&lt;a aria-hidden="true" href="#cb1-13" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-14"&gt;&lt;a aria-hidden="true" href="#cb1-14" tabindex="-1"&gt;&lt;/a&gt;    variables &lt;span class="op"&gt;=&lt;/span&gt; np.array(variables)&lt;/span&gt;
&lt;span id="cb1-15"&gt;&lt;a aria-hidden="true" href="#cb1-15" tabindex="-1"&gt;&lt;/a&gt;    variable_indexes &lt;span class="op"&gt;=&lt;/span&gt; {v: i &lt;span class="cf"&gt;for&lt;/span&gt; i, v &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;enumerate&lt;/span&gt;(variables)}&lt;/span&gt;
&lt;span id="cb1-16"&gt;&lt;a aria-hidden="true" href="#cb1-16" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-17"&gt;&lt;a aria-hidden="true" href="#cb1-17" tabindex="-1"&gt;&lt;/a&gt;    sub_clauses &lt;span class="op"&gt;=&lt;/span&gt; []&lt;/span&gt;
&lt;span id="cb1-18"&gt;&lt;a aria-hidden="true" href="#cb1-18" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-19"&gt;&lt;a aria-hidden="true" href="#cb1-19" tabindex="-1"&gt;&lt;/a&gt;    solver &lt;span class="op"&gt;=&lt;/span&gt; Glucose4(clauses)&lt;/span&gt;
&lt;span id="cb1-20"&gt;&lt;a aria-hidden="true" href="#cb1-20" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-21"&gt;&lt;a aria-hidden="true" href="#cb1-21" tabindex="-1"&gt;&lt;/a&gt;    i &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-22"&gt;&lt;a aria-hidden="true" href="#cb1-22" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;while&lt;/span&gt; i &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(table):&lt;/span&gt;
&lt;span id="cb1-23"&gt;&lt;a aria-hidden="true" href="#cb1-23" tabindex="-1"&gt;&lt;/a&gt;        row &lt;span class="op"&gt;=&lt;/span&gt; table[i]&lt;/span&gt;
&lt;span id="cb1-24"&gt;&lt;a aria-hidden="true" href="#cb1-24" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;assert&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(row) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(variables)&lt;/span&gt;
&lt;span id="cb1-25"&gt;&lt;a aria-hidden="true" href="#cb1-25" tabindex="-1"&gt;&lt;/a&gt;        assignment &lt;span class="op"&gt;=&lt;/span&gt; [&lt;span class="bu"&gt;int&lt;/span&gt;(v &lt;span class="cf"&gt;if&lt;/span&gt; x &lt;span class="cf"&gt;else&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;v) &lt;span class="cf"&gt;for&lt;/span&gt; x, v &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;zip&lt;/span&gt;(row, variables)]&lt;/span&gt;
&lt;span id="cb1-26"&gt;&lt;a aria-hidden="true" href="#cb1-26" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;if&lt;/span&gt; solver.solve(assignment):&lt;/span&gt;
&lt;span id="cb1-27"&gt;&lt;a aria-hidden="true" href="#cb1-27" tabindex="-1"&gt;&lt;/a&gt;            &lt;span class="co"&gt;# This row is a valid assignment. Move on to the next one.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-28"&gt;&lt;a aria-hidden="true" href="#cb1-28" tabindex="-1"&gt;&lt;/a&gt;            i &lt;span class="op"&gt;+=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-29"&gt;&lt;a aria-hidden="true" href="#cb1-29" tabindex="-1"&gt;&lt;/a&gt;            &lt;span class="cf"&gt;continue&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-30"&gt;&lt;a aria-hidden="true" href="#cb1-30" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-31"&gt;&lt;a aria-hidden="true" href="#cb1-31" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# This row is an invalid assignment, so we can learn a new clause&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-32"&gt;&lt;a aria-hidden="true" href="#cb1-32" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# from it by extracting an unsatisfying core.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-33"&gt;&lt;a aria-hidden="true" href="#cb1-33" tabindex="-1"&gt;&lt;/a&gt;        clause &lt;span class="op"&gt;=&lt;/span&gt; [&lt;span class="op"&gt;-&lt;/span&gt;v &lt;span class="cf"&gt;for&lt;/span&gt; v &lt;span class="kw"&gt;in&lt;/span&gt; solver.get_core()]&lt;/span&gt;
&lt;span id="cb1-34"&gt;&lt;a aria-hidden="true" href="#cb1-34" tabindex="-1"&gt;&lt;/a&gt;        sub_clauses.append(clause)&lt;/span&gt;
&lt;span id="cb1-35"&gt;&lt;a aria-hidden="true" href="#cb1-35" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-36"&gt;&lt;a aria-hidden="true" href="#cb1-36" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# We now remove all rows that don't satisfy this clause. Note that&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-37"&gt;&lt;a aria-hidden="true" href="#cb1-37" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# all rows before this one must satisfy it as they are valid&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-38"&gt;&lt;a aria-hidden="true" href="#cb1-38" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# assignments.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-39"&gt;&lt;a aria-hidden="true" href="#cb1-39" tabindex="-1"&gt;&lt;/a&gt;        clause_mask &lt;span class="op"&gt;=&lt;/span&gt; np.array(clause)&lt;/span&gt;
&lt;span id="cb1-40"&gt;&lt;a aria-hidden="true" href="#cb1-40" tabindex="-1"&gt;&lt;/a&gt;        positive_mask &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(n, dtype&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="bu"&gt;bool&lt;/span&gt;)&lt;/span&gt;
&lt;span id="cb1-41"&gt;&lt;a aria-hidden="true" href="#cb1-41" tabindex="-1"&gt;&lt;/a&gt;        negative_mask &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(n, dtype&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="bu"&gt;bool&lt;/span&gt;)&lt;/span&gt;
&lt;span id="cb1-42"&gt;&lt;a aria-hidden="true" href="#cb1-42" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-43"&gt;&lt;a aria-hidden="true" href="#cb1-43" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;for&lt;/span&gt; v &lt;span class="kw"&gt;in&lt;/span&gt; clause:&lt;/span&gt;
&lt;span id="cb1-44"&gt;&lt;a aria-hidden="true" href="#cb1-44" tabindex="-1"&gt;&lt;/a&gt;            &lt;span class="cf"&gt;if&lt;/span&gt; v &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;:&lt;/span&gt;
&lt;span id="cb1-45"&gt;&lt;a aria-hidden="true" href="#cb1-45" tabindex="-1"&gt;&lt;/a&gt;                positive_mask[variable_indexes[v]] &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-46"&gt;&lt;a aria-hidden="true" href="#cb1-46" tabindex="-1"&gt;&lt;/a&gt;            &lt;span class="cf"&gt;else&lt;/span&gt;:&lt;/span&gt;
&lt;span id="cb1-47"&gt;&lt;a aria-hidden="true" href="#cb1-47" tabindex="-1"&gt;&lt;/a&gt;                negative_mask[variable_indexes[&lt;span class="op"&gt;-&lt;/span&gt;v]] &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-48"&gt;&lt;a aria-hidden="true" href="#cb1-48" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-49"&gt;&lt;a aria-hidden="true" href="#cb1-49" tabindex="-1"&gt;&lt;/a&gt;        positive_satisfaction &lt;span class="op"&gt;=&lt;/span&gt; table[:, positive_mask] &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-50"&gt;&lt;a aria-hidden="true" href="#cb1-50" tabindex="-1"&gt;&lt;/a&gt;        negative_satisfaction &lt;span class="op"&gt;=&lt;/span&gt; table[:, negative_mask] &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="va"&gt;False&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-51"&gt;&lt;a aria-hidden="true" href="#cb1-51" tabindex="-1"&gt;&lt;/a&gt;        satisfying &lt;span class="op"&gt;=&lt;/span&gt; np.&lt;span class="bu"&gt;any&lt;/span&gt;(&lt;/span&gt;
&lt;span id="cb1-52"&gt;&lt;a aria-hidden="true" href="#cb1-52" tabindex="-1"&gt;&lt;/a&gt;            np.concatenate(&lt;/span&gt;
&lt;span id="cb1-53"&gt;&lt;a aria-hidden="true" href="#cb1-53" tabindex="-1"&gt;&lt;/a&gt;                [positive_satisfaction, negative_satisfaction], axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-54"&gt;&lt;a aria-hidden="true" href="#cb1-54" tabindex="-1"&gt;&lt;/a&gt;            ),&lt;/span&gt;
&lt;span id="cb1-55"&gt;&lt;a aria-hidden="true" href="#cb1-55" tabindex="-1"&gt;&lt;/a&gt;            axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,&lt;/span&gt;
&lt;span id="cb1-56"&gt;&lt;a aria-hidden="true" href="#cb1-56" tabindex="-1"&gt;&lt;/a&gt;        )&lt;/span&gt;
&lt;span id="cb1-57"&gt;&lt;a aria-hidden="true" href="#cb1-57" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;assert&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(satisfying) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(table)&lt;/span&gt;
&lt;span id="cb1-58"&gt;&lt;a aria-hidden="true" href="#cb1-58" tabindex="-1"&gt;&lt;/a&gt;        table &lt;span class="op"&gt;=&lt;/span&gt; table[satisfying]&lt;/span&gt;
&lt;span id="cb1-59"&gt;&lt;a aria-hidden="true" href="#cb1-59" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; sub_clauses, table&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This takes some table of assignments of the set of variables and
refines it until it contains only valid assignments, learning a set of
clauses along the way required to achieve that subset. This allows us to
fairly rapidly filter out any unsatisfying assignments, so that when the
set of valid assignments is small we should typically run in a much
smaller number of SAT calls. Even in an absolute worst case scenario
(where every core is the length of the whole set of variables) this does
no worse than a more direct brute force enumeration.&lt;/p&gt;


&lt;p&gt;The big advantage of this over previous attempts I’ve made at doing
it via direct enumeration is that it’s not in any way sensitive to
variable ordering. If you’re attempting the obvious recursive bit and
you find a new clause, you’ve now got to figure out how to propagate
that clause up and back down the recursion. Having everything in a
single table of assignments means this is handled automatically for
you.&lt;/p&gt;


&lt;p&gt;We can then just run this on a table consisting of the set of all
possible assignments of the variables, but we can do slightly better
than that by breaking the variables up as follows:&lt;/p&gt;


&lt;div class="sourceCode" id="cb2"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span id="cb2-1"&gt;&lt;a aria-hidden="true" href="#cb2-1" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="kw"&gt;def&lt;/span&gt; enumerate_subsets(n):&lt;/span&gt;
&lt;span id="cb2-2"&gt;&lt;a aria-hidden="true" href="#cb2-2" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;"""Returns a 2^n x n table of all possible subsets of {0, ..., n - 1}"""&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-3"&gt;&lt;a aria-hidden="true" href="#cb2-3" tabindex="-1"&gt;&lt;/a&gt;    numbers &lt;span class="op"&gt;=&lt;/span&gt; np.arange(&lt;span class="dv"&gt;2&lt;/span&gt;&lt;span class="op"&gt;**&lt;/span&gt;n, dtype&lt;span class="op"&gt;=&lt;/span&gt;np.uint32)[:, np.newaxis]&lt;/span&gt;
&lt;span id="cb2-4"&gt;&lt;a aria-hidden="true" href="#cb2-4" tabindex="-1"&gt;&lt;/a&gt;    powers_of_two &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt; &lt;span class="op"&gt;**&lt;/span&gt; np.arange(n, dtype&lt;span class="op"&gt;=&lt;/span&gt;np.uint32)&lt;/span&gt;
&lt;span id="cb2-5"&gt;&lt;a aria-hidden="true" href="#cb2-5" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; (numbers &lt;span class="op"&gt;&amp;amp;&lt;/span&gt; powers_of_two) &lt;span class="op"&gt;!=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-6"&gt;&lt;a aria-hidden="true" href="#cb2-6" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-7"&gt;&lt;a aria-hidden="true" href="#cb2-7" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-8"&gt;&lt;a aria-hidden="true" href="#cb2-8" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="kw"&gt;def&lt;/span&gt; concatenate_matrix_rows(matrix1, matrix2):&lt;/span&gt;
&lt;span id="cb2-9"&gt;&lt;a aria-hidden="true" href="#cb2-9" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;"""Return a matrix containing every concatenation of a row&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-10"&gt;&lt;a aria-hidden="true" href="#cb2-10" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    from each of its two source matrices."""&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-11"&gt;&lt;a aria-hidden="true" href="#cb2-11" tabindex="-1"&gt;&lt;/a&gt;    m, n &lt;span class="op"&gt;=&lt;/span&gt; matrix1.shape&lt;/span&gt;
&lt;span id="cb2-12"&gt;&lt;a aria-hidden="true" href="#cb2-12" tabindex="-1"&gt;&lt;/a&gt;    j, k &lt;span class="op"&gt;=&lt;/span&gt; matrix2.shape&lt;/span&gt;
&lt;span id="cb2-13"&gt;&lt;a aria-hidden="true" href="#cb2-13" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-14"&gt;&lt;a aria-hidden="true" href="#cb2-14" tabindex="-1"&gt;&lt;/a&gt;    reshaped1 &lt;span class="op"&gt;=&lt;/span&gt; matrix1.reshape(m, &lt;span class="dv"&gt;1&lt;/span&gt;, n).repeat(j, axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;)&lt;/span&gt;
&lt;span id="cb2-15"&gt;&lt;a aria-hidden="true" href="#cb2-15" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-16"&gt;&lt;a aria-hidden="true" href="#cb2-16" tabindex="-1"&gt;&lt;/a&gt;    reshaped2 &lt;span class="op"&gt;=&lt;/span&gt; matrix2.reshape(&lt;span class="dv"&gt;1&lt;/span&gt;, j, k).repeat(m, axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;)&lt;/span&gt;
&lt;span id="cb2-17"&gt;&lt;a aria-hidden="true" href="#cb2-17" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-18"&gt;&lt;a aria-hidden="true" href="#cb2-18" tabindex="-1"&gt;&lt;/a&gt;    result &lt;span class="op"&gt;=&lt;/span&gt; np.concatenate([reshaped1, reshaped2], axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;).reshape(&lt;/span&gt;
&lt;span id="cb2-19"&gt;&lt;a aria-hidden="true" href="#cb2-19" tabindex="-1"&gt;&lt;/a&gt;        m &lt;span class="op"&gt;*&lt;/span&gt; j, n &lt;span class="op"&gt;+&lt;/span&gt; k&lt;/span&gt;
&lt;span id="cb2-20"&gt;&lt;a aria-hidden="true" href="#cb2-20" tabindex="-1"&gt;&lt;/a&gt;    )&lt;/span&gt;
&lt;span id="cb2-21"&gt;&lt;a aria-hidden="true" href="#cb2-21" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-22"&gt;&lt;a aria-hidden="true" href="#cb2-22" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; result&lt;/span&gt;
&lt;span id="cb2-23"&gt;&lt;a aria-hidden="true" href="#cb2-23" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-24"&gt;&lt;a aria-hidden="true" href="#cb2-24" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-25"&gt;&lt;a aria-hidden="true" href="#cb2-25" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="kw"&gt;def&lt;/span&gt; assignment_table_and_clauses(clauses, variables):&lt;/span&gt;
&lt;span id="cb2-26"&gt;&lt;a aria-hidden="true" href="#cb2-26" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;"""Returns a set of sub-clauses over `variables` such that&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-27"&gt;&lt;a aria-hidden="true" href="#cb2-27" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    an assignment of variables can be extended to an assignment&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-28"&gt;&lt;a aria-hidden="true" href="#cb2-28" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    for the whole set of clauses iff it satisfies those subclauses,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-29"&gt;&lt;a aria-hidden="true" href="#cb2-29" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    together with an assignment table representing all valid assignments&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-30"&gt;&lt;a aria-hidden="true" href="#cb2-30" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    of the variables."""&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-31"&gt;&lt;a aria-hidden="true" href="#cb2-31" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-32"&gt;&lt;a aria-hidden="true" href="#cb2-32" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(variables) &lt;span class="op"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="dv"&gt;8&lt;/span&gt;:&lt;/span&gt;
&lt;span id="cb2-33"&gt;&lt;a aria-hidden="true" href="#cb2-33" tabindex="-1"&gt;&lt;/a&gt;        sub_clauses &lt;span class="op"&gt;=&lt;/span&gt; []&lt;/span&gt;
&lt;span id="cb2-34"&gt;&lt;a aria-hidden="true" href="#cb2-34" tabindex="-1"&gt;&lt;/a&gt;        table &lt;span class="op"&gt;=&lt;/span&gt; enumerate_subsets(&lt;span class="bu"&gt;len&lt;/span&gt;(variables))&lt;/span&gt;
&lt;span id="cb2-35"&gt;&lt;a aria-hidden="true" href="#cb2-35" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;else&lt;/span&gt;:&lt;/span&gt;
&lt;span id="cb2-36"&gt;&lt;a aria-hidden="true" href="#cb2-36" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# Solve the problem for two halves of the variables and then&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-37"&gt;&lt;a aria-hidden="true" href="#cb2-37" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# merge the tables. NB: If we had a reliable heuristic for&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-38"&gt;&lt;a aria-hidden="true" href="#cb2-38" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# grouping variables together that have a lot of influence&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-39"&gt;&lt;a aria-hidden="true" href="#cb2-39" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# on each other we should probably use it here. e.g. it probably&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-40"&gt;&lt;a aria-hidden="true" href="#cb2-40" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;# makes sense to group variables in the same clause together.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-41"&gt;&lt;a aria-hidden="true" href="#cb2-41" tabindex="-1"&gt;&lt;/a&gt;        split &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(variables) &lt;span class="op"&gt;//&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb2-42"&gt;&lt;a aria-hidden="true" href="#cb2-42" tabindex="-1"&gt;&lt;/a&gt;        sub_clauses_left, table_left &lt;span class="op"&gt;=&lt;/span&gt; assignment_table_and_clauses(&lt;/span&gt;
&lt;span id="cb2-43"&gt;&lt;a aria-hidden="true" href="#cb2-43" tabindex="-1"&gt;&lt;/a&gt;            clauses, variables[:split]&lt;/span&gt;
&lt;span id="cb2-44"&gt;&lt;a aria-hidden="true" href="#cb2-44" tabindex="-1"&gt;&lt;/a&gt;        )&lt;/span&gt;
&lt;span id="cb2-45"&gt;&lt;a aria-hidden="true" href="#cb2-45" tabindex="-1"&gt;&lt;/a&gt;        sub_clauses_right, table_right &lt;span class="op"&gt;=&lt;/span&gt; assignment_table_and_clauses(&lt;/span&gt;
&lt;span id="cb2-46"&gt;&lt;a aria-hidden="true" href="#cb2-46" tabindex="-1"&gt;&lt;/a&gt;            clauses, variables[split:]&lt;/span&gt;
&lt;span id="cb2-47"&gt;&lt;a aria-hidden="true" href="#cb2-47" tabindex="-1"&gt;&lt;/a&gt;        )&lt;/span&gt;
&lt;span id="cb2-48"&gt;&lt;a aria-hidden="true" href="#cb2-48" tabindex="-1"&gt;&lt;/a&gt;        sub_clauses &lt;span class="op"&gt;=&lt;/span&gt; cub_clauses_left &lt;span class="op"&gt;+&lt;/span&gt; sub_clauses_right&lt;/span&gt;
&lt;span id="cb2-49"&gt;&lt;a aria-hidden="true" href="#cb2-49" tabindex="-1"&gt;&lt;/a&gt;        table &lt;span class="op"&gt;=&lt;/span&gt; concatenate_matrix_rows(table_left, table_right)&lt;/span&gt;
&lt;span id="cb2-50"&gt;&lt;a aria-hidden="true" href="#cb2-50" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb2-51"&gt;&lt;a aria-hidden="true" href="#cb2-51" tabindex="-1"&gt;&lt;/a&gt;    new_clauses, table &lt;span class="op"&gt;=&lt;/span&gt; refine_assignment_table(clauses, variables, table)&lt;/span&gt;
&lt;span id="cb2-52"&gt;&lt;a aria-hidden="true" href="#cb2-52" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; sub_clauses &lt;span class="op"&gt;+&lt;/span&gt; new_clauses, table&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When there are subsets of the variables that also have fairly
restricted assignments, working out their possible assignments first
means that you can do a little better than exponential. If those subsets
end up being mostly unconstrained you at least don’t do worse.&lt;/p&gt;


&lt;p&gt;Now, obviously this is all still exponential in the worst case, and
the worst case is actually pretty common, but it’s fast enough that it
significantly improves what I thought was the number of variables that
it was reasonable to expect this sort of thing to work on - in my
current experiments it seems to work pretty well with sets of up to
about 16-20 variables (16 is fast, 20 is tractable) in a SAT problem of
high hundreds of variables. This isn’t huge, but it’s still a useful
boundary to have pushed up a bit.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2024-10-15-09:00.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2024-12-03-12:01.html</id>
    <title>Filtered sampling from sorted values with incremental binary search</title>
    <updated>2024-12-03T13:19:13+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Filtered sampling from sorted values with incremental binary search&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2024-12-03&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I had a problem recently that I solved badly. As is traditional, I
figured out how to solve it well in the shower this morning. The
solution is obvious in retrospect, but I thought I’d share it anyway as
it’s an interesting algorithm that I’d not seen before.&lt;/p&gt;


&lt;div class="sourceCode" id="cb1"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span id="cb1-1"&gt;&lt;a aria-hidden="true" href="#cb1-1" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="kw"&gt;def&lt;/span&gt; filtered_sample(random, values, lower_bound, upper_bound):&lt;/span&gt;
&lt;span id="cb1-2"&gt;&lt;a aria-hidden="true" href="#cb1-2" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;"""Samples a random value `x` from `values`, which must be a sorted list,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-3"&gt;&lt;a aria-hidden="true" href="#cb1-3" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    such that lower_bound &amp;lt;= x &amp;lt;= upper_bound, or raises ValueError if there are no such values.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-4"&gt;&lt;a aria-hidden="true" href="#cb1-4" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;    """&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-5"&gt;&lt;a aria-hidden="true" href="#cb1-5" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;# Invariant: If 0 &amp;lt;= i &amp;lt; lo then values[i] &amp;lt; lower_bound&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-6"&gt;&lt;a aria-hidden="true" href="#cb1-6" tabindex="-1"&gt;&lt;/a&gt;    lo &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-7"&gt;&lt;a aria-hidden="true" href="#cb1-7" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;# Invariant: If hi &amp;lt;= i &amp;lt; len(values) then values[i] &amp;gt; upper_bound&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-8"&gt;&lt;a aria-hidden="true" href="#cb1-8" tabindex="-1"&gt;&lt;/a&gt;    hi &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(values)&lt;/span&gt;
&lt;span id="cb1-9"&gt;&lt;a aria-hidden="true" href="#cb1-9" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;while&lt;/span&gt; lo &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; hi:&lt;/span&gt;
&lt;span id="cb1-10"&gt;&lt;a aria-hidden="true" href="#cb1-10" tabindex="-1"&gt;&lt;/a&gt;        i &lt;span class="op"&gt;=&lt;/span&gt; random.randrange(lo, hi)&lt;/span&gt;
&lt;span id="cb1-11"&gt;&lt;a aria-hidden="true" href="#cb1-11" tabindex="-1"&gt;&lt;/a&gt;        x &lt;span class="op"&gt;=&lt;/span&gt; values[i]&lt;/span&gt;
&lt;span id="cb1-12"&gt;&lt;a aria-hidden="true" href="#cb1-12" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;if&lt;/span&gt; x &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; lower_bound:&lt;/span&gt;
&lt;span id="cb1-13"&gt;&lt;a aria-hidden="true" href="#cb1-13" tabindex="-1"&gt;&lt;/a&gt;            lo &lt;span class="op"&gt;=&lt;/span&gt; i &lt;span class="op"&gt;+&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-14"&gt;&lt;a aria-hidden="true" href="#cb1-14" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;elif&lt;/span&gt; x &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; upper_bound:&lt;/span&gt;
&lt;span id="cb1-15"&gt;&lt;a aria-hidden="true" href="#cb1-15" tabindex="-1"&gt;&lt;/a&gt;            hi &lt;span class="op"&gt;=&lt;/span&gt; i&lt;/span&gt;
&lt;span id="cb1-16"&gt;&lt;a aria-hidden="true" href="#cb1-16" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;else&lt;/span&gt;:&lt;/span&gt;
&lt;span id="cb1-17"&gt;&lt;a aria-hidden="true" href="#cb1-17" tabindex="-1"&gt;&lt;/a&gt;            &lt;span class="cf"&gt;return&lt;/span&gt; x&lt;/span&gt;
&lt;span id="cb1-18"&gt;&lt;a aria-hidden="true" href="#cb1-18" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;raise&lt;/span&gt; &lt;span class="pp"&gt;ValueError&lt;/span&gt;(&lt;/span&gt;
&lt;span id="cb1-19"&gt;&lt;a aria-hidden="true" href="#cb1-19" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="ss"&gt;f"`values` does not contain any elements between &lt;/span&gt;&lt;span class="sc"&gt;{&lt;/span&gt;lower_bound&lt;span class="sc"&gt;}&lt;/span&gt;&lt;span class="ss"&gt; and &lt;/span&gt;&lt;span class="sc"&gt;{&lt;/span&gt;upper_bound&lt;span class="sc"&gt;}&lt;/span&gt;&lt;span class="ss"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-20"&gt;&lt;a aria-hidden="true" href="#cb1-20" tabindex="-1"&gt;&lt;/a&gt;    )&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is a nice example algorithm that varies smoothly between two
slightly more obvious algorithms:&lt;/p&gt;


&lt;ol type="1"&gt;
&lt;li&gt;Rejection sampling (just sample until you get a value in the right
range)&lt;/li&gt;
&lt;li&gt;Always do a binary search to find the bounds, and then random sample
within that range.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Rejection sampling is the best when almost all of the values are in
range. Binary search is best when few or none are (in particular the
first cannot easily detect it. There are ways to mitigate that
though)&lt;/p&gt;


&lt;p&gt;My initial implementation did one round of rejection sampling and
then the binary search, which is a sort of awkward compromise. The way
the new algorithm works is essentially doing only as much of the binary
search as you need. If a sample gives you a suitable value now, you can
stop and return that. If it doesn’t, you’ve effectively chosen a
midpoint in your binary search and can update the bounds for the next
round of the sample.&lt;/p&gt;


&lt;p&gt;The first algorithm basically only works as well as whichever is
better of the first two, while the new algorithm smoothly varies between
the two extremes. I believe (but haven’t checked the details) that it
runs in &lt;code&gt;O(log(n / (k + 1))&lt;/code&gt; where &lt;code&gt;k&lt;/code&gt; is the
number of elements in the array satisfying the condition.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2024-12-03-12:01.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2025-02-08-09:26.html</id>
    <title>How do LLMs work?</title>
    <updated>2025-02-10T11:18:24+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;How do LLMs work?&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2025-02-08&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;Someone asked me to explain how LLMs work to them without metaphors.
This is my attempt.&lt;/p&gt;


&lt;p&gt;Specifically, this is an attempt to explain what LLMs do and what the
various parts of the pipeline look like. There are a lot of extremely
technical internal details that you’re not going to understand without a
lot more maths and engineering background. Much of them &lt;em&gt;I&lt;/em&gt; don’t
understand. Think of this as an explanation at the level of “A car has
an engine. The engine makes the car go. It does this by burning fuel and
spinning a crank”. If you want to know how engines work, talk to a
better mechanic than me.&lt;/p&gt;


&lt;h3 id="the-parts-of-a-chatbot"&gt;The parts of a chatbot&lt;/h3&gt;


&lt;p&gt;I think part of the reason LLMs are confusing is that you don’t
actually interact with an LLM. You interact with &lt;em&gt;a piece of software
built on top of an LLM&lt;/em&gt;. The software is often a pretty thin layer,
so this may seem like a bit of a distinction without a difference, but I
think it leads to a misleading idea of what the LLM actually does,
especially with regards to training.&lt;/p&gt;


&lt;p&gt;Roughly speaking,&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;There are a bunch of other things going on too. For
example there’s likely a variety of classifiers running as an extra
layer to detect harmful output - whenever ChatGPT tells you a message
violates its content policies for example, that’s some other layer
having decided that the LLM output is inappropriate in some way. But the
three layers I describe are enough to create something very like what
you experience when talking to an LLM.&lt;/span&gt; you can think of something like
Claude or ChatGPT as consisting of three layers:&lt;/p&gt;


&lt;ol type="1"&gt;
&lt;li&gt;A &lt;strong&gt;chatbot&lt;/strong&gt; - the bit you actually talk to.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;completion engine&lt;/strong&gt; - this is a piece of software
that takes some text and continues it in some way. e.g. If you give it
the text “The quick brown fox” it might complete it by adding “jumps
over the lazy dog” or by adding “is a bit of a good idea”&lt;label class="margin-toggle sidenote-number" for="fn2"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn2" type="checkbox"/&gt;&lt;span class="sidenote"&gt;This is what Android’s autocomplete suggests for that
start.&lt;/span&gt;.
When people talk about LLMs “just being fancy autocomplete”, this is the
layer they mean, and they’re right but that “just” is doing a lot of
work. It’s &lt;em&gt;very&lt;/em&gt; fancy autocomplete.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;language model&lt;/strong&gt; - this is a piece of software
which, given a bit of text, gives you a set of possible completions to
that text, with probabilities for each one. You don’t really need to
understand probability to know how to interpret this.&lt;label class="margin-toggle sidenote-number" for="fn3"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn3" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Though if you want to, &lt;a href="https://notebook.drmaciver.com/posts/2021-10-29-09:43.html"&gt;I’ve
got this other piece right here&lt;/a&gt;&lt;/span&gt;.
All “probability” means is that the numbers are between 0 and 1 and add
up to 1. For example, with our start of “The quick brown fox”, an LLM
might say that you could continue this with “jumps over the lazy dog”
with probability 90% and “is a bit of a good idea” with probability
10%.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;All of the magic happens in making the language model, and how good a
chat bot you get depends entirely on the quality of the underlying
language model. All of the big innovations of the last couple of years
have been in learning to build much better language models than we’ve
historically had, and part of the key to that is in the word “large” in
“large language model”. The language models we’re building are just much
large than ones we used to build - ones at the smallest end are still a
couple of gigabytes. This is partly because we’ve got hardware that can
handle that, and partly because we’ve figured out how to put that size
to good work. Exactly how we’ve done that… I don’t plan to tell you in
much detail. Clever computer science and big computers. But I will tell
you a bit about what goes into making them shortly.&lt;/p&gt;


&lt;h3 id="building-a-completion-engine"&gt;Building a completion engine&lt;/h3&gt;


&lt;p&gt;So we’ve got our language model. It takes text, and it gives us a set
of continuation probabilities.&lt;/p&gt;


&lt;p&gt;First, we need to build a completion engine.&lt;/p&gt;


&lt;p&gt;The way you build a completion engine out of a language model is you
write a program that takes text and feeds it to a large language model,
which gives you back some continuations of that text with probabilities.
You then pick any one of those continuations for which the probability
is greater than zero.&lt;/p&gt;


&lt;p&gt;There are two natural choices:&lt;/p&gt;


&lt;ol type="1"&gt;
&lt;li&gt;You can pick the one with the highest probability (picking
arbitrarily if there are ties)&lt;/li&gt;
&lt;li&gt;You can pick one at random with the provided probabilities. So
e.g. with the above probabilities 90% of the time we’d continue with
“jumps over the lazy dog” and 10% of the time we’d pick “is a bit of a
good idea”.&lt;label class="margin-toggle sidenote-number" for="fn4"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn4" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Try not to get too bogged down in what this means. If it
helps, imagine rolling a 100-sided fair die and picking the first on a
roll of 1-90 and a the second on a roll of 91-100.&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;If you’ve heard of a “temperature” parameter, the first is what we
mean by “temperature 0” and the latter is what we mean by “temperature
1”. A temperature between the two means that we pick randomly but we
skew the choices so that we’re more likely to pick a number with high
probability than the base probabilities suggest.&lt;label class="margin-toggle sidenote-number" for="fn5"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn5" type="checkbox"/&gt;&lt;span class="sidenote"&gt;e.g. if we have a temperature of 0.1 we might pick
“jumps over the lazy dog” with probability more like 99%. We would still
sometimes generate the less probable option, but more rarely.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;Typically, you will want to do this more than once, until some
condition is fulfilled, feeding the text plus your previous completion
back into the completion engine. For example, to get the “is a bit of a
good idea” completion I repeatedly asked the Google autocomplete for one
word at a time until I was bored, so I actually got the following series
of completions:&lt;/p&gt;


&lt;ol type="1"&gt;
&lt;li&gt;“The quick brown fox” -&amp;gt; ” is”&lt;/li&gt;
&lt;li&gt;“The quick brown fox is” -&amp;gt; ” a”&lt;/li&gt;
&lt;li&gt;“The quick brown fox is a” -&amp;gt; ” bit”&lt;/li&gt;
&lt;li&gt;“The quick brown fox is a bit” -&amp;gt; ” of”&lt;/li&gt;
&lt;li&gt;“The quick brown fox is a bit of” -&amp;gt; ” a”&lt;/li&gt;
&lt;li&gt;“The quick brown fox is a bit of a” -&amp;gt; ” good”&lt;/li&gt;
&lt;li&gt;“The quick brown fox is a bit of a good” -&amp;gt; ” idea”&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In a program, rather than stopping when you get bored, you will
usually feed the text you generated from the language model back into
itself until either you hit some length limit or the completion returned
contains some piece of text that indicates you should stop
completing.&lt;/p&gt;


&lt;h3 id="building-a-chatbot"&gt;Building a chatbot&lt;/h3&gt;


&lt;p&gt;Now that you’ve got a completion engine, you build a chatbot by
basically having the completion engine and the human coauthor a
transcript together.&lt;/p&gt;


&lt;p&gt;For example, suppose I start a conversation with “Hi Claude. What’s
the capital of France?”, the chatbot software will generate the
following text:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Human: Hi Claude. What’s the capital of France?&lt;/p&gt;
&lt;p&gt;Assistant:&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;Which indicates that in this transcript it’s now the assistant’s turn
to speak. The completion engine then continues writing this transcript,
completing it to the following:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Human: Hi Claude. What’s the capital of France?&lt;/p&gt;
&lt;p&gt;Assistant: Paris is the capital of France.&lt;/p&gt;
&lt;p&gt;Human:&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;With the “Human:” bit now indicating that it’s up for the human to
continue the transcript, and the chatbot software now letting you type
your next response.&lt;label class="margin-toggle sidenote-number" for="fn6"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn6" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Side note: This description is somewhat Claude specific.
I think ChatGPT works the same way. Many of the open source models use a
different format. The same basic idea applies though.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;You can see this play out in funny ways:&lt;/p&gt;


&lt;p&gt;&lt;img src="/images/confusedclaude.png"/&gt;&lt;/p&gt;


&lt;p&gt;What’s happening here is that every time Claude tries to explain the
transcript format to me, it does so by writing “Human:” at the start of
the line. This causes the chatbot part of the software to go “Ah, a line
starting with ‘Human:’. Time to hand back over to the human.” and
interrupt Claude before it can finish what it’s writing.&lt;/p&gt;


&lt;p&gt;This is a bit of a digression, but I like this sort of abstraction
leaking.&lt;/p&gt;


&lt;p&gt;The main thing I want to get across here though is that when you are
interacting with a chatbot you are not “talking to the LLM” (or even to
the completion engine). You are talking to “Assistant”, a character that
the completion engine is writing.&lt;/p&gt;


&lt;p&gt;One way to see this in action is that the completion engine will also
just as happily write the “Human:” parts of the transcript if you let
it. A while back I was writing some evaluations to test an LLM
capability, and accidentally screwed up the code that would stop it from
continuing the completion when it had written “Human:”. What happened
was a transcript that looked roughly like the following&lt;label class="margin-toggle sidenote-number" for="fn7"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn7" type="checkbox"/&gt;&lt;span class="sidenote"&gt;This is a made up example and not the actual evaluation
I was running.&lt;/span&gt;&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Human: What is the capital of France?&lt;/p&gt;
&lt;p&gt;Assistant: The capital of France is Paris.&lt;/p&gt;
&lt;p&gt;Human: That’s correct! Thank you for being such a helpful assistant.
What’s the capital of Germany?&lt;/p&gt;
&lt;p&gt;Assistant: The capital of Germany is Berlin.&lt;/p&gt;
&lt;p&gt;Human: Correct again. You’re such a good assistant. Good job.&lt;label class="margin-toggle sidenote-number" for="fn8"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn8" type="checkbox"/&gt;&lt;span class="sidenote"&gt;I’m exaggerating a little but this was genuinely the
tone of the human side of the conversations, repeatedly praising the LLM
for being such a good assistant. It was very funny.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(this goes on for a while)&lt;/p&gt;
&lt;p&gt;Human: OK that’s enough questions for now. Thank you for your
assistance today.&lt;/p&gt;
&lt;p&gt;Assistant: You’re very welcome!&lt;/p&gt;
&lt;p&gt;Human: OK bye now.&lt;/p&gt;
&lt;p&gt;Assistant: Goodbye&lt;/p&gt;
&lt;p&gt;Human: OK I’m leaving.&lt;/p&gt;
&lt;p&gt;Assistant: It is probably time to end this conversation yes.&lt;/p&gt;
&lt;p&gt;Human: Goodbye&lt;/p&gt;
&lt;p&gt;Assistant: Goodbye&lt;/p&gt;
&lt;p&gt;Human: Bye now.&lt;/p&gt;
&lt;p&gt;(this goes on for a while until it hit the length limit)&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;In theory you could get it to write any other characters you wanted
in this transcript too, but the language models we use are typically
optimised for this human and assistant dialogue.&lt;/p&gt;


&lt;h3 id="building-a-language-model"&gt;Building a language model&lt;/h3&gt;


&lt;p&gt;Now we get to the hard part, building a language model. Or, rather,
building a language model is easy, building one good enough to be worth
using in this way is hard.&lt;/p&gt;


&lt;p&gt;We’re not going to cover how to build one good enough, but we will
see how you could build a bad one, and where the hard technical details
of building a better one will slot in.&lt;/p&gt;


&lt;p&gt;First, a note: A language model typically operates on
&lt;em&gt;tokens&lt;/em&gt;. A token is basically a small chunk of text. A token
might be a single letter, or a whole word, or even several words. Often
what you want is for common pieces of text to have a dedicated token.
For example, it might be reasonable to represent “Human:” and
“Assistant:” as their own single tokens.&lt;label class="margin-toggle sidenote-number" for="fn9"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn9" type="checkbox"/&gt;&lt;span class="sidenote"&gt;I think this is not in fact typically done and they’re
short sequences of tokens, but I’m not sure about this.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;For the purposes of ease of explanation I’m going to ignore this and
talk about language models that are operating on single characters, so
the goal of the model at any given point is to give you a set of
characters that might come next in the sequence and probabilities for
that. So for example you might give it the text “Th” and it might return
that “e” has probability 80% as the next character, “a” has 15%, …., “z”
has 0.01%, etc. &lt;label class="margin-toggle sidenote-number" for="fn10"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn10" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Note that these probabilities are not in any sense the
“true” probabilities of the next character. They’re some completely
arbitrary numbers given to you by the language model. They can be better
or worse for some purpose, but they’re not right or wrong, they’re
simply the numbers the model gives you.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;Let’s consider the simplest possible class of language model, which
completely ignores the text you give it and just gives you the same
character probabilities each time. Even that has some variation.&lt;/p&gt;


&lt;p&gt;Here are two completions (starting from an empty text) from two
different examples of such a model.&lt;/p&gt;


&lt;p&gt;The first:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;[9sV7tWP3iÇqëT#S’o56fRIgi N3-hVg,œ5:CA :Pp’%4DM&lt;span class="math inline"&gt;\(™b%H;  %\)&lt;/span&gt;Æ‘g4YOdee85IqzÀq1X55wàHœR‘Æ—_&amp;amp;jOaçbv%;sBJL”k&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;and the second:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;dt ,gsadeaohti yee Pauml th ooid hso&lt;/p&gt;
&lt;p&gt;,;lt .rec arsn . ae&lt;/p&gt;
&lt;p&gt;,khe tIr’Tpym rHil aim em&lt;/p&gt;
&lt;p&gt;n wt&lt;/p&gt;
&lt;p&gt;m,ssou&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;In the first, each of the characters it knows about is given equal
probabilty. In the second, the probabilities are matched to how often
the characters appear in the complete works of Shakespeare. Despite
this, the second example is decidedly &lt;em&gt;not&lt;/em&gt; Shakespeare, but it
looks a little bit less like Gibberish than the first does.&lt;/p&gt;


&lt;p&gt;These two models have the same structure, which is that we’ve got a
single number for each character (of which we have 107, all the ones
that appear in the complete works of Shakespeare I have), which
determines the probabilities. In the first, each has a probability of
&lt;span class="math inline"&gt;\(\frac{1}{107}\)&lt;/span&gt;. In the second,
e.g. ‘e’ has a probability of 8.4%, and ‘Q’ has a probability of 0.02%&lt;label class="margin-toggle sidenote-number" for="fn11"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn11" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Note that these are case sensitive. “E” has a
probability of 0.6% and “q” 0.05%. Capital letters are just less common
than lower case ones.&lt;/span&gt; These numbers that determine the
specific behaviour of the model once we know its structure are called
&lt;em&gt;parameters&lt;/em&gt; or &lt;em&gt;weights&lt;/em&gt;&lt;label class="margin-toggle sidenote-number" for="fn12"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn12" type="checkbox"/&gt;&lt;span class="sidenote"&gt;I think the distinction people mean here is that the
weights are a specific set of numbers that the parameters corresponding
to a given structure take in a particular language model. So e.g. “the
probability of ‘e’” is a parameter, and it has weight 8.4% here. I’m not
actually sure this distinction is reliably held to in normal usage
though.&lt;/span&gt;,
and training a large language model consists entirely of picking the
weights for a fixed structure.&lt;label class="margin-toggle sidenote-number" for="fn13"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn13" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Note that in general weights are not just
probabilities, and are any numbers that might go into computing those
final probabilities. There will often be intermediate values in the
calculuation that don’t have any straightforward interpretation as a
probability of a particular token. e.g. you might have a weight that
represents the probability that the text is in French, or one that is
how much preference to give to asking questions. Most weights will end
up being much less straightforwardly interpretable than this.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;To give you a brief idea of how much the structure matters, let’s
consider the second simplest type of language model, which is a type of
what’s called a Markov chain.&lt;label class="margin-toggle sidenote-number" for="fn14"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn14" type="checkbox"/&gt;&lt;span class="sidenote"&gt;In a certain sense, an LLM is actually still a Markov
chain, but it’s a Markov chain in much the same way that it’s fancy
autocompletion.&lt;/span&gt; We look at only the
last character of the text to determine the likelihood of the next
character. Here’s a sample from a completion engine using such a
language model:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Ay, s Allomel cobeinonlat ant cod. Wer a s d NTr’s heee mee at,
OLSire hthe w theind ha CHestisthoo&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;Again, it’s not exactly Shakespeare, but it’s &lt;em&gt;less&lt;/em&gt; not
exactly Shakespeare than either of the previous examples. Adding more
ability to look at the text that it’s given gives you the ability to
produce something that looks more and more like coherent text. You can
extend this idea further, letting it look at more characters at a time,
and eventually you get something that starts to look almost like
coherent English until you try to actually make sense of its
meaning.&lt;/p&gt;


&lt;p&gt;For context, &lt;a href="https://github.com/shreydan/shakespeareGPT/blob/main/saved/v2/generated.txt"&gt;here’s
some output from (someone else’s) toy LLM trained on
Shakespeare&lt;/a&gt;:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;And, do remember, in the ungracious of war.&lt;/p&gt;
&lt;p&gt;Hear’s my tent&lt;/p&gt;
&lt;p&gt;And see me that crystal’s death, Tranio, he affections of love
you.&lt;/p&gt;
&lt;p&gt;GLOUCESTER:&lt;/p&gt;
&lt;p&gt;YORK:&lt;/p&gt;
&lt;p&gt;GLOUCESTER:&lt;/p&gt;
&lt;p&gt;In God the curtain are you, this wilful Chertsey monastery a puissant
stoop’d me.&lt;/p&gt;
&lt;p&gt;GLOUCESTER:&lt;/p&gt;
&lt;p&gt;And then, my lord, we were ‘shall’?&lt;/p&gt;
&lt;p&gt;KING HENRY VI: widow’d from his foe surprised at once, being so:&lt;/p&gt;
&lt;p&gt;As heavy&lt;/p&gt;
&lt;p&gt;I am subtle&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;It &lt;em&gt;still&lt;/em&gt; doesn’t make sense, but with each improvement in
the complexity of a model its ability to sound almost like it knows what
its talking about improves.&lt;/p&gt;


&lt;p&gt;One key thing to note in going from the simpler version to the
complicated version is that there are now a lot more parameters. Our
previous model that just gave every character equal a probability
independently of what came before it has 107 parameters - one for each
possible character in the text - while our slightly better one that uses
the last previous character has 11449 parameters - one for each pair of
characters (e.g. the probability of a ‘t’ coming after a ‘c’). This
holds in general: The more able to represent language your language
model is, the more parameters you should expect it to have. When people
are talking about model sizes, they mean the number of paramters. &lt;a href="https://en.wikipedia.org/wiki/GPT-4"&gt;Wikipedia says&lt;/a&gt; that GPT-4
probably has about 1.76 trillion parameters.&lt;/p&gt;


&lt;p&gt;Getting the structure of your language model right is essential for
making sure it can actually represent language well enough to do the job
you want it to, but whether it actually does that all comes down to
whether the weights have the right value. The third example is only
better than the second because of the particular choice of weights! I
could just as easily have set it so that the probabilities were all
equal and we’d have got something as bad as the first example.&lt;/p&gt;


&lt;p&gt;So, once you’ve got the structure of your language model, you need to
figure out the right weights to give it. That’s where training comes
in.&lt;/p&gt;


&lt;h3 id="how-to-train-a-language-model"&gt;How to train a language
model&lt;/h3&gt;


&lt;p&gt;So you’ve got a language model and you want to give it a good set of
weights. How? Training!&lt;/p&gt;


&lt;p&gt;Except, “training” is something of a misleading metaphor. You’re not
really taking a language model and teaching it anything, you’re taking a
language model and creating a new language model more suited to your
needs. “Training” is what we call that though. It’s also called
“reinforcement learning”.&lt;label class="margin-toggle sidenote-number" for="fn15"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn15" type="checkbox"/&gt;&lt;span class="sidenote"&gt;This technically means something more specific, but
don’t worry about it.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;We do this by defining what’s called a &lt;em&gt;loss function&lt;/em&gt;. This
is some program that takes a language model and gives it a score. You
want to create a language model with as low a score as possible.&lt;label class="margin-toggle sidenote-number" for="fn16"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn16" type="checkbox"/&gt;&lt;span class="sidenote"&gt;e.g. a loss function could be a measure of how many
mistakes it makes.&lt;/span&gt; The process we use is what’s called
&lt;em&gt;gradient descent&lt;/em&gt;. This is a very conceptually straightforward
process&lt;label class="margin-toggle sidenote-number" for="fn17"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn17" type="checkbox"/&gt;&lt;span class="sidenote"&gt;with a huge amount of technical detail to make work
well enough&lt;/span&gt;, which basically consists of taking
your current weights, and finding a very small adjustment to them that
descreases the loss function slightly.&lt;label class="margin-toggle sidenote-number" for="fn18"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn18" type="checkbox"/&gt;&lt;span class="sidenote"&gt;How to actually find that is a little bit technical.
You can imagine trying lots of small variations and seeing if any of
them work, but usually it’s possible to do better than that. Getting
this step fast is one of the big things that we’ve needed to develop a
lot of robust technology in order to make progress in this field.&lt;/span&gt;
You replace your weights with those modified ones, and keep going, You
do this for as long as you want, stopping when you can’t find any way to
improve the loss function or, more likely, you’ve exceeded your budget
for computer time. When training a model, you start from some previous
model that you got from your last training run, or if you don’t have one
you just set all of its weights to arbitrary values (typically random),
and then you use gradient descent to improve it.&lt;/p&gt;


&lt;p&gt;A large language model is trained in two stages. Funnily, these are
called “pre-training” and “post-training”, with no training stage. These
correspond to different loss functions.&lt;/p&gt;


&lt;p&gt;Pre-training produces what is called a “base model”, which is a
language model that is not particularly well suited to being used as a
chatbot, but has taken in a large body of text (typically a decent chunk
of the internet). Here, the loss function feeds it many short chunks of
text, and measures how often it correctly predicts the following text.
In this way, the base model ends up “learning” a lot of the knowledge
encoded in its text, because it is able to represent it well enough to
predict what comes next.&lt;label class="margin-toggle sidenote-number" for="fn19"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn19" type="checkbox"/&gt;&lt;span class="sidenote"&gt;It could of course do this by memorisation, and
sometimes it will do this, but generally you don’t &lt;em&gt;want&lt;/em&gt; it to
do that. You want it to encode the knowledge in a way that generalises
to unfamiliar text, representing it as efficiently as possible. One of
the things that helps you do this is making sure that the text you’re
training on is much much larger than the model, which is part of why
training is so data hungry. It will sometimes memorise anyway, but this
is a bug not a feature.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;Once you have a base model, you create a chat model in the
post-training phase. This involves creating a loss function that
represents “how good it is at being a chatbot”. You do this by giving it
a large number of starting texts to a completion engine using the model
(some of which might be as simple as “Human:”, letting it generate the
whole conversation, and some of which might have quite specific
prompts), running the completion on it, and then scoring the result in
some way. The average score over all those prompts is your loss
function, and post-training is designed to make the model behave well
with those prompts.&lt;/p&gt;


&lt;p&gt;Some of this loss function is as simple as things like “Does it hand
back to the human in a reasonable time?”. Others might evaluate whether
it gets correct answers to particular questions (e.g. if you ask it
arithmetic questions, does it give you the right answer?). Others yet
might be more nebulous things, like “Is it helpful, harmless, and
honest?”. That’s where RLHF, reinforcement learning from human feedback,
and &lt;em&gt;preference models&lt;/em&gt; come in.&lt;/p&gt;


&lt;h3 id="using-preference-models-in-training"&gt;Using preference models in
training&lt;/h3&gt;


&lt;p&gt;One of the basic features of training a large language models is the
use of human feedback, comparing two or more different completions and
rating which one is better, or marking a particular answer as harmful,
wrong, etc. You want to be able to incorporate this into your model
training. However, this would be intolerably slow. Gradient descent
makes many small changes, and asking a human for feedback at each point
in that process would take literally forever.&lt;/p&gt;


&lt;p&gt;Instead what we do is we training up another type of model, called a
&lt;em&gt;preference model&lt;/em&gt;. A preference model takes some piece of text
and gives it a score, which we expect to correspond to a score that a
human would give it This gives us a program that we can use as a good
proxy for human feedback: We expect it to score texts roughly as a human
would.&lt;label class="margin-toggle sidenote-number" for="fn20"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn20" type="checkbox"/&gt;&lt;span class="sidenote"&gt;You may of course be noticing that there’s a key
problem with this: A preference model can only learn the texts as well
as its structure allows it to. If humans are scoring things well or
badly based on whether they’re correct in some way an LLM can’t actually
represent, the same will be true for the preference model.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;A preference model is constructed in roughly the same way as a
language model, with a set of weights that you can train up to minimise
some loss function:&lt;/p&gt;


&lt;ol type="1"&gt;
&lt;li&gt;We put together a large number of examples along with the score they
“should” have, or pairs where we say which one is better (e.g. a choice
when asked which of two LLM answers you prefer). These scores are set up
so that a lower score is better, as with a normal loss function.&lt;/li&gt;
&lt;li&gt;We define a loss function which is lower the better the preference
model predicts those scores.&lt;/li&gt;
&lt;li&gt;We train a preference model to minimise that loss.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In post-training, we then perform our reinforcement learning to
minimise a loss function that comes from the preference model: We run
the completions on our training set, score them according to the
preference model’s rating of them, and calculate the loss function as
some combination (e.g. an average) of those scores.&lt;/p&gt;


&lt;p&gt;When the preference model is calculated by predicting humans raters’
responses, this is what’s called RLHF: Reinforcement Learning from Human
Feedback.&lt;/p&gt;


&lt;p&gt;A nice thing about this process is that it allows us to “bank” our
ratings. We don’t have to keep asking humans the same questions over and
over again, or running expensive tests, we can build up a large body of
scored text and use that for training the preference model. It also
generalises better, because the preference model learns quite general
forms of the sorts of features that are preferred.&lt;/p&gt;


&lt;p&gt;Another approach you can take is RLAIF, reinforcement learning from
AI feedback. This is exactly the same sort of thing, but your raters are
now previously trained LLMs who have been given a constitution that says
how to rate responses, and these are used to produce your training data
for the preference model. This is Anthropic’s &lt;a href="https://arxiv.org/abs/2212.08073"&gt;constitutional AI&lt;/a&gt; approach
and despite all the ways it causes your eyebrows to raise when you first
hear it, it reportedly works very well.&lt;/p&gt;


&lt;h3 id="tbd"&gt;TBD&lt;/h3&gt;


&lt;p&gt;There’s probably more to say here and I’ll update this in response to
human feedback.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2025-02-08-09:26.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2025-02-18-10:11.html</id>
    <title>You only need shallow justifications</title>
    <updated>2025-03-05T10:29:24+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;You only need shallow justifications&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2025-02-18&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;This started off on the newsletter and then wandered in a direction I
didn’t intend and as a result stalled at turning into something
adequately newsletter shaped, so I’ve moved it over here to finish it
off in whatever form it wants to be in.&lt;/p&gt;


&lt;hr/&gt;


&lt;p&gt;I have a concept of “the legitimacy of conflict” that I’ve been
trying to articulate for ages and failing to really get a good handle
on. I’ve recently had a hit tweet that I think gives me part of that
handle.&lt;/p&gt;


&lt;p&gt;&lt;a href="https://x.com/DRMacIver/status/1887844134196515016"&gt;The
tweet goes as follows&lt;/a&gt;:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;One mistake people make in salary negotiations is that they try to
make a practical or moral argument for the higher value. This can be
useful, but I’ve had surprisingly good luck recently with words to the
tune of “I like money. How about you offer more money than that?”&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;The point is that you don’t need to justify wanting more money.
Obviously you want more money. All you need to communicate is that the
amount of money that has currently been offered isn’t enough for you to
say yes on the spot, and more would be welcomed.&lt;/p&gt;


&lt;p&gt;Sometimes they will just say yes and offer you more money. Generally
speaking, hiring is expensive and finding a good candidate is hard, and
by the time you’ve got to this point, there’s a pretty wide range of
amounts they’re willing to pay, and they haven’t offered you the highest
amount they’d be willing off the bat.&lt;/p&gt;


&lt;p&gt;Sometimes they will flat out say no. Sometimes they’ll even mean it.
If you want more money at this point you’ll need to either walk away or
&lt;a href="https://books.rixx.de/chris-voss/never-split-the-difference/"&gt;break
out a more serious negotiating toolkit&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;Most of the time this will result in a further discussion about the
value you bring, or how to justify the decision to someone else, or
something of that ilk. Essentially never&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;OK, essentially never under the assumption that you’re
operating in a relatively high strength negotiating position. There’s a
decent chance of this going badly in e.g. minimum wage jobs that are
oversupplied with candidates and run by petty tyrants.&lt;/span&gt;
will they ask you why you want more money, or descend into some Oliver
Twist style rant about how dare you ask for more you miserable little
urchin.&lt;/p&gt;


&lt;p&gt;Generally speaking though, in a remotely healthy environment the
discussion will never involve asking why you want more money, it will
always be about why you are worth more money to the company. Because,
from a business point of view, of course you want more money. Money is
good. Sure maybe you need it to pay the rent, or to care for your
seventeen children, or one of any other reasons you might have, but this
does not actually affect any of the basic business reality, which is
that it’s perfectly reasonable for you to want more money and it’s also
perfectly reasonable for them to not to spend more money.&lt;label class="margin-toggle sidenote-number" for="fn2"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn2" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Now the reality is that the person you’re talking to
does not have incentives perfectly aligned with the business, and as a
result telling them a sob story about how you’ve got all these cute
orphans you’re looking after and you need the extra money to buy them
expensive medicine for their rare genetic diseases please sir won’t you
think of the orphans, might well sway them to try to work a bit harder
to offer you more money. But now, functionally, you’ve just got a repeat
of the general situation with you and them working together to negotiate
with the broader business which is, ultimately, not in the business of
curing rare genetic diseases in orphans for free, but instead makes
widgets.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;A negotiation like this is a great example of what I mean when I talk
about a conflict, and why conflict is legitimate.&lt;/p&gt;


&lt;p&gt;Specifically, I’m using “conflict” in the economics sense of Thomas
Schelling, as articulated in his great book “The Strategy of Conflict”:
A conflict is a positive-sum game with imperfect alignment.&lt;/p&gt;


&lt;p&gt;That is, you have a situation with two (or more) parties in
which:&lt;/p&gt;


&lt;p&gt;There are outcomes which are better for all parties than not
participating in the situation.&lt;/p&gt;


&lt;p&gt;Among those outcomes, there is disagreement as to which is the best
outcome.&lt;/p&gt;


&lt;p&gt;Whenever I use the word “conflict” in this article, I mean it in this
sense of conflicting desires in a situation where it is to your mutual
benefit to figure out an outcome compatible with them both.&lt;/p&gt;


&lt;p&gt;A salary negotiation is precisely this sort of scenario:
Participation is getting hired (which requires both your agreement), and
getting hired is at this point in the process presumed to be better for
both of you than not getting hired (you want the job, they want someone
to do the job and think you would be a good such someone), but the best
outcome for you is that they pay you a lot, and the best outcome for
them is that they pay you a little.&lt;label class="margin-toggle sidenote-number" for="fn3"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn3" type="checkbox"/&gt;&lt;span class="sidenote"&gt;There are all sorts of complexities to this about
retention, ethics, etc. Try to ignore those. At any rate, even to the
degree that those matter, they matter mostly for large changes. A 1%
difference either way probably doesn’t affect either of those that much,
but a 1% pay raise is clearly slightly better for you, and a 1% pay
decrease is clearly slightly better for them.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;One of the difficulties I’ve had in articulating the legitimacy of
conflict is that people hear “conflict” and they think “fight”, where
Schelling and I mean it more in the sense of “things that are in tension
with each other that you need to resolve to make the situation work
well”.&lt;/p&gt;


&lt;p&gt;The other difficulty is that part of the problem I’m pointing at is
that I don’t think people emotionally distinguish the two and do, in
fact, experience something like this as if it were a fight.&lt;/p&gt;


&lt;h3 id="no-correct-answers"&gt;No correct answers&lt;/h3&gt;


&lt;p&gt;I think one reason people experience the salary negotiation as a
fight is that it feels too easy to frame it in moral terms. There’s an
amount of money you “deserve” and if you ask for more than that you’re
greedy and if you get less than that the company is greedy. Either way
someone is bad. This viewpoint is, I must emphasise, wrong, and you’ll
make less miserable and earn less money than you could if you adopt it,
but it’s a very easy trap to fall into. But I think it’s hard to see
that starting from this scenario.&lt;/p&gt;


&lt;p&gt;Here’s another example, I think originally from Schelling:&lt;label class="margin-toggle sidenote-number" for="fn4"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn4" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Though his originally example was probably a bit
different. I think it involved the movies?&lt;/span&gt; Suppose you and I want to hang out,
and settle on going out for a meal. We rule out anything literally
unacceptable to one of us (e.g. as much as I’d like to I can’t eat
pizza, you need options that aren’t super spicy) and settle on a number
of options that we’d be fine with. My favourite is the Thai option,
yours is the Mexican option. Either of us would be fine with either
option, but each of us has a clear preference for one over the other.&lt;label class="margin-toggle sidenote-number" for="fn5"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn5" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Why not find a third option that we both like? Well,
what are you, some sort of fancy big city type with lots of restaurant
options? In my small town, you’re lucky if there’s one restaurant you
like! Two is just luxury.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;More seriously, this is always a tempting rebuttal but I think
basically doesn’t work. There just sometimes aren’t such options. Some
people really do have incompatible food preferences where you’re never
going to find a meal that delights them both, and “delights one of them
and is more or less fine for the other” is probably the best outcome you
can achieve.&lt;/p&gt;


&lt;p&gt;This is particularly true once you add hard constraints. Try feeding
a vegan at the same time as someone with soy and nut allergies and see
how well that goes for you.&lt;/p&gt;


&lt;p&gt;This is a clear conflict: We both want to go out for a meal. We don’t
agree which restaurant is best, but we’d prefer either to not going out
for a meal.&lt;/p&gt;


&lt;p&gt;It would also, I feel, be utterly idiotic for this conflict to turn
into a fight. We’re friends. It’s not just that we can resolve this
amicably, it’s that it getting unamicable in the first place at all
would be a little weird. We can flip a coin, or take turns deciding, or
you can decide where we eat and I decide what bar we go to after, or
adopt one of a variety of other resolution mechanisms, and it will be
fine.&lt;/p&gt;


&lt;p&gt;One thing we shouldn’t do is decide on the basis of who wants their
outcome more. Or, we might do that sometimes - if there’s a cool new
Mexican place in town that you’ve been really excited to try out, and I
want to go to the Thai place I’m familiar with because it’s consistently
pretty good, it seems reasonable that we should pick the Mexican place.
If, however, every time we go out you’re like this, and I never really
acquire your taste for novelty despite our best efforts, I think I’d
quite reasonably feel a little hard done by if my preferences never got
a hearing.&lt;label class="margin-toggle sidenote-number" for="fn6"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn6" type="checkbox"/&gt;&lt;span class="sidenote"&gt;This may seem like a suspiciously specific example, but
it’s honestly entirely fictional.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;Because this is a negotiation as part of an ongoing relationship,
it’s clear that there doesn’t have to be a single right answer to the
conflict in desires, you resolve so that mostly everybody is happy.&lt;label class="margin-toggle sidenote-number" for="fn7"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn7" type="checkbox"/&gt;&lt;span class="sidenote"&gt; I originally wrote “so that everything averages out
over time” but actually I think this is not correct. It may well just be
the case that the result is consistently “unfair” but still better than
not participating. Many different dynamics can lead to this, and it’s
not intrinsically a problem.&lt;/span&gt;&lt;/p&gt;


&lt;h3 id="resource-constraints"&gt;Resource Constraints&lt;/h3&gt;


&lt;p&gt;Another example of a conflict I often use is: Do you take the last &lt;a href="https://en.wikipedia.org/wiki/%C3%89clair"&gt;Eclair&lt;/a&gt; in the shop?
Someone else might want it after all!&lt;label class="margin-toggle sidenote-number" for="fn8"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn8" type="checkbox"/&gt;&lt;span class="sidenote"&gt;This is a slightly strange example of a positive sum
game because although participating is never worse than not
participating, unless there’s an Eclair there it’s not really better
either. Unless you needed to go to the shop anyway.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;I think, clearly, the answer is yes.&lt;label class="margin-toggle sidenote-number" for="fn9"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn9" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Although when I’m buying a lot of something, I will
usually make sure to leave at least one on the shelves. e.g. there’s a
particular frozen chicken curry that the child is currently obsessed
with and that we’re perfectly happy to be a default meal for her, so I
bought a bunch of them recently, and made sure to leave one in the
freezer. It’ll get restocked soon, but in the immediate future if
there’s someone looking for a mediocre Morrisons own brand butter
chicken, they won’t go lacking.&lt;/span&gt;&lt;/p&gt;


&lt;h3 id="abandonment-note"&gt;Abandonment note&lt;/h3&gt;


&lt;p&gt;At this point I stopped writing and I’ve lost enough context that I
no longer intend to resume at this point. I did say that it’s hard for
me to write about this subject!&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2025-02-18-10:11.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2025-03-05-10:29.html</id>
    <title>World counting as a tool for understanding probability</title>
    <updated>2025-03-05T17:10:42+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;World counting as a tool for understanding probability&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2025-03-05&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;There are a number of basic conditional probability problems that
seem counter-intuitive but become intuitive with an extremely basic
technique.&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;So basic that it’s almost embarrassing to describe it as
a technique to be honest, but then I see people getting confused on
problems trivially solvable by it and think it probably warrants that
label after all.&lt;/span&gt; I thought it would be worth
explaining this technique, as I think it’s both a good tool to have in
your toolkit explicitly, and is potentially a good route in for teaching
people to reason about probability.&lt;/p&gt;


&lt;p&gt;The technique is “world counting”. You enumerate all the possible
versions of the world, cross out any ones that have been ruled out by
evidence, and then work out what fraction of the worlds remaining have
the property of interest.&lt;/p&gt;


&lt;p&gt;For example, say I roll two dice, and tell you only that I rolled a
sum of 8 or more. What is the probability that at least one of those
rolls was a 6?&lt;/p&gt;


&lt;p&gt;First, we enumerate all the possible rolls: 1/1, 1/2, …, 2/1, …,
6/6.&lt;/p&gt;


&lt;p&gt;Now, we cut out all of the ones where the sum is less than 8, and get
the following combinations remaining: 2/6, 3/5, 3/6, 4/4, 4/5, 4/6, 5/3,
5/4, 5/5, 5/6, 6/2, 6/3, 6/4, 6/5, 6/6&lt;/p&gt;


&lt;p&gt;That gives us 15 worlds, of which 9 have a 6 in them, so the
probability of a 6 is &lt;span class="math inline"&gt;\(\frac{9}{15} =
0.6\)&lt;/span&gt;.&lt;/p&gt;


&lt;p&gt;This works well with the dice example, but we run into a wrinkle with
a slightly more complicated one.&lt;/p&gt;


&lt;p&gt;Consider the following problem: I flip a coin. If it shows heads, I
flip it again. If it shows tails, I don’t.&lt;/p&gt;


&lt;p&gt;This gives us three worlds: HH, HT, T. Two where I got heads the
first time, one where I got tails the second time.&lt;/p&gt;


&lt;p&gt;Now, what is the probability of getting heads on the first toss? It’s
half, obviously, but the world enumeration approach as I’ve described it
so far gives &lt;span class="math inline"&gt;\(\frac{2}{3}\)&lt;/span&gt; because
there are twice as many worlds with heads as tails. This is clearly
wrong.&lt;/p&gt;


&lt;p&gt;One way to fix this is to imagine that we flip the second coin
regardless but that flip is only “visible” if we got heads on the first
flip. This gives us the worlds HH, HT, TH, TT, where to the observer TH
and TT look identical, and we recover the right probability (&lt;span class="math inline"&gt;\(\frac{1}{2}\)&lt;/span&gt;). For the purpose of getting
the right answer, this is a perfectly valid approach and can sometimes
be very useful, but it doesn’t help that well for understanding what’s
going on and why it wasn’t valid to treat the HH, HT, and T worlds as
equally likely.&lt;/p&gt;


&lt;p&gt;The real problem here is that the world counting method I described
only works for situations where every world is equally likely, and in
this case that’s not true. The T world is twice as likely as either the
HT or the HH worlds.&lt;/p&gt;


&lt;p&gt;So, let’s generalise the technique slightly: A &lt;em&gt;weighted world
enumeration&lt;/em&gt; is a non-empty list of possible worlds together with
some non-zero weight associated with each of them. In order to calculate
the probability of something, you add up the weight of the worlds in
which it’s true, and divide by the total weight of the worlds.&lt;/p&gt;


&lt;p&gt;Now, the question is… how do you get weighted world enumerations?&lt;/p&gt;


&lt;p&gt;You build them as follows:&lt;/p&gt;


&lt;ol type="1"&gt;
&lt;li&gt;You start from a basic enumeration which is just a single world in
which anything could happen, and some arbitrary weight (it doesn’t
matter what. Conventionally it would be 1, but you can pick this for
convenience)&lt;/li&gt;
&lt;li&gt;You can at any point split a world into one or more subworlds which
are mutually incompatible. e.g. if I flip a coin, you could split the
world into the subworlds where it comes up heads and the subworlds that
it comes up tails. The weights you assign to those subworlds can be
whatever you like, as long as they add up to the weight of the original
world. If you think of the subworlds as equally likely you should split
the weight between them evenly.&lt;label class="margin-toggle sidenote-number" for="fn2"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn2" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Note that you’re allowed to throw away possibilities
when you do this split. e.g. if you believe that something is a
necessary consequence of one of your worlds, you can replace it with
some more specialised condition without changing the weights.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;To get a conditional weighted world enumeration (i.e. the world
enumeration you want after observing something that rules out some of
the worlds), you just remove everything from the list that has been
ruled out.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So, with our two coins example, we start with a world weighted at
(say) 4, we first split it into (H, 2), (T, 2), and then again into (HH,
1), (HT, 1), (T, 2). This gets us the intuitive result that the T world
is twice as big as each of the HH and HT worlds (but has the same weight
as them combined).&lt;/p&gt;


&lt;p&gt;Here’s the problem where I first realised quite how useful this
technique is:&lt;label class="margin-toggle sidenote-number" for="fn3"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn3" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Paraphrased from &lt;a href="https://x.com/julianboolean_/status/1836867126360944873"&gt;this
tweet&lt;/a&gt;. In the original form it asks whether the probability that
there’s a ball under the cup has gone up or down, and this doesn’t
actually require you to calculate the probability, because of an
interesting general principle: If &lt;span class="math inline"&gt;\(A,
B\)&lt;/span&gt; are disjoint events with non-zero probabilities of occurring,
then necessarily &lt;span class="math inline"&gt;\(P(A | \neg B) &amp;gt;
P(A)\)&lt;/span&gt;.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;There are 10 opaque cups face down on a table. I flipped a fair coin,
if it was heads I picked 1 cup uniformly at random and placed a ball
under it.&lt;/p&gt;


&lt;p&gt;You flip the first 9 cups. No ball.&lt;/p&gt;


&lt;p&gt;What is the probability that there’s a ball under cup 10?&lt;/p&gt;


&lt;p&gt;We can build up our world enumeration as follows:&lt;/p&gt;


&lt;p&gt;First, for convenience, pick the initial weight to be 20.&lt;/p&gt;


&lt;p&gt;Now, split this into two worlds based on the flip of a coin. Our
world enumeration is now (H, 10), (T, 10). Half the world weight goes to
each coin flip.&lt;/p&gt;


&lt;p&gt;If we picked heads, we now pick a cup at random. This splits the H
world into the 10 possible subworlds, one for each cup. So now we have
(T, 10), (H1, 1), (H2, 1), …, (H10, 1).&lt;/p&gt;


&lt;p&gt;Now, we want our conditional world enumeration, which has determined
that there is no ball in the first 9 cups. This removes all the worlds
in which we placed a ball there, so now our world enumeration is (T,
10), (H10, 1).&lt;/p&gt;


&lt;p&gt;This means that the probability that the ball is in the 10th cup is
&lt;span class="math inline"&gt;\(\frac{1}{10 + 1} = \frac{1}{11}\)&lt;/span&gt;,
because the total weight of the worlds remaining is &lt;span class="math inline"&gt;\(11\)&lt;/span&gt; and of that weight, &lt;span class="math inline"&gt;\(1\)&lt;/span&gt; of it has the ball there.&lt;label class="margin-toggle sidenote-number" for="fn4"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn4" type="checkbox"/&gt;&lt;span class="sidenote"&gt;So to answer the original question, it goes up, because
the original probability before revealing those balls is &lt;span class="math inline"&gt;\(\frac{1}{20}\)&lt;/span&gt;.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;I think this approach is also useful for reasoning about the dreaded
&lt;a href="https://en.wikipedia.org/wiki/Monty_Hall_problem"&gt;Monty Hall
problem&lt;/a&gt;:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Suppose you’re on a game show, and you’re given the choice of three
doors: Behind one door is a car; behind the others, goats. You pick a
door, say No. 1, and the host, who knows what’s behind the doors, opens
another door, say No. 3, which has a goat. He then says to you, “Do you
want to pick door No. 2?” Is it to your advantage to switch your
choice?&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;We start with our initial world, which we’ll give weight 1 out of
convention, and split it three ways based on which door has a car behind
it: 1, 2, 3. We believe these to be equally likely&lt;label class="margin-toggle sidenote-number" for="fn5"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn5" type="checkbox"/&gt;&lt;span class="sidenote"&gt;If we didn’t believe that we could in our head shuffle
the doors and relabel them to make this the case.&lt;/span&gt;, so
give them each weight &lt;span class="math inline"&gt;\(\frac{1}{3}\)&lt;/span&gt;.&lt;/p&gt;


&lt;p&gt;Monty then picks a door, 2 or 3. This gives us 6 theoretically
possible worlds. 12, 13, 22, 23, 32, 33.&lt;/p&gt;


&lt;p&gt;Now, we know that Monty knows which door the car is behind, and thus
presumably is deliberately picking a door which doesn’t have a car. This
means that the worlds 22 and 33 are actually impossible, because Monte
will never pick that door. Thus our actual list is 12, 13, 23, 32.&lt;/p&gt;


&lt;p&gt;The worlds in which it’s an advantage to us to switch are 23 and 32,
so to calculate how likely it is for switching to be good we need to
know their total weight. We don’t know the weights of 12, 13, but we do
know that the total weight stayed unchanged (as we’ve not removed any
possible worlds, only impossible ones that we’d have had to assign
weight zero to), so the total weight is &lt;span class="math inline"&gt;\(1\)&lt;/span&gt; and the probability that switching is
good for us is the sum of the weights of worlds 23 and 32, which is
&lt;span class="math inline"&gt;\(\frac{2}{3}\)&lt;/span&gt;. So switching is good
most of the time.&lt;/p&gt;


&lt;p&gt;It’s interesting to compare this to the version in which Monty
doesn’t know which door has the car behind it and picks randomly (with
you just losing immediately if he picks a car). We now have all six of
our possible worlds 12, 13, 22, 23, 32, 33, with each of these having an
equal weight of &lt;span class="math inline"&gt;\(\frac{1}{6}\)&lt;/span&gt;. Now,
we’ve observed a goat, which cuts out the worlds 22 and 33, so now we’ve
got worlds 12, 13, 23, 32. But this time all of these have equal weight,
so switching is to our advantage with probabilityh &lt;span class="math inline"&gt;\(\frac{1}{2}\)&lt;/span&gt;, and there is no benefit (or
disadvantage) to switching.&lt;/p&gt;


&lt;p&gt;You can extend this technique to general Bayesian reasoning, which is
essentially a modification where rather than cutting out worlds that are
impossible you reweight them based on how likely they are to show the
relevant data, but that’s a topic for another time.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2025-03-05-10:29.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2025-03-22-21:54.html</id>
    <title>There is no sludge</title>
    <updated>2025-03-22T22:50:58+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;There is no sludge&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2025-03-22&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I’ve been thinking recently about sludge.&lt;/p&gt;


&lt;p&gt;Specifically, as a metaphor. When thinking is hard, or motivation is
hard, it feels like trying to think through sludge. Details are hard to
see, everything is harder, and whatever you do it ends up coming away
sortof grimy. It’s a very compelling metaphor. It’s also a common one
for people who, like me, are coming off a stimulant&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;If you have ADHD and rely on stimulants to manage that,
please assume I’m suitably sympathetic to your situation and there are
whatever caveats you need to not treat this article about my own
personal experience with them as an attack. For me they come with
significant downsides and I’m trying to figure out my way to manage
without them, but that doesn’t mean I think they’re in principle an
invalid solution.&lt;/span&gt;
addiction.&lt;label class="margin-toggle sidenote-number" for="fn2"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn2" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Which is to say, I’ve stopped drinking coffee again.
I’ve also heard this from people who quit cigarettes. I assume it’s a
thing for people who quit stronger stimulants but I don’t know for
sure.&lt;/span&gt;.&lt;/p&gt;


&lt;p&gt;Withdrawal is real, of course, I’m not denying that. I’ve certainly
had the agonising headaches to prove it. But when you’re past the first
few weeks of caffeine withdrawal, everything still feels like sludge.
But I think maybe it’s just a skill issue.&lt;/p&gt;


&lt;p&gt;The thing about stimulants is… stimulants are really good at solving
problems. Having a button you can press and make a whole class of of
problems go away is fantastic. Need to get some work done? Drink some
coffee. Feeling tired? Drink some coffee. Bored and feeling kinda
listless? Drink some coffee. Taking a stimulant amps up your general
ability to do things, and that’s pretty good if you’ve got a problem
shaped like needing to do something and not doing it. I’m struggling to
keep focus on writing this right now, and a coffee sure would help.&lt;label class="margin-toggle sidenote-number" for="fn3"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn3" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Except for the fact that it’s 10PM and drinking coffee
right now would be a disaster at the best of times.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;I think a bunch about addiction. I’m not sure I think correct things
about it, but some toy models I have of it are really interesting for
illuminating other things.&lt;/p&gt;


&lt;p&gt;Take heroin, for example.&lt;label class="margin-toggle sidenote-number" for="fn4"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn4" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Which is to say, it’s best if you don’t take heroin.&lt;/span&gt; It’s not hard to imagine
the appeal of what basically amounts to wireheading. Push button, be
happy. Currently unhappy? Take some heroin, that will fix it!&lt;label class="margin-toggle sidenote-number" for="fn5"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn5" type="checkbox"/&gt;&lt;span class="sidenote"&gt;I do want to reemphasise that this is a rhetorical
flourish and not advice.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;Except… it’s not generally my impression that the typical heroin user
is particularly happy. Probably they’re at least temporarily happy while
on heroin, but that doesn’t get to be a 24/7 state.&lt;/p&gt;


&lt;p&gt;&lt;a href="https://drmaciver.substack.com/i/43763473/choosing-not-to-eat-the-lotus"&gt;I’ve
talked about this example before&lt;/a&gt; and one of the pieces of feedback I
got from people who have used heroin is that it’s honestly not that
compelling to them when their life is going well. I think that makes
sense. My guess is &lt;a href="https://en.wikipedia.org/wiki/Rat_Park"&gt;rat
park&lt;/a&gt; is mostly fake&lt;label class="margin-toggle sidenote-number" for="fn6"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn6" type="checkbox"/&gt;&lt;span class="sidenote"&gt;I base this on two key facts: The first is that it
didn’t replicate, the second more important one is that it’s 20th
century psych research.&lt;/span&gt;, but I do still more or less buy the
addiction-as-solution model.&lt;/p&gt;


&lt;p&gt;But… I’m not sure the thing the addiction is solving is unhappiness
exactly. My guess is that if you compare the average heroin user’s life
to one without heroin, the one without is probably in aggregate still
happier. I think the thing the addiction is providing is
&lt;em&gt;clarity&lt;/em&gt;. There is a simple solution to all your problems, it’s
heroin. At any given moment where you have a problem, the heroin will
make your life better, the only cost is that each time you pick this
solution you overall make your life much worse, as an increasingly large
fraction of the problems that heroin solves for you, it also causes.&lt;/p&gt;


&lt;p&gt;Similarly, if you compare my life without coffee to my life with
coffee,&lt;label class="margin-toggle sidenote-number" for="fn7"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn7" type="checkbox"/&gt;&lt;span class="sidenote"&gt;My experience with caffeine is probably worse than
you’re imagining, but even so I am aware this is comparing two very
different extremes, you don’t need to point it out.&lt;/span&gt; I am in fact on average more
functional without coffee than with. But it’s higher variance, and on
any given day where I really need to function, coffee sure would help a
lot… But the cost is that I’m highly likely to relapse and find myself
with another problem which coffee would help a lot with the next day or
so, and then suddenly I’m having coffee every few days, and then daily,
and oh no I can’t quit now I can’t afford to lose three days to
headaches.&lt;label class="margin-toggle sidenote-number" for="fn8"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn8" type="checkbox"/&gt;&lt;span class="sidenote"&gt;If this sounds overdramatic to you, I promise you it’s
not. I’ve quit coffee and relapsed like this multiple times. Even
knowing this I can’t rule out that it’s going to happen again. Although
I have in fact had caffeine exactly once since quitting. I think it was
justified in context, and thankfully it did not result in a second
time.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;Anyway, sludge.&lt;/p&gt;


&lt;p&gt;My theory is that “sludge” is actually fake, and is the addict
response to leaving a world of false clarity and encountering the
messiness of normal human experience.&lt;/p&gt;


&lt;p&gt;Stimulants help you do things, and they do this by helping you bypass
parts of the normal process by which you &lt;em&gt;decide&lt;/em&gt; to do things.
And that process, like all real mature human interaction with the world,
is situation dependent. You have a specific problem, it admits a
specific solution.&lt;/p&gt;


&lt;p&gt;One of the things I notice much more without coffee is that there’s a
sort of anxiety that adheres to the basic act of creation. I’m feeling
quite tense right now just trying to put words on the page, even in a
notebook quality way. There’s a genuine sense of fear and anxiety
manifesting in my upper body. It’s honestly very weird. It keeps making
me want to bounce off the process. Everything feels… not hard and
effortful exactly, but &lt;em&gt;painful&lt;/em&gt;. Like there’s a literal physical
pain to it.&lt;/p&gt;


&lt;p&gt;But that’s not the only thing of course. Sometimes I’m just tired.&lt;label class="margin-toggle sidenote-number" for="fn9"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn9" type="checkbox"/&gt;&lt;span class="sidenote"&gt;No! Fake! Tired isn’t a real thing. It’s many different
real things.&lt;/span&gt; Sometimes I know what I have to do
and there’s no anxiety associated with it but it’s &lt;em&gt;boring&lt;/em&gt; and I
don’t want to do it.&lt;/p&gt;


&lt;p&gt;Sometimes, even worse, I might have to voluntarily do something
mildly uncomfortable.&lt;/p&gt;


&lt;p&gt;Sometimes in order to decide what to do I have to engage with the
most terrifying question of all: What do I actually want?&lt;/p&gt;


&lt;p&gt;All of these are different situations which, if I don’t pay attention
to their fine grained texture, feel like sludge.&lt;/p&gt;


&lt;p&gt;And they’re all completely different situations. The only thing that
unifies them is that stimulants are a solution to them.&lt;/p&gt;


&lt;p&gt;And if I’ve rejected stimulants as a solution &lt;em&gt;there is no
unification&lt;/em&gt;, and their different character becomes apparent as soon
as I attend to my actual experience and try to solve the problem.&lt;/p&gt;


&lt;p&gt;That’s hard. If I’ve spent my entire adult life papering over a
problem, no wonder there’s a skill issue, both with finding out what the
specific problem is and also with solving it.&lt;/p&gt;


&lt;p&gt;But if I ignore it and just talk about sludge, I’m being &lt;a href="https://notebook.drmaciver.com/posts/2022-05-30-18:41.html"&gt;a
victim of metonymy&lt;/a&gt;, and worse I’m doing it by conflating a specific
problem &lt;em&gt;with a broader category I don’t have access to&lt;/em&gt; unless I
choose to adopt the category of solution I’ve rejected. I shouldn’t do
that.&lt;/p&gt;


&lt;p&gt;Don’t adopt ontologies that force you into strategies you want to
avoid. Attend to the specific, and understand your problems in a way
that lets you act on them appropriately.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2025-03-22-21:54.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2025-03-23-21:48.html</id>
    <title>How I clean my kitchen at the end of the day</title>
    <updated>2025-03-23T22:46:50+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;How I clean my kitchen at the end of the day&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2025-03-23&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;This is a guide on how to clean my kitchen. I wrote it up because I
was writing it in my head while cleaning my kitchen and found that
helpful. The audience this post is most useful for is me, and when I say
“you” in it the you in question is mostly me, but some of the general
points might be of broad interest.&lt;/p&gt;


&lt;p&gt;However, many of the specifics of this are only useful if you share a
kitchen with me, and thus have the exact same sets of problems,
constraints, and affordances. If so this post isn’t really intended for
you as that would be massively passive aggressive for me to write a
whole essay telling you how to clean the kitchen. Instead, if you share
a kitchen with me, I refer you to the large poster with helpfully big
letters and clear easy to understand instructions that I have posted on
the kitchen wall.&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Ha ha, just kidding. I haven’t really done that. Yet.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;Anyway, there are some things you should understand before embarking
on cleaning the kitchen:&lt;/p&gt;


&lt;p&gt;The first is that it will, in fact, take about an hour. It might take
a bit less. If it’s a really big dinner or a lot of debris has built up
over the course of the day it might take more. But it will probably take
about an hour, and trying to speed that up massively is going to mostly
stress you out without saving a huge amount of time. You should probably
just let it take an hour. You can resent that, or you can just put up
with it, and you’ll have a better time in that hour if you do the
latter. Put on some music, or a noise track and spend the time in
contemplation, or if you must put on an audiobook. Maybe you can write a
blog post about how to clean the kitchen in your head.&lt;label class="margin-toggle sidenote-number" for="fn2"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn2" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Some of this process can be sped up by having an extra
pair of helping hands but honestly our kitchen is a nightmare to have
multiple people in and it doesn’t speed things up that much so I mostly
just prefer to do it solo.&lt;/span&gt;
Whatever you’re doing though, assume it will take an hour, so make sure
to start at least an hour before bedtime.&lt;/p&gt;


&lt;p&gt;The second thing to understand is that there is no such thing as
“clean”, there is only “clean enough”. You are not attempting
perfection, you are attempting to achieve a consistent standard of
cleaning. This standard mostly consists of the following:&lt;/p&gt;


&lt;ol type="1"&gt;
&lt;li&gt;There are no full bins.&lt;/li&gt;
&lt;li&gt;The dishwasher is running.&lt;/li&gt;
&lt;li&gt;The surfaces are clean.&lt;/li&gt;
&lt;li&gt;The handwashing has been done.&lt;/li&gt;
&lt;li&gt;The floor is not covered in debris.&lt;label class="margin-toggle sidenote-number" for="fn3"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn3" type="checkbox"/&gt;&lt;span class="sidenote"&gt;I’ve not historically been super reliable about this,
and am trying to add sweeping the kitchen to my cleaning process, but I
don’t yet know if it will stick.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The sink should be empty and clean.&lt;/li&gt;
&lt;li&gt;The kitchen should pass a “general inspection” for anything that
seems like it really should be done every time and isn’t explicitly on
this list and doesn’t massively increase the workload.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The third thing to understand is that it doesn’t matter if you’re too
tired to clean the kitchen. The kitchen still needs to be cleaned, and
if you don’t do it then tomorrow will go much worse. If you get part way
through the process and you really can’t continue, you can stop, but
you’ll probably be fine, because the process is on your side.&lt;/p&gt;


&lt;p&gt;The fourth thing to remember is that the process is on your side.
It’s there to help keep momentum up, make it feel achievable to do more,
and minimise the amount of decision making you have to do, so you can
mostly just autopilot the entire experience, and it’s there so that if
you do give up midway through (you probably won’t), you’ve done the
right things first, and so that you won’t at any point have to go “Oh
fuck, I didn’t do that thing, I can’t be bothered to do that now…”.&lt;/p&gt;


&lt;p&gt;Anyway, here is the process.&lt;/p&gt;


&lt;p&gt;The process consists of a rolling series of goals. You should always
do the highest priority one.&lt;/p&gt;


&lt;p&gt;The first goal is &lt;em&gt;there should be no full bins&lt;/em&gt;. There is a
recycling bin, a waste bin, and a food scraps bin, and if at any point
(including the start) of the process one fills up, you should stop
whatever you are doing and empty it. If it’s obvious at some point that
it’s going to fill up later, you can prioritise filling it up and then
emptying it, but this isn’t required.&lt;label class="margin-toggle sidenote-number" for="fn4"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn4" type="checkbox"/&gt;&lt;span class="sidenote"&gt;I’ve not historically been super reliable about this
making this top priority is a modification that became clearly good as a
result of drafting this post.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;The second goal is &lt;em&gt;the dishwasher should be running&lt;/em&gt;. Once
the bins are empty, all actions should be directed towards getting
things in the dishwasher and getting it running. If it’s currently got
clean dishes in it, first empty it, then load it, then run it.&lt;/p&gt;


&lt;p&gt;The third goal is &lt;em&gt;surfaces outside the kitchen should be
cleared&lt;/em&gt;. That means the dining room and living room table need to
have everything taken off them and the tables need to be wiped down.&lt;/p&gt;


&lt;p&gt;From here, it’s a process of narrowing the area that still needs to
be cleaned over time, one surface at a time.&lt;/p&gt;


&lt;p&gt;My kitchen has two counters. Let’s call them sinkside and kettleside.
The next goal is to &lt;em&gt;get the kettleside counter cleaned and wiped
down&lt;/em&gt;. This is partly as a restriction move, and partly because
you’ll need the space to put extra drying up. Once it’s clear, lay some
cloths down to put extra drying up on it.&lt;/p&gt;


&lt;p&gt;The sinkside counter has two halves, left of stove and right of
stove. Now clean the surface left of stove, corralling all remaining
dirty dishes onto the stove top and right of stove area.&lt;/p&gt;


&lt;p&gt;Your priority is now to &lt;em&gt;wash the subset of those dishes that
can’t go in the dishwasher when it’s finished running&lt;/em&gt;. These will
be left to dry, you’re not going to manually dry them up.&lt;/p&gt;


&lt;p&gt;At this point if you’re genuinely exhausted and it’s late, you may
stop, but ideally you would not.&lt;/p&gt;


&lt;p&gt;Your next goal is to &lt;em&gt;handwash any of the dishes that could go in
the dishwasher tomorrow&lt;/em&gt;, because this enabled the following goal
which is &lt;em&gt;get the right of stove counter completely clean&lt;/em&gt;.&lt;/p&gt;


&lt;p&gt;If you can face it, &lt;em&gt;clean the stove&lt;/em&gt;. Don’t do a proper deep
clean of it, that’s not your job, but do get the worst of debris and the
like off it.&lt;label class="margin-toggle sidenote-number" for="fn5"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn5" type="checkbox"/&gt;&lt;span class="sidenote"&gt;We have a gas stove and I hate it so much. I dislike gas
in general, but I especially hate how gross and hard to clean it is.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;Now, &lt;em&gt;clean the sink&lt;/em&gt;. Throw out any food caught in the food
traps, give it a general wipe down.&lt;/p&gt;


&lt;p&gt;Finally, &lt;em&gt;sweep the floor&lt;/em&gt;. Doesn’t have to be super thorough,
just get the most obvious debris off the floor.&lt;/p&gt;


&lt;p&gt;And, finally, done. Good job.&lt;/p&gt;


&lt;p&gt;As I said before: Having a process like this really helps, because it
lets you maintain momentum, see clear and visible progress as you go,
and prioritise in a way that makes sure the most important tasks all get
done.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2025-03-23-21:48.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2025-03-24-21:45.html</id>
    <title>Particularity</title>
    <updated>2025-03-24T23:02:31+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Particularity&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2025-03-24&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;&lt;a href="https://notebook.drmaciver.com/posts/2025-03-23-21:48.html"&gt;I
wrote a post about cleaning my kitchen&lt;/a&gt; yesterday. You might have
seen it.&lt;/p&gt;


&lt;p&gt;I posted it on discord saying “this is probably really boring”. I
shouldn’t have done that, it was just random self-deprecation for the
fact that I’d written about such a “mundane” subject, but really I knew
it was a pretty good piece, and people liked it as expected. More
importantly, this is the sort of thing I think we need more of rather
than less, so I shouldn’t put it down.&lt;/p&gt;


&lt;p&gt;One of the things I’ve been worrying about recently is… deskilling I
guess. One of the concerns I have about AI in its current and near
future form is that there are a lot of things where people feel it’s
just no longe rworth doing things pretty well. If you’re bad at art, AI
is probably better at art than you. If you’re bad at writing, an LLM is
probably better at writing than you. Even if you’re pretty good at
these, a lot of the time they can do a good enough job much faster than
you.&lt;/p&gt;


&lt;p&gt;This isn’t a new problem. A lot of progress has been devaluing old
skills because they’ve been replaced with something cheaper. e.g. many
things are no longer worth repairing (or can’t be repaired) and you just
replace them when they break.&lt;/p&gt;


&lt;p&gt;The big problem with this is that being bad at things is how you get
good at things, so this creates a real difficulty where you’re sortof
discouraged from getting good at things because all the initial steps
are taken away from you.&lt;/p&gt;


&lt;p&gt;The thing is… I think this problem is economically real, in that a
lot of demand for this sort of thing &lt;em&gt;is&lt;/em&gt; sortof fungible and
replacing an expensive human with a machine that can cheaply produce
good enough work &lt;em&gt;is&lt;/em&gt; a win for many use cases, but the problem
is also sortof fake in that a lot of people experience it as “what’s the
point of producing things if you’re not already a genius?” and that’s
really not how it works.&lt;/p&gt;


&lt;p&gt;The “How I clean my kitchen” essay is, in a real sense, not possible
for an AI to have written. I don’t mean it couldn’t literally put words
like that on a page. An AI can totally write an essay about how it
cleans its kitchen. &lt;a href="https://claude.site/artifacts/6cb6635f-a7d8-4954-8392-d7c17fa7ec39"&gt;Here’s
one&lt;/a&gt;. I got Claude to write it for me in a couple of minutes of
prompting. It’s sortof overwrote and mawkish and you don’t learn much
about cleaning from it, but it is certainly an essay that one could
write about how to clean a kitchen. &lt;a href="https://claude.site/artifacts/baa1d8f0-3163-4b2b-a3a2-6b81c1488134"&gt;Here’s
another one&lt;/a&gt; where I told it to be a bit more useful. It will create
as many essays about cleaning kitchens for you as you like! If you ask
nicely it might even do one by someone who isn’t a woman named Chen with
a charmingly rustic kitchen.&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Claude has a major mode collapse problem where it’s not
very creative when comig up with characters.&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;A sufficiently advanced version of Claude might even do this in a
psychologically realistic and practically useful way. We can definitely
imagine a world in which Claude is much better at writing essays about
cleaning its kitchen than I am.&lt;/p&gt;


&lt;p&gt;But there is one very important difference between Claude’s version
of the essay and mine: Claude doesn’t have a kitchen, and didn’t get any
better at cleaning it as a result of writing about it.&lt;/p&gt;


&lt;p&gt;Additionally, Claude doesn’t have friends or people who are
interested in its specific ways of relating to the world that can learn
about it and how it is similar and different to them by reading about
its relationship with the kitchen. Claude could have created any
personality you want. &lt;a href="https://claude.site/artifacts/94c552ac-2c51-4b08-9dd6-5fdda565c633"&gt;Here’s
one by a ballerina biker named Magnus “Sledge” Thornton&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;None of these people are real, you can’t have real relationships with
them. As fictional artefacts, they can be interesting,&lt;label class="margin-toggle sidenote-number" for="fn2"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn2" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Although honestly all of these have the flavour of slop
to me, and I sortof cringe and my eyes slide off them as I try to read
them, so I’ve only skimmed any of them.&lt;/span&gt; but
they don’t have a particular human at their center.&lt;/p&gt;


&lt;p&gt;Creation is relational, and it is particular. When you put yourself
in a situation where the value of what you do is interchangeable with
anyone else doing it, of course you end up being devalued by machines.
Economics can force you to be in that position, but that doesn’t mean
that what you do lacks value in and of itself. The fact that you
produced something gives it a value in how it related you to the world -
it changes you to create it, and in being particular it also matters
more to those who are relationship to you, above the artefact
itself.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2025-03-24-21:45.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2025-03-25-20:16.html</id>
    <title>Figuring out what hurts</title>
    <updated>2025-03-25T21:29:41+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Figuring out what hurts&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2025-03-25&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;One of the biggest reasons why the &lt;a href="https://notebook.drmaciver.com/posts/2022-05-30-18:41.html"&gt;victim
of metonymy&lt;/a&gt; concept is important is that it helps you figure out
what you need to vary to fix a problem.&lt;/p&gt;


&lt;p&gt;Something interesting that’s happened to me recently is that I’ve had
several work calls that were 2-3 hours long. Part of why this is
interesting is that they didn’t &lt;em&gt;feel&lt;/em&gt; that long, because we were
pair programming on Tuple. This meant we weren’t looking at each other’s
faces (didn’t even have our webcams on).&lt;/p&gt;


&lt;p&gt;Not that there’s anything wrong with my colleague’s face you
understand. But there’s something about &lt;em&gt;looking at faces on video
calls&lt;/em&gt; that ends up producing something like fatigue&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Your regular reminder that fatigue isn’t a real thing.&lt;/span&gt;. I
have recurring video calls with dear friends and family whose faces I
enjoy seeing very much, and usually my brain is mush an hour in.&lt;/p&gt;


&lt;p&gt;I also have a recurring phone call with a friend, which I usually
take while walking or doing chores, and I have no such mush effect. I’m
reasonably sure the difference is the fact that it’s voice only and my
attention is focused on something rather than someone.&lt;/p&gt;


&lt;p&gt;In constrast, spending time with someone in person has no such
limitations. Part of that is that you don’t actually spend most of your
in person interactions staring directly at someone’s face, you look
around, you look at things together, your gaze wanders. This is normal.
There’s somethign about the fixity of the face on the screen that is a
real problem.&lt;/p&gt;


&lt;p&gt;Video calls seem particularly prone to being treated as a unitary
thing that just has intrinsic problems, even when those problems are
easily fixed or avoided. e.g. back during the height of the pandemic I
did singing lessons online. I’m glad I did it but it was, frankly, a bit
of a miserable experience to do online. My singing teacher clearly
thought so too.&lt;/p&gt;


&lt;p&gt;One of the things that was bad about it was that she insisted on
trying to keep time with me as I sung. It took quite some time of me
explaining in detail how latency worked and how what she was asking for
was literally a violation of the laws of physics&lt;label class="margin-toggle sidenote-number" for="fn2"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn2" type="checkbox"/&gt;&lt;span class="sidenote"&gt;I think this isn’t technically true and we were
physically close enough that a point to point laser comms could have got
latency acceptably low.&lt;/span&gt;
before we figured out better ways to do time keeping using a metronome
app.&lt;/p&gt;


&lt;p&gt;I assume she would have figured this out eventually - we’re now at a
point where &lt;em&gt;most&lt;/em&gt; people have figured out you can’t sing
together on a video call at least - but it was just sortof baffling to
be faced with someone so specifically doing the thing that obviously
wasn’t working and just blaming it on “well video calls suck”.&lt;/p&gt;


&lt;p&gt;Once we’d fixed this and switched to using a local metronome it
worked much better. I still didn’t love doing singing lessons online,
but it was no longer an active exercise in frustration.&lt;/p&gt;


&lt;p&gt;This is the thing about attending to the specific: When you know
what, precisely, it is about it that hurts, you can fix that.&lt;/p&gt;


&lt;p&gt;Not all my calls with friends and family have moved to the less
painful mode, for one reason or another, but I’ve figured out my own
affordances for them. Sometimes I cook while talking, sometimes I play
Slay the Spire during a call, and both of these engage my eyes in a way
that allows me to maintain the actual conversation.&lt;/p&gt;


&lt;p&gt;And sometimes I just put up with it. Looking at my friends and family
has its upsides! But knowing the source of the specific problem gives me
options that I wouldn’t have if I just treated the effect as a
mysterious gestalt effect of video calls.&lt;label class="margin-toggle sidenote-number" for="fn3"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn3" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Not that I actually fully understand why the specific
thing causes this problem, but I don’t need to to work aroudn it.&lt;/span&gt;&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2025-03-25-20:16.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2025-03-26-19:06.html</id>
    <title>Creativity on demand</title>
    <updated>2025-03-26T19:06:00+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Creativity on demand&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2025-03-26&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;One of the things in my bags of tricks is using random prompts for
things. What should I write about today? IDK, draw a tarot card, flip to
a random page of a book. Write something in response to it.&lt;/p&gt;


&lt;p&gt;Another thing in my bag of tricks is making big lists. Can’t think of
anything? Think of 100 things instead! Can’t think how to start the
list? Come up with a bad idea. Or, even worse, a mediocre one.&lt;/p&gt;


&lt;p&gt;Another thing in my bag of tricks is to look at something mundane and
figure out what’s interesting about it. &lt;a href="https://notebook.drmaciver.com/posts/2025-03-23-21:48.html"&gt;Cleaning
the kitchen&lt;/a&gt; is an example of that. &lt;a href="https://drmaciver.substack.com/p/using-what-youre-given"&gt;So is
making apples&lt;/a&gt;. To some degree the entire &lt;a href="https://drmaciver.substack.com/p/my-no-longer-secret-magical-practice"&gt;spellcraft
thing&lt;/a&gt; is about this.&lt;/p&gt;


&lt;p&gt;The point that I’m trying to get at is that with all these tools in
my toolkit, it’s pretty easy for me to solve any drying up of the
creative wellspring. Not only &lt;a href="https://notebook.drmaciver.com/posts/2020-03-07-09:44.html"&gt;can’t
you run out of ideas&lt;/a&gt;, there are reliable procedures for producing
more ideas.&lt;/p&gt;


&lt;p&gt;For example, here off the top of my head is a spell for writing a
blog post at the end of the day:&lt;/p&gt;


&lt;p&gt;Get a sheet of paper. Write down what you did today - not in detail,
just a list of high level things.&lt;/p&gt;


&lt;p&gt;Pick one. If one of them catches your eye, use that, otherwise pick
semi-randomly.&lt;/p&gt;


&lt;p&gt;Try to pick out some specific features of it - things that remind you
of other things, or some way it’s unlike other things in that category.
Especially focus on how you made any decisions about it, or how it
felt.&lt;/p&gt;


&lt;p&gt;Write about that.&lt;/p&gt;


&lt;p&gt;Anyway you’ll notice I’m not using this spell right now. I can’t rule
out that I might in future - I did do the first part of listing out
stuff that happened in my day, and for example I thought I might write
something about some work I’m doing on parsing with derivatives, which I
thought was interesting because of a series of pickles I got myself in
and out of, or I thought I might write about the lunch I made for
myself, in which I took some leftovers from our sunday roast and some
vegetables and turned it into a vaguely Chinese meal.&lt;label class="margin-toggle sidenote-number" for="fn1"&gt;&lt;/label&gt;&lt;input class="margin-toggle" id="fn1" type="checkbox"/&gt;&lt;span class="sidenote"&gt;Fun fact: Left over roast potatoes are great when stir
fried with a bit of ginger.&lt;/span&gt;
There’s also something to be said about the experience of waking up, or
&lt;a href="https://drmaciver.substack.com/p/losing-yourself-in-an-audiobook"&gt;my
changing relationship with audiobooks&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;I didn’t though, partially because I’d already started on this,
partially because I didn’t really feel like it.&lt;/p&gt;


&lt;p&gt;I’m reading a recent translation of Max Weber’s “Science ad a
Vocation” right now, and the following passage struck me:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;In both [a laboratory and a factory] it is necessary for somethign,
and the right thing at that, to &lt;em&gt;occur&lt;/em&gt; to people if they are to
achieve anything worthwhile.&lt;/p&gt;
&lt;p&gt;But inspiration cannot be produced to order. And it has nothing in
common with cold calculation.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;I’ve also recently been reading “The Cult of Information” by Theodore
Roszak, which is surprisingly good, and it has a similar thing in it
about how a computer-focused education with its heavy emphasis on
procedural thinking cannot possibly teach children the full range of
creative and intuitive thought that we need them to develop.&lt;/p&gt;


&lt;p&gt;Anyway, the point is that on some level I think they’re wrong.
Intuitively I believe that I really could write a couple of pages of
instructions that you could semi-mechanically follow and produce a
pretty good essay out of. I’m not claiming you could produce
&lt;em&gt;every&lt;/em&gt; good essay this way you understand, only that it’s
possible to produce a pretty good generator of essays this way, one that
means that you should never have to suffer the problem of sitting down
in front of the computer and not know what to write.&lt;/p&gt;


&lt;p&gt;And yet, every time I think about actually doing this, my soul
rebels.&lt;/p&gt;


&lt;p&gt;Things like the random book prompt or tarot prompt or the like seem
mostly fine. The above spell for coming up with something interesting
about your day to report on also seems mostly fine - maybe a little over
constraining, and requires me to look at uncomfortable questions like
“What &lt;em&gt;did&lt;/em&gt; I do today?” sometimes, but it’s actually an
interesting exercise.&lt;/p&gt;


&lt;p&gt;But if you tell me to, say, roll a die and use it to determine on the
basis of that what to write today, I will fight you.&lt;/p&gt;


&lt;p&gt;If, on the other hand, you tell me that if I’m not sure which one to
write then I &lt;em&gt;get to&lt;/em&gt; roll a dice to pick one, that’s just &lt;a href="https://drmaciver.substack.com/p/how-to-make-easy-decisions"&gt;a
spell for making decisions&lt;/a&gt;. Great stuff, no complaints. Big fan,
even.&lt;/p&gt;


&lt;p&gt;The thing about procedures is that they’re great tools and terrible
masters. The kitchen cleaning procedure is very good because I need to
do the thing and I don’t want to have to make decisions about how to do
the thing. A writing procedure that removes all decisions would be
terrible, because the decisions &lt;em&gt;are&lt;/em&gt; the creative process. I can
be aided in making them, but anything that tries to take the ultimate
authority away from me causes me to bristle.&lt;/p&gt;


&lt;p&gt;And this is sortof a problem for the goal of creativity on demand,
because the problem isn’t actually that I can’t produce creativity on
demand - I can - it’s that I &lt;em&gt;don’t want to&lt;/em&gt;. The whole
experience is one of dragging myself kicking and screaming towards some
goal.&lt;/p&gt;


&lt;p&gt;The idea of a fully mechanized process for writing, even one with
explicitly allowances for breaking out of it and doing your own thing,
is still fundamentally designed to take away the first and most
important decision about creating something: Whether you want to do it
at all. Once that decision is taken away from it, the creative part of
my mind mostly just wants to sit there and sulk and refuse to
participate.&lt;/p&gt;


&lt;p&gt;And you know what? Too bad. Gonna write anyway. When the dishes gotta
be done, you do the dishes whether you like it or not. When the daily
writing gotta be done, you do the daily writing whether you like it or
not. It’s probably not going to be as bad as you think, and if it is &lt;a href="https://drmaciver.substack.com/p/believing-that-you-can-stop"&gt;you
can stop&lt;/a&gt;, but that’s no reason not to start.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2025-03-26-19:06.html" rel="alternate"/>
  </entry>
</feed>
