<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://notebook.drmaciver.com/</id>
  <title>DRMacIver's notebook</title>
  <updated>2020-07-16T09:22:00+01:00</updated>
  <author>
    <name>David R. MacIver</name>
    <email>david@drmaciver.com</email>
  </author>
  <link href="https://notebook.drmaciver.com" rel="alternate"/>
  <link href="https://notebook.drmaciver.com/feed.xml" rel="self"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-03-21:03.html</id>
    <title>Death of the Reader</title>
    <updated>2020-07-03T21:15:21+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Death of the Reader&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-03&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/3ekINM9"&gt;Clear and simple as the truth&lt;/a&gt; by Thomas and Turner, page 138:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Word for word, Gallanda's version is the worst written, the most fraudulent and the weakest, but it was the most widely read. [...] Its orientalism, which we now find tame, dazzled the sort of person who inhaled snuff and plotted tragedies in five acts. [...] We, mere anachronistic readers of the twentieth century, perceive in these volumes the cloyingly sweet taste of the eighteenth century and not the evanescent oriental aroma that two hundred years ago was their innovation and their glory. No one is to blame for this missed encounter, least of all Galland.&lt;/p&gt;
&lt;p&gt;Jorge Luis Borges, "Los traductores de las 1001 noches" [The translators of the 1001 nights]&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;I wrote before about the difficulty of &lt;a href="https://notebook.drmaciver.com/posts/2020-06-20-15:10.html"&gt;understanding historical fact from historical records&lt;/a&gt;. This, I think, highlights another problem for understanding history: As modern readers, it is extremely difficult for us to understand how a text would have been read at the time.&lt;/p&gt;


&lt;p&gt;I've encountered this problem with reading historical mathematics. It is very hard to read historical mathematics, because one tends to read them through the lens of modern mathematics. For example, reading texts from prior to the invention of modern mathematical notation, it's very easy to think of it in terms of its modern representation.&lt;/p&gt;


&lt;p&gt;The thing is, that modern representation makes certain things very obvious that were not at all obvious to historical mathematicians.&lt;/p&gt;


&lt;p&gt;In general, it is very hard to read a text without looking at it through the lens of culture that we have built since that text was written. That culture gives us ways of looking, and thinking, and feeling, about the contents of the text that make it impossible for us to read it as it would have been read at the time.&lt;/p&gt;


&lt;p&gt;Sometimes that culture is even informed by the text. For example, I found reading 1984 very tedious, because I found its contents obvious and predictable, but a lot of why they are obvious and predictable is that everybody has read 1984.&lt;/p&gt;


&lt;p&gt;I am reminded of people who read Lord of the Rings and find it laughable because all of the characters in it are fantasy cliches.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-03-21:03.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-05-11:29.html</id>
    <title>Getting Test-Case Reduction Unstuck with Automaton Inference</title>
    <updated>2020-07-09T13:47:06+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Getting Test-Case Reduction Unstuck with Automaton Inference&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-05&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;A question I get asked a lot by friends is why I don't talk about my PhD much.
The answer is generally very simple: It's because almost nobody actually wants to know about my PhD, and it's not worth the half hour of bringing them up to speed while they politely pretend that they do.&lt;/p&gt;


&lt;p&gt;But, on the other hand, (a small handful of) people do generally seem interested when I talk about my technical work on Twitter, and this notebook is where I get to write about whatever the hell I want, and part of &lt;a href="https://drmaciver.substack.com/p/why-am-i-not-working-on-my-phd"&gt;the plan to sort out my PhD&lt;/a&gt; is to do some more non-paper research writing.&lt;/p&gt;


&lt;p&gt;So today's post is about my research. Sorry for everyone who is here mainly for feelings and social commentary.&lt;/p&gt;


&lt;p&gt;Context: I work on a thing called test-case reduction. Test-case reduction is basically "just" constrained optimisation of a total order (the reduction order), over some set (the test cases), where the constraint is just a black box predicate you've been handed (the interestingness test), and you've been handed a single witness of the set (the initial test case).&lt;/p&gt;


&lt;p&gt;For reasons that are explained in &lt;a href="https://drmaciver.github.io/papers/reduction-via-generation-preview.pdf"&gt;our paper about Hypothesis&lt;/a&gt; I'm particularly interested in the case where the test cases are all byte strings (we present it as bit strings in the paper, but same deal really) where the order is the shortlex order. i.e. \(s \preceq t\) if \(|s| &amp;lt; |t|\) or \(|s| = |t|\) and \(s \leq t\) in the lexicographic order.&lt;/p&gt;


&lt;p&gt;There is an idea that I've had in the back of my head for a while:&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Given some interestingness test \(p\) and some starting test case \(s\), the set \(\{t: p(t) \wedge t \preceq s\}\) is finite and thus a regular language.&lt;/li&gt;
&lt;li&gt;If we had a deterministic finite automaton for that language, we could just read off the shortlex-minimal string matched by that automaton using a breadth-first search, giving us the global minimum interesting test case.&lt;/li&gt;
&lt;li&gt;We can infer (an approximation to) that automaton using &lt;a href="https://www.sciencedirect.com/science/article/pii/0890540187900526"&gt;the L* algorithm&lt;/a&gt; (or see &lt;a href="https://www.youtube.com/watch?v=zlHck7X4F20"&gt;my PyCon UK talk on the subject&lt;/a&gt; for a less technical account).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This suggests a great approach to test-case reduction: We set up the L* algorithm, make sure it gets the correct result on our initial test case, read off the minimal matching string of the automaton, check if that's correct and if not pdate the automaton, and iterate that until the automaton-minimal string is also interesting.&lt;/p&gt;


&lt;p&gt;This idea is genuinely very good except for two small problems.&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;It's prohibitively slow.&lt;/li&gt;
&lt;li&gt;It doesn't work.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;It's prohibitively slow because the complexity of L* is weird and depends a lot on the language being learned, but in practice you're lucky if it's only \(O(n^2)\) when you use it this way. It doesn't work because although the language is regular it doesn't have a particularly nice structure a lot of the time and the algorithm isn't always able to pick up on important details as a result. For example, L* does not easily learn the constraint that two bytes can be equal but it doesn't matter what value they take beyond that.&lt;/p&gt;


&lt;p&gt;This is all very unfortunate because it's a lovely idea.
Fortunately, as I discovered recently, there is actually a context in which L* can potentially be a significant boon to test-case reduction after all: You can use it offline to learn new reduction passes (a reduction pass is basically just a simple test-case reducer, but effective test-case reducers tend to be assembled out of multiple reduction passes) that are &lt;em&gt;not&lt;/em&gt; prohibitively slow and correspond to established deficiencies in your existing reducer.&lt;/p&gt;


&lt;p&gt;We can find deficiencies in a reducer by basically looking for ways it gets "stuck". This is based on an idea called &lt;a href="https://agroce.github.io/issta17.pdf"&gt;test case normalization&lt;/a&gt; which is essentially that the goal of a reducer should be that its output does not depend on the initial test case, only the interestingness test.&lt;/p&gt;


&lt;p&gt;If you have some interestingess test that demonstrable does not normalize on an initial corpus of test cases (in my case these are generated by Hypothesis) you can learn a set of new reduction passes that normalizes that corpus. You do this by picking the two smallest examples, adding a reduction pass that turns the larger of the two into the smaller, and rerunning until all of the corpus is normalized (which takes at most one pass per corpus element).&lt;/p&gt;


&lt;p&gt;Suppose \(s \preceq t\) are the two smallest elements of that corpus after reduction. There is a trivial reduction pass that just replaces the string \(t\) with the string \(s\) wherever it finds it. This is actually quite good, because after reduction (assuming the initial reducer is doing an OKish job of normalizing) you're much more likely to see \(t\) than chance would suggest. You can improve these odds further by removing a common prefix and suffix from each of \(s\) and \(t\).&lt;/p&gt;


&lt;p&gt;But you can also generalise this further. Suppose we've extracted those common prefixes, say \(u\) and \(v\).
We now use L* to learn the predicate \(f(x) = |uxv| \leq |t| \wedge p(uxv)\), making sure that it learns \(s\) correctly. i.e. we learn a smallish language that captures just enough information to turn the central bit of \(t\) into the central bit of \(s\).&lt;/p&gt;


&lt;p&gt;We now use this language to define a new reduction pass which finds any substrings that match this DFA and replace them with the smallest element of it. By making sure L* has learned enough about \(s\) and \(t\) to correctly predict them, this ensures that the new pass can transform \(t\) into \(s\).&lt;/p&gt;


&lt;p&gt;At a high level what this is doing is essentially learning patterns where your existing reducer gets "stuck" and learn just enough about the linguistic structure of the test case format to work around them.&lt;/p&gt;


&lt;p&gt;Does this work?&lt;/p&gt;


&lt;p&gt;Well, maybe.&lt;/p&gt;


&lt;p&gt;There are roughly two ways this can go wrong:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;The DFA may still be impractical to learn.&lt;/li&gt;
&lt;li&gt;The learned passes may not generalise well enough to be useful, so this ends up with a very large number of passes.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Initial experiments on Hypothesis are pretty promising as to whether the DFAs can be learned, but the Hypothesis test-case format is very forgiving. I'll be surprised if this works well enough out of the box on human-readable test-case formats and I expect it will need more tinkering. In particular when the reduced test-cases in the corpus are far apart, I think one needs some intermediate steps to learn smaller transformations first (I have some ideas for how to use &lt;a href="https://en.wikipedia.org/wiki/A*_search_algorithm"&gt;A*&lt;/a&gt; to do this. I do know algorithms that aren't just a letter and an asterisk, I promise).&lt;/p&gt;


&lt;p&gt;The number of learned passes is potentially more of a problem, but conveniently I was &lt;a href="https://github.com/HypothesisWorks/hypothesis/pull/2478"&gt;already working&lt;/a&gt; on an approach to make it practical to run test-case reduction with a large number of mostly useless passes.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-05-11:29.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-06-09:52.html</id>
    <title>Indexing a DFA in shortlex order</title>
    <updated>2020-07-06T10:56:10+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Indexing a DFA in shortlex order&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-06&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;Bad news feelings-readers, it's another technical post. Having feelings will resume shortly (probably in tomorrow's newsletter if nothing else)&lt;/p&gt;


&lt;p&gt;I &lt;a href="https://twitter.com/DRMacIver/status/1278986162443636736"&gt;asked the other day&lt;/a&gt; about how to find the shortlex-predecessor of a string in a regular language represented by a DFA. I eventually came up with &lt;a href="https://gist.github.com/DRMacIver/89a7a27b70bbb795748fd20d1ad50f82"&gt;a solution that wasn't too bad&lt;/a&gt;.
This morning in the shower I came up with a better one (disclaimer: I haven't written code for this or tested it, so it's probably subtly wrong and has off by one errors, and it may perform poorly, but the basic idea is sound in theory).&lt;/p&gt;


&lt;p&gt;The basic idea is this: It's well known that using breadth first search we can enumerate the regular language in shortlex-ascending order as \(x_0, \ldots, x_n, \ldots\). Finding the predecessor of \(s\) given this enumeration is as simple as looking up \(s = x_n\) and returning \(x_{n - 1}\). Unfortunately, \(n\) may be exponentially large in \(|s|\) so doing the enumeration is horribly impractical.&lt;/p&gt;


&lt;p&gt;But! It turns out, you don't have to, and this solution can be made workable (given a good bigint library to represent the indices) as follows:&lt;/p&gt;


&lt;p&gt;Maintain a dynamic programming table \(C(i, k)\) which counts the number of accepted strings of length \(k\) starting from state \(i\) in the DFA. Also maintain a dynamic programming table \(R(i, k) = \sum\limits_{j = 0}^k C(i, j)\) (i.e. the number of strings of length at most \(k\) accepted from state \(i\)).&lt;/p&gt;


&lt;p&gt;What this table lets you do is essentially skip over the bits of the breadth first search that you know won't lead to where you're going, and count how many strings you skipped by doing so.&lt;/p&gt;


&lt;p&gt;Now, you look up \(x_n\) by first comparing values of \(R(0, k)\) to find \(k\) with \(R(0, k) &amp;lt; n \leq R(0, k + 1)\).
We know \(k = |x_n|\). Now, walk the DFA, using \(C(i, k)\) to determine which node to transition to (using essentially the same idea - this is easier to explain in code, but I haven't written the code yet!).&lt;/p&gt;


&lt;p&gt;Now, to find \(x_n = s\) we just count the number of strings in the langugage which are shortlex smaller than \(s\).
This can be established relatively easily: First count \(R(0, |s| - 1)\), all strings that are shorter than \(s\), and then work out the number of strings of length \(|s|\) that are lexicographically smaller than \(s\) by adding up sum of the strings of length \(k - i\) by walking the DFA and after reading \(i\) characters count the number of strings of length \(|s| - i - 1\) that you would get from following each \(c &amp;lt; s_i\).&lt;/p&gt;


&lt;p&gt;To be honest, I think this explanation was all clear as mud, sorry. I think either you'll get the idea immediately from the initial seed of "Use dynamic programming to get the indices" or the explanation won't clarify it, and I should have written the code instead of trying to explain this in prose. Alternatively, this is a sign that I really need to get much better at explaining this sort of thing in prose. &lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-06-09:52.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-07-22:30.html</id>
    <title>The Possibility and the Actuality of Change</title>
    <updated>2020-07-07T22:36:13+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;The Possibility and the Actuality of Change&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-07&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;(A very short post, because I've been struggling to maintain the daily writing habit recently and it's useful to remind myself that this is possible)&lt;/p&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/2ZCqSNc"&gt;A Little Book on the Human Shadow&lt;/a&gt; by Robert Bly, page 77:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Wallace Stevens was not willing to change his way of life, despite all the gifts he received, and all the advice he read in his own poems.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;This is, I think, a useful reminder to anyone who wants to change their life, but it's especially a useful reminder to me and others who have a perhaps overly intellectual approach to the problem. I'm building a practice of change on a huge amount of reading and writing, which can lead to the trap of confusing the knowledge of how to change with the change itself.&lt;/p&gt;


&lt;p&gt;Much of what I write is, if I do say so myself, very good advice to how to change and improve your life (especially if your problems look like mine). I'm not always that great at following it - I'm no Wallace Stevens (though I do not know if Bly's criticisms of him are at all fair) - I do listen to my own advice, and I do follow it, but perhaps not as consistently as I might.&lt;/p&gt;


&lt;p&gt;Knowing how and what to change is important - vital even - but change has to be grounded in a willingness to change, and to then do the work of changing.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-07-22:30.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-08-14:56.html</id>
    <title>Precarity and Conformity</title>
    <updated>2020-07-08T16:41:55+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Precarity and Conformity&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-08&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/36wKnYH"&gt;Rewriting the Rules&lt;/a&gt; by Meg-John Barker, page 94:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;We've seen throughout this book that rigid rules are generally bad both for those who fall outside them &lt;em&gt;and&lt;/em&gt; for those who fit within them.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;A thing I find myself thinking about a lot is &lt;em&gt;precarity&lt;/em&gt;: Specifically, the scenario where nothing is wrong &lt;em&gt;now&lt;/em&gt;, but you are well aware that this could change at any time.&lt;/p&gt;


&lt;p&gt;This is I think different from a lot of cases where we think of precarity as being associated with things being bad in the moment. e.g. poverty is a precarious situation, because you can't absorb sudden large financial blows, but poverty is also quite bad in the moment.&lt;/p&gt;


&lt;p&gt;Many privileged states however have a form of precarity to them. This often goes hand in hand with privileges. For example, being able-bodied is a precarious situation - there are no invulnerable bodies, and you could lose your status as able-bodied at any time.&lt;/p&gt;


&lt;p&gt;The reality, of course, is that &lt;em&gt;all&lt;/em&gt; situations are precarious. Just as there are no invulnerable bodies, there are no invulnerable lives. Everything can be lost, and we don't like to think about that.&lt;/p&gt;


&lt;p&gt;But although everything is precarious, some situations are significantly more precarious than others.&lt;/p&gt;


&lt;p&gt;One particular source of precarity is attached to changes of worldview. What if you are wrong about something major? It does happen. What if, in particular, you were wrong about some major source of ethics or value?&lt;/p&gt;


&lt;p&gt;I think conformity to the rules often plays out this way: For people who fit the rules, their self-worth is often tied to the fact that very fact of conformity. This makes it precarious in several major ways:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;The rules can change.&lt;/li&gt;
&lt;li&gt;You can change.&lt;/li&gt;
&lt;li&gt;You can be deemed not "really" conforming.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The third is particularly scary because actually all social rules are fake and the reality is a much more messy interplay between expectation and social power, so the only thing that is needed to be declared nonconforming is for someone with more social clout than you to want to declare you so. All conformity can buy you is a certain measure of defence against that attack.&lt;/p&gt;


&lt;p&gt;The problem is that as long as you are using conformity as your measure of self-worth, or even just as a way to make your life easier, you &lt;em&gt;really&lt;/em&gt; don't want to be subject to that attack, or otherwise become nonconforming, and so are willing to sink in a lot of effort to reduce your precarity. Because conformance is central to your experience, that precarity feels central, so you end up sinking a lot of time and effort into conforming to the rules, and feeling anxious because you can never reduce the precarity to zero.&lt;/p&gt;


&lt;p&gt;In contrast, nonconformity is often more robust. It's not necessarily &lt;em&gt;better&lt;/em&gt; - often the punishment for nonconformance is quite high - but because you don't have to worry about losing your privileged status you can devote your energies to other things. The bad thing you were worried about has already happened, and there's not much you can do about it other than get on with life.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-08-14:56.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-10-16:52.html</id>
    <title>Learning to use the system</title>
    <updated>2020-07-10T17:28:06+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Learning to use the system&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-10&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/2WYeYew"&gt;Talking About Machines&lt;/a&gt; by Julian Orr, page 48:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;These same users, however, cannot be bothered to learn how they are supposed to use the machines. They claim they do not have time, he says. They seem to have plenty of time to stand around and bitch, though. [...] He finds this very frustrating, and the customers do not respond to his frustration.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;In context, a technician is frustrated that he is constantly having to fix problems caused by the users' improper use of the machines he maintains. If the users learned how to use the machines properly, which they refuse to do, the machines wouldn't be broken so often.&lt;/p&gt;


&lt;p&gt;I share both the technician's irritation and the user's reluctance to learn: The limitation is probably not time but some mix of learned helplessness (believing that it's too difficult) and managerial unwilling to budget time and money for the training needed. On the other hand, the users could certainly stand to acquire some &lt;a href="https://www.drmaciver.com/2015/02/what-are-developer-adjacent-skills/"&gt;adjacent skills&lt;/a&gt; to make the technician's life easier. Every time you encounter a problem it's worth asking the person responsible for fixing it a little bit about what caused it and what you could have done to avoid that. A lot of the time you'll forget or not be able to implement the answer, but over time you might pick up enough by osmosis to make both of your lives better.&lt;/p&gt;


&lt;p&gt;On the other hand, the technician is probably underestimating just how hard it is to use the system without breaking it.
People are &lt;a href="https://blog.regehr.org/archives/861"&gt;operant conditioned by the systems they use&lt;/a&gt; into using them in a way that mostly doesn't break, and experts are the ones who have experienced the most of this, so probably experience the system very differently than the users do.&lt;/p&gt;


&lt;p&gt;But even experience users sometimes still seem to use the system in ways that break it.
I think part of what's going on here is that operant conditioning only works when you have the ability to learn,
and many people never acquire enough of a working model of the system to be able to learn.
If you don't know what it was you did that caused the bad result, how can you learn to avoid that?
(This is why it's important to ask questions).&lt;/p&gt;


&lt;p&gt;If we want to learn to use a system, part of that is speeding up this process of operant conditioning - learning what's safe, and what to avoid.
Having an adequate mental model of the system seems to be a key part of that, because it lets you figure out this mapping of action to outcome.&lt;/p&gt;


&lt;p&gt;I notice this a lot with git. There's a really huge difference in how people experience git, and it seems to largely come down to whether they have a good working model of it. git is a horrible mess of weird multipurpose arbitrary commands, and it has a number of sharp edges, but honestly for me it's basically fine and it causes me almost no problems, and it has been like that almost from day one, because I roughly know how it works and so can think of things in terms of what I'm trying to achieve in the underlying model. Without that model, you only see the external complexity, which is almost impossible to navigate.&lt;/p&gt;


&lt;p&gt;Another place I've seen this is mathematics education. People try to teach maths as a bunch of facts that you have to memorise. This is basically impossible and nobody will ever learn to be a mathematician that way - they will either learn to be a mathematician on their own, they will brute force their way through some exams and then forget everything, or they will fail horribly, because without an underlying understanding of the nature of mathematics and how everything fits together it's just an unreasonable amount of pointless rules and facts to memorise and you will experience all of it as arbitrary.&lt;/p&gt;


&lt;p&gt;People often seem very resistant to acquiring models of the systems they use, because that seems hard and time consuming, but the reality is that trying to use a system without some sort of model of it is much harder and will be a source of constant and bewildering suffering.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-10-16:52.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-11-10:49.html</id>
    <title>Automatic Boltzmann Sampling for Context Free Grammars</title>
    <updated>2020-07-11T11:53:58+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Automatic Boltzmann Sampling for Context Free Grammars&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-11&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;A Boltzmann sampler of parameter \(x\) for some class of combinatorial objects (e.g. strings in some language) is a random sampler that picks an object of size \(n\) with probability proportional to \(x^n\).&lt;/p&gt;


&lt;p&gt;&lt;a href="https://www.drmaciver.com/2017/03/fully-automated-luxury-boltzmann-sampling-for-regular-languages/"&gt;Ages ago&lt;/a&gt; I figured out how to do Boltzmann Sampling for arbitrary regular languages, by computing the deterministic finite automaton for the language and doing some linear algebra.&lt;/p&gt;


&lt;p&gt;Its major limitation was basically that it required computing the whole deterministic finite automaton, which may be exponentially large. Implicitly this also meant that it doesn't work on anything &lt;em&gt;other&lt;/em&gt; than regular languages, because you have to have finitely many states.&lt;/p&gt;


&lt;p&gt;Anyway, shower thought for this morning is that I can lift both these restrictions and automatically derive a Boltzmann sampler for any context free language, as long as \(|x| &amp;lt; \frac{1}{|A|}\) (where \(x\) is the Boltzmann parameter and \(A\) is the alphabet of the language). This condition isn't too onerous and is necessary to guarantee that a Boltzmann sampler exists for any language over that alphabet, though some languages may have Boltzmann samplers with larger parameter values.&lt;/p&gt;


&lt;p&gt;The technique is actually very simple: Much simpler than the previous one in some ways, although partly because the complexity lives in black boxes.&lt;/p&gt;


&lt;p&gt;It works by breaking it down into two parts:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;Pick the size of the target string according to the right distribution.&lt;/li&gt;
&lt;li&gt;Sample uniformly among strings of that size.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So all we need to do is figure out how to do these things.&lt;/p&gt;


&lt;p&gt;In order to do the first, let \(c(n)\) be the number of strings in the language of length \(n\).
Necessarily \(c(n) \leq |A|^n\) because there are at most \(|A|^n\) strings of length \(n\), let alone in the language.&lt;/p&gt;


&lt;p&gt;The Boltzmann sampler distribution is that we pick a string of length \(n\) with probability proportional to \(c(n) x^n\).&lt;/p&gt;


&lt;p&gt;We can thus pick the length as follows: Pick a real number \(z\) uniformly at random in the region \([0, B(x)]\), where \(B(x) = \sum c(n) x^n\) and choose the first \(n\) such that \(\sum\limits_{k=0}^n c(k) x^k \geq z\).&lt;/p&gt;


&lt;p&gt;Unfortunately we don't know this infinite sum. You can calculate it relatively easy for some classes of language, but if the grammar for your language is ambiguous that gets harder.&lt;/p&gt;


&lt;p&gt;However! We don't actually need to do that at all, because the restriction on the parameter choice allows us to create increasingly good upper bounds on this sum as follows. Because we know that \(c(n) \leq |A|^n\) we know that \(\sum\limits_{k = n}^\infty c(k) x^k \leq \frac{(|A|x)^n}{1 - |A| x}\). Thus we can define sequences \(L_n = \sum\limits_{k \leq n} c(k) x^k \) \(U_n = \frac{(|A| x)^{n + 1}}{1 - |A|x} + L_n\). These have the property that \(L_n \leq B(x) \leq U_n\), the \(L_n\) are monotonically decreasing, the \(U_n\) are monotonically decreasing, and they both converge to \(B(x)\).&lt;/p&gt;


&lt;p&gt;We can now sample the size as follows:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;Maintain a value \(m\) which we have evaluated \(L_m, U_m\) up to.&lt;/li&gt;
&lt;li&gt;Pick a real number \(z\) uniformly in \([0, U_m]\).&lt;/li&gt;
&lt;li&gt;If \(z \leq L_m\) then as before find and return the first \(n\) such that \(L_n \leq z\). Necessarily \(n \leq m\).&lt;/li&gt;
&lt;li&gt;Otherwise, find \(m' &amp;gt; m\) such that either \(z \leq L_{m'}\) or \(U_{m'} &amp;lt; z\).&lt;ol&gt;
&lt;li&gt;Set \(m = m'\).&lt;/li&gt;
&lt;li&gt;If \(z \leq m'\) then pick \(n\) as in (3) and stop.&lt;/li&gt;
&lt;li&gt;Go back to (2), drawing a new value of \(z\) based on our new \(m\).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;(This is a relatively well known technique for values of "relatively well known" that mean "if you work in a very niche field you've definitely heard about it" but I think I independently reinvented it a while back because I don't work in that very niche field so hadn't heard about it)&lt;/p&gt;


&lt;p&gt;So that's how we pick the size. Now we are left with two problems:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;How do we calculate \(c(n)\)?&lt;/li&gt;
&lt;li&gt;How do we pick uniformly among values of size \(n\)?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The answer to both is that we use the things I've been writing about recently!&lt;/p&gt;


&lt;p&gt;First, note that we can turn any context-free grammar into a deterministic &lt;em&gt;infinite&lt;/em&gt; automaton by using language derivatives, as per the &lt;a href="http://matt.might.net/articles/parsing-with-derivatives/"&gt;Parsing with Derivatives&lt;/a&gt; approach: States are labelled by a derivative of the initial grammar, and are created lazily on demand as you traverse the automaton.&lt;/p&gt;


&lt;p&gt;You can now from this automaton use the same techniques I described in &lt;a href="https://notebook.drmaciver.com/posts/2020-07-06-09:52.html"&gt;Indexing a DFA in shortlex order&lt;/a&gt;, which work equally well on an infinite automaton (they only ever require looking a finite depth away from a given state).
This means we can easily calculate \(c(n)\) using dynamic programming as before, and we can sample uniformly from the strings of length \(c(n)\) using the same technique we used to index: Pick a random integer \(i\) in the range \([0, c(n) - 1]\) and then walk the DFA the i'th string of length \(n\) in lexicographic order.&lt;/p&gt;


&lt;p&gt;One potential issue with the conversion to a DFA that this neatly avoids is that you can run into cases with context free languages where it is undecidable if the grammar contains any strings. Fortunately, for random sampling this doesn't matter! Because long strings are increasingly unlikely, there is (probabilistically speaking) no observable difference between a state that has no strings starting from it and a state that only has ridiculously long strings.&lt;/p&gt;


&lt;p&gt;How practical is this? I don't know exactly, but my suspicion is that for a lot of simple grammars it will work very well. I may attempt to put together an implementation soon.&lt;/p&gt;


&lt;p&gt;PS. It's likely that the parameter restriction can be lifted by making more precise estimates of the tail sums using the automaton, but I haven't quite figured out how yet. For my purposes it's generally a relatively mild restriction so I'm not that worried about it.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-11-10:49.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-14-13:42.html</id>
    <title>Living Room Rules</title>
    <updated>2020-07-14T16:27:56+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Living Room Rules&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-14&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://twitter.com/DRMacIver/status/1281131593948692480"&gt;my Twitter the other day&lt;/a&gt;:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;A pattern I notice over and over again: you don't have to get all of a community's norms right from day one, but you do have to get a community's norms of agreeing on community norms right from day one or your community is doomed to fall apart, probably messily.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;One of the ways in which I have seen this manifest is tech communities and the adoption of code of conducts.
Tech communities that don't have codes of conduct from very early on tend to have vicious battles when it comes to adopting them, with a lot of the existing members of the community absolutely livid about the idea that they should be held to specific rules of behaviour.
This is what happens if you build a community without building legitimacy for a mode of forming norms.&lt;/p&gt;


&lt;p&gt;One way to solve this is to have a code of conduct from the beginning, and it may be worth doing this anyway (especially a generic "no racism, transphobia, etc." one), but I think using this as the main solution sets the bar too low, and there is an easier and more flexible approach that will build a healthy and functional community:
Simply declare "Le code de conduite, c'est moi". This is your space, as the founder of the community, and the people there are your guests. They should behave accordingly, and if they are not prepared to behave accordingly they are welcome to leave.&lt;/p&gt;


&lt;p&gt;These are what I call &lt;em&gt;living room rules&lt;/em&gt; - you are treating the space as your virtual living room. As the host you are responsible for people having a good time, and the guests are responsible for behaving well in your space.&lt;/p&gt;


&lt;p&gt;Living room rules work best for small communities whose management is well within the scope of a single individual.
The reason for this is that as a single individual founder you have authority over the space, and thus over how people behave within it. Your word is law. Once authority is delegated to multiple people, the possibility of conflict exists, and the authority is weakened (this can, of course, be a good thing too).&lt;/p&gt;


&lt;p&gt;A Code of Conduct is essentially a tool you can use for claiming authority over people's behaviour without possessing that authority on your own.
You, a member of the community or one of many organisers, have the ability to say "This behaviour is not on, because it says so in the code of conduct". By making the rules of the space explicit, authority to get people to conform to those rules is conferred.&lt;/p&gt;


&lt;p&gt;But you can just skip that step and claim authority on your own. It's allowed, I promise. You do this every time you have your friends around for dinner (a thing that we used to do prior to 2020) and it generally works fine.&lt;/p&gt;


&lt;p&gt;Why might you want to do this?&lt;/p&gt;


&lt;p&gt;Well, it's more flexible and produces better results. The goal is not to get people to follow some set of rules to the letter, the goal is to get a community that behaves in a positive way. Explicit systems of rules are easy to rules lawyer, and it's hard for them to account for every scenario. That doesn't mean a code of conduct is bad in and of itself - they're on balance probably good things - but they are &lt;em&gt;insufficient&lt;/em&gt; and a community where people are constantly skirting the edge of the code of conduct is one that you might as well let fail because it's basically dead anyway.&lt;/p&gt;


&lt;p&gt;In contrast, when a community is small enough that you can have it overseen by a single person, that person is able to use their judgement and shape the behaviour of the community into more positive directions, and it's much harder to rules lawyer against individual personal judgement.&lt;/p&gt;


&lt;p&gt;There are a couple reasons people don't want to do this:&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Claiming authority is kinda scary, at least if you're the sort of person worth trusting with authority. "Because I say so" is a harder thing to say than "because these reasons", even if you are the one who defined those reasons.&lt;/li&gt;
&lt;li&gt;Some people are going to try hard to argue with and manipulate you to get the outcomes they want, and then you have to go through the social awkwardness of showing that person the door.&lt;/li&gt;
&lt;li&gt;It's hard to do well, and it feels like a personal judgement when you don't do it well.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I'm still figuring out the details of how to do it well myself, but I think the following principles are roughly what I'm trying to follow:&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Claim this authority explicitly and early.&lt;/li&gt;
&lt;li&gt;Be explicit about the sort of space that you are trying to achieve. "This is what I want out of this space, though details may be subject to change, and I am the arbiter of whether that is being achieved."&lt;/li&gt;
&lt;li&gt;Don't be an asshole with your authority (obviously), but more than that do your best to be someone who people are glad has authority. Your role here is to be the host, and to use the authority that gives you to make sure everyone has a good time.&lt;/li&gt;
&lt;li&gt;When you make decisions, record them somewhere. This can evolve into a document that you can use to better define and shape your community norms. Eventually it might acquire authority in its own right, once the community starts to outgrow you.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I've run a few online spaces more or less on these principles. The most explicit is my recent creation of the Weird CS Theory Community Discord, which started with me creating &lt;a href="https://docs.google.com/document/d/1y3DkPGuAZiGbeNYlwje6jTlFxz9peIGnjSuZTwF4NGM/edit?usp=sharing"&gt;a document based on these principles fairly early on&lt;/a&gt;. Hard to say how it's going, because it's early days yet, but we've mostly got the vibe I want and the one incident which promtped the creation of the document was entirely civil even before I intervened, and the intervention was well received.&lt;/p&gt;


&lt;p&gt;Should all online spaces be run this way? Absolutely not. Not even all small ones. But I do think there should be more online spaces run this way, and that many more people would benefit from running one than currently do.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-14-13:42.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-15-09:33.html</id>
    <title>Nice Problems to Have</title>
    <updated>2020-07-15T09:48:35+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Nice Problems to Have&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-15&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I've been thinking about "nice problems to have" as a phenomenon recently. I have a feeling they're widely misunderstood.&lt;/p&gt;


&lt;p&gt;My standard response to the concept has often been "Yes but nice problems to have are still problems", but actually I think it goes deeper than that: Nice problems to have are also rarely actually nice to have. The phrase is misleading.&lt;/p&gt;


&lt;p&gt;What we generally mean when we say "nice problem to have" is that the &lt;em&gt;preconditions&lt;/em&gt; for having this problem are nice to have. i.e. you cannot have this problem unless some other nice thing happens first.
This is not at all a reliable predictor of the amount of suffering involved.&lt;/p&gt;


&lt;p&gt;Consider, for example, having all your friends and family constantly making demands on you, far beyond your capacity to satisfy, to the point where it would be emotionally exhausting even if you could say yes to everyone but also you have to say no to most of them and they will resent you for it.
This is obviously an intensely painful problem, right?&lt;/p&gt;


&lt;p&gt;Now suppose it's because you come from poverty and have landed a well paid job. Suddenly it's a "nice problem to have" (you have it because you're being well paid! Being well paid is good!), but that doesn't change how miserable it is.&lt;/p&gt;


&lt;p&gt;The thing is, by definition, "nice problems to have" are ones that are not going to be shared by a lot of people (otherwise the preconditions would just be normal behaviour and it wouldn't be a nice problem to have).
As a result:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;Most people probably won't be able to relate to it, because their lived experiences will be so different.&lt;/li&gt;
&lt;li&gt;People will be envious of you for the nice thing.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Thus the defining characteristic about nice problems to have is not that they are nice, but that you will be afforded no sympathy for having them. Nice problems to have are the opposite of nice: They isolate you from the ability to &lt;a href="https://drmaciver.substack.com/p/you-should-complain-about-it"&gt;complain about them&lt;/a&gt;, which removes a major bonding activity with people who have not been successful in the same way as you, and also causes you to feel worse about the problems with those who do not share your burden.&lt;/p&gt;


&lt;p&gt;There is a phenomenon of people succeeding and leaving their former friends behind, and there are many very bad reasons why people do this, but I suspect the nice problems to have issue is one of them: None of their former friends are able to be remotely sympathetic to things that are very real sources of their suffering (and indeed that unrelatability of it is probably &lt;em&gt;part&lt;/em&gt; of their suffering).&lt;/p&gt;


&lt;p&gt;This is in many ways a variant of the &lt;a href="https://drmaciver.substack.com/p/being-deep-in-an-abstraction-stack"&gt;abstraction stack&lt;/a&gt; problem - you need people around you who just &lt;em&gt;get&lt;/em&gt; the things you want to talk about - and the solution is probably the same: Regardless of who else you hang out with (and you should hang out with a variety of people) you do need &lt;em&gt;some&lt;/em&gt; people you can talk to who understand your problems, even if they're supposedly nice problems to have.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-15-09:33.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-16-09:22.html</id>
    <title>Habit Overload</title>
    <updated>2020-07-16T09:22:00+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Habit Overload&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-16&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;Attention Conservation Notice: Personal debugging. May be of interest if you have similar problems, which you probably do, but I've not made any particular effort to generalise it into a coherent point.&lt;/p&gt;


&lt;p&gt;I've been noticing recently that a lot of my recurring habits have been slipping,
so I'm attempting to take stock of this and try to figure out what to do about it.&lt;/p&gt;


&lt;p&gt;I think the problem has a couple of roots:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;I'm trying to do too much. There's plenty of time to do all the habits if I do nothing else, and even if I do a reasonable amount of work on my PhD, but the reality is that I don't do a reasonable amount of work on my PhD because my two modes are "not really doing enough work on it" and "total all consuming obsession", and the latter tends to impinge on the habits and make me feel bad about neglecting them.&lt;/li&gt;
&lt;li&gt;Some of the habits probably weren't working that well for me.&lt;/li&gt;
&lt;li&gt;Some of the habits have lost cues that were previously helpful for keeping on top of them.&lt;/li&gt;
&lt;li&gt;My planning system became lightly aversive because it was trying to keep me on track despite the above fighting against me.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I think this can mostly be solved by taking stock a bit of what's reasonable, and deciding what habits to drop and what to keep, and part of that is looking at what's not working and dropping or modifying it based on where it fits into the above.&lt;/p&gt;


&lt;p&gt;For example I've dropped my &lt;a href="https://twitter.com/DRMacIver/status/1283138137129193472"&gt;daily paper reading habit&lt;/a&gt; because I decided it wasn't working that well for me - it felt high overhead (because I had to decide to read) and low value (I'd be better served chatting on &lt;a href="https://docs.google.com/document/d/1y3DkPGuAZiGbeNYlwje6jTlFxz9peIGnjSuZTwF4NGM/edit?usp=sharing"&gt;Weird CS Theory Discord&lt;/a&gt;) or reading a book.&lt;/p&gt;


&lt;p&gt;Another example is that I had a habit of rereading bits of books I'd previously read every day. I've almost entirely dropped that, not deliberately. I think I'm going to formally drop that - doing book driven posts on here fills the same need.&lt;/p&gt;


&lt;p&gt;I've also, for the moment, dropped my planning system (which was coping board based). It's a good system and I want to get back to it but I think I need to leave some time for my life to normalise into a new routine before I bring it back, and right now it has a bit of guilt attached to it, so I've thrown away the cards and will resume when it feels right to do so.&lt;/p&gt;


&lt;p&gt;Another thing that's been a problem is that I've been reading much less recently. This is I think a loss of a cue - I previously started reading during my morning coffee, which put me in the right mindset for the rest of the day and I would read more later as and when the mood struck me. Recently I've been doing morning pages during morning coffee, which is great and is a high value habit that I will absolutely retain, but robs me of that cue. I've started finishing morning pages by doing at least five minutes of reading, which I'm hoping will help with that.&lt;/p&gt;


&lt;p&gt;This notebook itself is, of course, one of the daily habits. For the moment I don't plan to drop it as such - I get a lot of value out of it, and I think other people do. Recently it's become a bit erratic and lower quality I think, but it's in the nature of a daily writing practice that that will happen. I might however start skipping days where I've otherwise been doing a lot of writing, or otherwise make some minor modifications to it.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-16-09:22.html" rel="alternate"/>
  </entry>
</feed>
