<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://notebook.drmaciver.com/</id>
  <title>DRMacIver's notebook</title>
  <updated>2021-07-05T10:30:00+01:00</updated>
  <author>
    <name>David R. MacIver</name>
    <email>david@drmaciver.com</email>
  </author>
  <link href="https://notebook.drmaciver.com" rel="alternate"/>
  <link href="https://notebook.drmaciver.com/feed.xml" rel="self"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-01-11-06:13.html</id>
    <title>Representing Steps in Test-Case Reduction</title>
    <updated>2021-01-11T07:50:20+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Representing Steps in Test-Case Reduction&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-01-11&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;In &lt;a href="https://notebook.drmaciver.com/posts/2021-01-01-11:22.html"&gt;my last post&lt;/a&gt; on reducer design I included the following deletion algorithm:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This iterates forward through a sequence and tries deleting each element of it in turn to see if it's still interesting.
It's a foundation for a lot of useful test-case reduction passes.&lt;/p&gt;


&lt;p&gt;It's also in the wrong order. Really for most problems you probably want to write it as follows:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# We're only calling this for the side effect of updating self.best_test_case&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reason for this order is that later elements often depend on early elements (e.g. if a variable declared earlier is used later),
so this often does less wasted work because it takes fewer passes to get to a fixed point - going forward, you need to rerun the entire pass to delete the earlier bits that have been enabled by the later bits.&lt;/p&gt;


&lt;p&gt;One problem with either of these is that they can lead to long stalls, where there's a large undeletable region and you're slowly going through all of it.
This is particularly annoying because often if you'd made the test case much smaller before checking all of those bits it would be much faster to go through them,
because the interestingness test is faster on smaller test cases.&lt;/p&gt;


&lt;p&gt;In order to avoid these stalls it might be better to write it like this:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pop_random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Might no longer be true because we've successfully reduced&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="c1"&gt;# We're only calling this for the side effect of updating self.best_test_case&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This iterates over the test case in a random order, avoiding stalls as a result - if a fraction \(p\) of the test case is deletable this will only stall for around \(\frac{1}{p}\) steps, while either deterministic version can potentially stall for \(O(n)\) calls.&lt;/p&gt;


&lt;p&gt;(this isn't a very good way of doing randomisation, and all of these are improved by &lt;a href="https://www.drmaciver.com/2017/06/adaptive-delta-debugging/"&gt;adaptive delta debugging&lt;/a&gt; of course, but we'll ignore that for now).&lt;/p&gt;


&lt;p&gt;The problem of course is that all of these are basically the same algorithm with a lot of code duplication between them, and we would like to be able to flexibly swap between modes based on what's working.&lt;/p&gt;


&lt;p&gt;Another related problem is that often we want to get an idea of how good a pass is before running it. If we were to e.g. run the random one for ten steps and see if it did anything before we decide whether to run the full thing, that would help a lot with the pass selection problem (how we know which reduction pass we should run right now).&lt;/p&gt;


&lt;p&gt;This example also makes it seem easier than it is. Consider the following reduction pass:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;put_in_order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This tries swapping all nearby out of order pairs in order to make the test case more sorted (which is a reduction lexicographically).
Here all the same considerations potentially apply, but there are two indices that vary instead of one.
The question of what order to try things in is also even more complicated, as potentially this one might need &lt;em&gt;many&lt;/em&gt; passes to reach convergence (there are \(n!\) possible permutations of a sequence so I think this has the potential to take \(O(n!)\) calls to complete given an adversarial interestingness test).&lt;/p&gt;


&lt;p&gt;All of this is problem set up as a preamble for telling you about how we solve the problem in Hypothesis, which is to consider a reduction pass as making a series of nondeterministic choices,
which it must exhaustively try all of before reduction is considered to have finished.
The API for making these choices looks as follows:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DeadBranch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Raised when a reduction pass is unable to continue from&lt;/span&gt;
&lt;span class="sd"&gt;    the current point."""&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Chooser&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""A source of nondeterminism for use in reduction passes."""&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;choose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;condition&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""Returns a value ``x`` in ``values`` such that&lt;/span&gt;
&lt;span class="sd"&gt;        ``condition(x)`` is True, possibly raising ``DeadBranch``&lt;/span&gt;
&lt;span class="sd"&gt;        if there are no such values, or if it turns out we have&lt;/span&gt;
&lt;span class="sd"&gt;        already exhausted all remaining values. """&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reduction passes above now look as follows:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_deletion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;put_in_order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="c1"&gt;# NB: Possible no such j exists in which case this raises.&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chooser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_test_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reducer is now free to try these in any order it likes, without the reduction passes overspecifying it.&lt;/p&gt;


&lt;p&gt;Currently we try two different orders in different contexts:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;Uniformly at random.&lt;/li&gt;
&lt;li&gt;Iterating backwards in reverse lexicographic order.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Switching between the two according some frankly fairly arbitrary internal heuristics that seem to work pretty well but are clearly suboptimal.&lt;/p&gt;


&lt;p&gt;The thing that's worth noting however is the representation. You can see it in &lt;a href="https://github.com/HypothesisWorks/hypothesis/blob/master/hypothesis-python/src/hypothesis/internal/conjecture/choicetree.py"&gt;the relevant Python file&lt;/a&gt;, but the important features of it are basically:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;We maintain a rose tree, lazily built as we walk it, where each node has &lt;code&gt;len(values)&lt;/code&gt; children, &lt;code&gt;values&lt;/code&gt; being the collection of choices made at that point. Each node also maintains a count of how many "live" (that is, not fully explored) children it has.&lt;/li&gt;
&lt;li&gt;When we complete running a pass step, we mark the final node as dead, decrement the count on its parent and if that is zero also mark it as dead and do the same with its parent etc.&lt;/li&gt;
&lt;li&gt;Every time we successfully reduce the current target, we delete the tree and start over with a fresh one, but for deterministic iteration start from the same point in the tree (to the greatest degree possible)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Essentially we are representing a step in a reduction pass as a sequence of integers representing nondeterministic choices,
and stopping when the full tree has been explored for the given shrink target.
As well as turning out to be quite a convenient representation, this is pleasingly similar to &lt;a href="https://2020.ecoop.org/details/ecoop-2020-papers/13/Test-Case-Reduction-via-Test-Case-Generation-Insights-From-the-Hypothesis-Reducer"&gt;how Hypothesis represents test-case generation&lt;/a&gt;.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-01-11-06:13.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-01-14-14:00.html</id>
    <title>Rejection Sampling in Hypothesis</title>
    <updated>2021-01-14T14:48:06+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Rejection Sampling in Hypothesis&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-01-14&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;(This post assumes you know roughly how Hypothesis does test-case reduction and generation. See &lt;a href="https://2020.ecoop.org/details/ecoop-2020-papers/13/Test-Case-Reduction-via-Test-Case-Generation-Insights-From-the-Hypothesis-Reducer"&gt;our ECOOP paper&lt;/a&gt; for a high level explanation)&lt;/p&gt;


&lt;p&gt;Suppose you have some generator &lt;code&gt;base&lt;/code&gt; and you want to sample from it conditionally.
In Hypotheis you can do this with &lt;code&gt;base.filter(condition)&lt;/code&gt;.&lt;/p&gt;


&lt;p&gt;Conceptually, this does &lt;em&gt;rejection sampling&lt;/em&gt;, which works roughly like this:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Filtered&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;base&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;condition&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__base&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;base&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__condition&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;condition&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;do_draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__base&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__condition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is, we're attempting to draw from the conditional distribution of values that come from &lt;code&gt;base&lt;/code&gt; that satisfy &lt;code&gt;condition&lt;/code&gt; and we do this by repeatedly drawing values from it until one satisfies the condition.&lt;/p&gt;


&lt;p&gt;This has a couple of problems:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;The underlying choice sequence contains a lot of probably useless garbage - we made a bunch of choices in each iteration of the loop when drawing from the base (and possibly even when calling the condition! This is allowed), and these are probably all irrelevant.&lt;/li&gt;
&lt;li&gt;These loops are potentially very long, especially if the condition is hard or impossible to satisfy.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;We adopt the following heuristic approach to deal with the second problem:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Filtered&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;do_draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__base&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__condition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mark_invalid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is, we terminate all rejection sampling at three iterations - it may well be possible to proceed from there, but if so we have a chance at trying again at the next generation, and spending longer seems largely not worth it.&lt;/p&gt;


&lt;p&gt;(Why three you ask? Well, let me ask you a better question: Why not three?)&lt;/p&gt;


&lt;p&gt;(That is to say, no good reason. It's a fairly arbitrarily chosen number that seems to work OK)&lt;/p&gt;


&lt;p&gt;The first problem we have a slightly more principled way to deal with: We want to be able to reduce the choice sequence by removing all the redundant draws.&lt;/p&gt;


&lt;p&gt;(This is made &lt;em&gt;slightly&lt;/em&gt; more complicated by the fact that the draws might not be redundant - Hypothesis generators are allowed to have side effects, such as inserting rows into a database, so it's possible for a filtered generator to do important work in the initial loops).&lt;/p&gt;


&lt;p&gt;Assuming for the moment that &lt;code&gt;condition&lt;/code&gt; does not make any choices (which it mostly won't, and we're not that fussed about producing worse results if it does) this is actually relatively easy:
The boundaries of the &lt;code&gt;draw&lt;/code&gt; call are marked on the choice sequence, and the reducer can try deleting that region of the choice sequence. This allows it to delete the early iterations of the loop,
resulting in an effect where in a reduced choice sequence we just get implausibly lucky and always satisfy the condition first time.&lt;/p&gt;


&lt;p&gt;We can, and do (although &lt;a href="https://github.com/HypothesisWorks/hypothesis/pull/2757"&gt;it turns out this got silently broken at some point and I fixed it as a result of writing this blog post&lt;/a&gt;), speed this up further by adding special boundary markers that indicate that the region is probably irrelevant.
This looks like the following:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Filtered&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;do_draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start_example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILTER_LABEL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;attempt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__base&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__condition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attempt&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_example&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;attempt&lt;/span&gt;
            &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;discard&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mark_invalid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reason this helps is that we can gather together all regions of the choice sequence that have been marked as discarded and attempt to delete them simultaneously.
This will, usually, work. In particular it allows us to make all filtered generators implausibly lucky simultaneously with one call.&lt;/p&gt;


&lt;p&gt;This kind of annotation of the choice sequence with useful information about common patterns in generation is often quite helpful for Hypothesis's test-case reduction. I think a lot of it is not that necessary any more, but this distinction between used and discarded regions seems to help a great deal and often allows for some significant performance improvements during reduction.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-01-14-14:00.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-01-19-11:41.html</id>
    <title>Some Theory of Test-Case Reduction</title>
    <updated>2021-01-19T14:39:06+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Some Theory of Test-Case Reduction&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-01-19&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;There's a bunch of test-case reduction theory that mostly lives in my head and currently-abandoned paper drafts.
I'm doing a week of PhD related writing so I thought it would be good to write up a dump of these ideas here.&lt;/p&gt;


&lt;p&gt;A test-case reducer is something that is designed to take an interesting test case and make it "simpler" in some sense while retaining the property of interest.&lt;/p&gt;


&lt;p&gt;Formally, a test-case reducer on some set of test-cases \(T\) is a function \(\{r : \{(U, x) : x \in U \subseteq T\} \to T\) such that \(r(U, x) \in U\). The set \(\{(U, x) : x \in U \subseteq T\}\) is called the set of reduction problems on \(T\).&lt;/p&gt;


&lt;p&gt;The first argument to the reducer is the interestingness test (the set of interesting test cases) and the second is some interesting test case that we want to reduce.&lt;/p&gt;


&lt;p&gt;A partial order \(\preceq\) is a reduction order for \(r\) (or \(r\) is a reducer for \(\preceq\)) if \(r(U, x) \preceq x\).
The reduction order captures what we mean by saying that a reducer makes the test case "simpler". A test case is simpler if it's smaller in the reduction order.&lt;/p&gt;


&lt;p&gt;(It's hopefully faily uncontroversial that the "simpler" relationship should be a partial order - it's basically to avoid cycles where sometimes \(x\) reduces to \(y\) and sometimes \(y\) reduces to \(x\) depending on the interestingness test. It's easy to construct reducers that do have these cycles, so not every reducer has a reduction order, but generally I think those reducers are badly designed)&lt;/p&gt;


&lt;p&gt;As I mentioned in &lt;a href="https://notebook.drmaciver.com/posts/2021-01-01-11:22.html"&gt;Basic Reducer Design&lt;/a&gt; I generally prefer the reduction order to be a total order.
There's no reason it has to be, but there's also no reason for it not to be - any partial order can be extended to a total order, and if a function is a reducer for some partial order then it's a reducer for any extension of it.&lt;/p&gt;


&lt;p&gt;One reason to prefer a total order is that if we want a test-case reducer to be normalizing (in the sense Groce et al. propose in &lt;a href="https://www.cefns.nau.edu/~adg326/issta17.pdf"&gt;One test to rule them all&lt;/a&gt;, that is its output does not depend on the initial test case) then it must be a total order because for any \(x, y\) we must have \(r(\{x, y\}, \cdot) = y\) or \(r(\{x, y\}, \cdot) = x\), so either \(x \preceq y\) or \(y \preceq x\).&lt;/p&gt;


&lt;p&gt;One nice thing about reduction orders is that they guarantee that reducers compose nicely without undoing each other's work. Given reducers \(r_1, r_2\) we can define a composite reducer straightforwardly as \((r_1 \cdot r_2)(U, x) = r_1(U, r_2(U, x))\) - we apply \(r_2\) then we apply \(r_1\).  If \(\preceq\) is a reduction order for both of \(r_1\) and \(r_2\) then it also is for \(r_1 \cdot r_2\).&lt;/p&gt;


&lt;p&gt;Without that common reduction order, you might end up in an annoying situation where \(r_2(U, x) = y\) and \(r_1(U, y) = x\). Note that you can have this even if \(r_1\) and \(r_2\) each have a reduction order but it's not the same one.&lt;/p&gt;


&lt;p&gt;One thing to consider is when a reducer is "done". We say a reduction problem \((U, x)\) is fully reduced for \(r\) if \(r(U, x) = x\),
and a reducer is idempotent if \((U, r(U, x))\) is fully reduced - i.e. running the reducer twice doesn't do any more than running the reducer once.&lt;/p&gt;


&lt;p&gt;Another nice feature of reduction orders is that they guarantee that you can always iterate a reducer to an idempotent fixed point (assuming the reduction order is reasonable. The technical condition we require here is &lt;a href="https://en.wikipedia.org/wiki/Well-founded_relation"&gt;well-foundedness&lt;/a&gt; but in basically every case of interest the order has the much stronger property that \(\{y: y \preceq x\}\) is finite for any \(x\)). Given a reducer \(r\) you can define the reducer \(r^+\) where \(r^+(U, x)\) is defined by iterating the sequence \(x_0 = x\), \(x_{n + 1} = r(U, x_n)\) until you reach a fixed point. Without a reduction order you can get cycles in this sequence so it may never terminate.&lt;/p&gt;


&lt;p&gt;You can also think of \(r^+\) as the fixed point of \(r\) under composition. i.e. \(r^+ = r \cdot r^+\), so we call \(r^+\) the &lt;em&gt;fixation&lt;/em&gt; of \(r\).&lt;/p&gt;


&lt;p&gt;By the way, these two constructs of reducer composition and fixation are not particularly abstract but are actually how reducers are built in practice.
You can think of your typical reducer as \((r_1 \cdot \ldots \cdot r_n\)^+) where the \(r_i\) are individual simpler reducers that are typically called &lt;em&gt;reduction passes&lt;/em&gt;.&lt;/p&gt;


&lt;p&gt;Generally speaking for a reducer we're most interested in the behaviour of \(r^+\) - what happens when you run the reducer to fixation - but it's often useful to consider reducers that are not necessarily run to fixation because of this pass structure. Anecdotally (and for solid heuristic reasons) it's generally better &lt;em&gt;not&lt;/em&gt; to run each pass to fixation before running the next one:
After running a reducer once it's typically "pretty close" to fixation, and running it again is likely to be wasted effort, while if other passes have run it may have been perturbed enough that more work is useful.&lt;/p&gt;


&lt;p&gt;Reasoning in terms of the fully reduced problems of a reducer also gives us another useful notion, which is that of reducer strength. We can say \(r_1\) is at least as strong as \(r_2\) if every reduction problem that is fully reduced for \(r_1\) is also fully reduced for \(r_2\) (and stronger if also there are some reduction problems that are fully reduced for \(r_2\) that are not for \(r_1\)).&lt;/p&gt;


&lt;p&gt;This is a slightly weaker notion of strength than some we might consider. It's possible for \(r_1\) to be stronger than \(r_2\) but to sometimes have \(r_2(U, x) \prec r_1(U, x)\), because \(r_1\) took a path that causes it to get stuck at a worse point.&lt;/p&gt;


&lt;p&gt;Under this notion of strength, \(r_1 \cdot r_2\) is at least as strong as each of \(r_1\) and \(r_2\), and \(r^+\) is equally strong to \(r\).&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-01-19-11:41.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-04-19-10:16.html</id>
    <title>Remastery Training</title>
    <updated>2021-04-19T11:10:12+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Remastery Training&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-04-19&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I play a lot of Slay the Spire.&lt;/p&gt;


&lt;p&gt;Slay the Spire is a game where you build up a deck of cards, which you use to fight a series of battles of varying difficulty.&lt;/p&gt;


&lt;p&gt;You ascend through the levels of the spire, until at the end of the game you attempt to defeat its heart.&lt;/p&gt;


&lt;p&gt;The game is divided into four acts, at the end of each of which you fight a boss.&lt;/p&gt;


&lt;p&gt;The heart is the boss of the fourth act.&lt;/p&gt;


&lt;p&gt;To get to Act 4, you must collect three keys before the end of Act 3.&lt;/p&gt;


&lt;p&gt;If you do not, the game ends early, with "Victory?" instead of "Victory"&lt;/p&gt;


&lt;p&gt;Slay the Spire has four characters, Ironclad, Silent, Defect, and Watcher.&lt;/p&gt;


&lt;p&gt;My preference goes Defect, Ironclad, Silent, Watcher, from favourite to least favourite.&lt;/p&gt;


&lt;p&gt;Slay the Spire has a series of difficulties called "Ascension levels".&lt;/p&gt;


&lt;p&gt;Ascension levels go from 0 to 20.&lt;/p&gt;


&lt;p&gt;You start at Ascension 0, and every time you complete Act 3 on an ascension level, you unlock the next one for that character.&lt;/p&gt;


&lt;p&gt;I would like to be able to defeat the heart on Ascension 20 with each character.&lt;/p&gt;


&lt;p&gt;So far I have managed this only with Defect.&lt;/p&gt;


&lt;p&gt;I am currently working on defeating the heart on Ascension 20 with Ironclad.&lt;/p&gt;


&lt;p&gt;My current, more modest, goal is to defeat Act 3, but so far I have not even been able to manage that.&lt;/p&gt;


&lt;p&gt;This is somewhat frustrating.&lt;/p&gt;


&lt;p&gt;I've tried quite a few times at this point.&lt;/p&gt;


&lt;p&gt;Probably not more than 10, but still.&lt;/p&gt;


&lt;p&gt;The problem is that I am no longer getting better at it, at least not very quickly.&lt;/p&gt;


&lt;p&gt;I have identified a strategy that would likely allow me to beat the heart, but I am very bad at it.&lt;/p&gt;


&lt;p&gt;To tell you about this strategy, I must first tell you about relics.&lt;/p&gt;


&lt;p&gt;Relics are items you acquire as you ascend the spire, which modify your abilities.&lt;/p&gt;


&lt;p&gt;Each character has a starter relic, unique to them.&lt;/p&gt;


&lt;p&gt;The Ironclad's starter relic is called "burning blood", and it heals you slightly at the end of each fight.&lt;/p&gt;


&lt;p&gt;One way to think about this is that it makes the game slightly easier, without making your character more powerful.&lt;/p&gt;


&lt;p&gt;This is not entirely right, but it's pretty close.&lt;/p&gt;


&lt;p&gt;At the beginning of the game, you encounter Neow, who offers you a blessing.&lt;/p&gt;


&lt;p&gt;One of the blessings you can get is to swap your starter relic for a random boss relic.&lt;/p&gt;


&lt;p&gt;Boss relics are very different from the Ironclad starter relic, in that most of them make the game harder while also making your character more powerful.&lt;/p&gt;


&lt;p&gt;Anyway, I noticed that I was never taking boss relic swap, because I liked the Ironclad starter relic too much.&lt;/p&gt;


&lt;p&gt;This suggested that my way through Ascension 20 might be to learn to boss relic swap on Ironclad.&lt;/p&gt;


&lt;p&gt;I tried this, and I died badly every time, because Ascension 20 is too hard for me to experiment on.&lt;/p&gt;


&lt;p&gt;So I went back down to Ascension 0.&lt;/p&gt;


&lt;p&gt;But even with the boss relic swap, Ascension 0 is very easy for me now.&lt;/p&gt;


&lt;p&gt;Slay the Spire is not an easy game, so a year or two ago this would have been inconceivable, but I've played more than 600 hours of it since then.&lt;/p&gt;


&lt;p&gt;I'm not &lt;em&gt;amazing&lt;/em&gt; at the game, but I'm quite good at it and, as a result, I can all but sleepwalk my way through Ascension 0, even with the harder tactics.&lt;/p&gt;


&lt;p&gt;So I decided that it wasn't enough for me to just beat act 3, I had to beat the heart to advance.&lt;/p&gt;


&lt;p&gt;The idea being that I have to learn to overshoot my goal.&lt;/p&gt;


&lt;p&gt;This starts to be at least a little difficult, even at the lower ascension levels.&lt;/p&gt;


&lt;p&gt;Enough that I have to pay attention, certainly.&lt;/p&gt;


&lt;p&gt;This approach lets me take advantage of the tighter feedback loops at lower ascension levels to focus on the parts of the game that I am still bad at.&lt;/p&gt;


&lt;p&gt;It removes slack. &lt;/p&gt;


&lt;p&gt;As a result I must be consistently good to progress, instead of just lucky.&lt;/p&gt;


&lt;p&gt;I will still progress by luck to some degree, because the game is hard enough that if you get unlucky you can die, even at lower ascension levels.&lt;/p&gt;


&lt;p&gt;But luck can only get you so far, and the changes I have made force me to develop more consistent skill.&lt;/p&gt;


&lt;p&gt;This is a bit like an idea called Mastery Training.&lt;/p&gt;


&lt;p&gt;Mastery Training is an approach where in order to learn something, you must master what comes before it.&lt;/p&gt;


&lt;p&gt;For example, requiring an A in all the prerequisite courses before you're allowed to take a course.&lt;/p&gt;


&lt;p&gt;This isn't always a good idea.&lt;/p&gt;


&lt;p&gt;I suspect it's often a bad idea.&lt;/p&gt;


&lt;p&gt;But what I'm doing here is an interesting variant of it.&lt;/p&gt;


&lt;p&gt;It is not mastery, but remastery.&lt;/p&gt;


&lt;p&gt;I have "completed the course" that is Ascensions 0 through 19.&lt;/p&gt;


&lt;p&gt;And now I'm retaking them, but with additional constraints, and trying to retake them at a higher level.&lt;/p&gt;


&lt;p&gt;I am relearning "familiar subjects" with the goal of getting an A in them this time.&lt;/p&gt;


&lt;p&gt;The theory being that what is holding me back from progressing past my current level might be a lack of consistency at the foundations.&lt;/p&gt;


&lt;p&gt;This seems like a good general lesson to apply.&lt;/p&gt;


&lt;p&gt;When we are stuck, start again from the beginning, but this time you have to do it &lt;em&gt;right&lt;/em&gt;.&lt;/p&gt;


&lt;p&gt;You have a clear path you walked before.&lt;/p&gt;


&lt;p&gt;You know the way.&lt;/p&gt;


&lt;p&gt;But now you must walk it again.&lt;/p&gt;


&lt;p&gt;This time carrying weights.&lt;/p&gt;


&lt;p&gt;Or paying careful attention to how you place your feet.&lt;/p&gt;


&lt;p&gt;Perhaps we should do this more generally?&lt;/p&gt;


&lt;p&gt;When we have something that is too hard, we replace it with something that is too easy, but done differently.&lt;/p&gt;


&lt;p&gt;For example, when we have reached a point where we feel our writing has plateaued, we start again with simpler writing, but done differently.&lt;/p&gt;


&lt;p&gt;Perhaps with a writing exercise where we change the cadence of our writing.&lt;/p&gt;


&lt;p&gt;And talk about mundane subjects.&lt;/p&gt;


&lt;p&gt;And use this to practice our voice, or our editing.&lt;/p&gt;


&lt;p&gt;You know, hypothetically.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-04-19-10:16.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-06-07-06:51.html</id>
    <title>A round-tripping problem with expected utility theory</title>
    <updated>2021-06-07T08:52:20+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;A round-tripping problem with expected utility theory&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-06-07&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;For reasons, I've been thinking about subjective expected utility theory and the &lt;a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem"&gt;Von Neumann-Morgenstern utility theorem&lt;/a&gt; ("the VNM theorem" for short) recently.
I have some significant philosophical objections to subjective expected utility and I'm going to explore a few of them in a series of notebook posts. This post is about an interesting thing I noticed while trying to articulate those objections.
I don't necessarily expect it to persuade anyone of anything.&lt;/p&gt;


&lt;p&gt;The problem is this: The VNM theorem loses information. You cannot actually reconstruct the original decision procedure from the output of the VNM theorem, because the VNM theorem's output isn't quite what it's usually framed as.&lt;/p&gt;


&lt;p&gt;The VNM theorem is as follows: Suppose you have a set of outcomes \(\mathcal{O} = \{O_1, \ldots, O_n\}\).
A lottery \(L\) is a random variable in \(\mathcal{O}\).&lt;/p&gt;


&lt;p&gt;An agent has preferences over lotteries, expressed by a preorder \(\preceq\) and an indifference relation \(\sim\).
That is \(L \preceq L'\) means that \(L'\) is at least as good as \(L\), and \(L \sim L'\) means the agent is indifferent between them.&lt;/p&gt;


&lt;p&gt;The VNM theorem is that under certain reasonableness conditions, this preorder must be expressible in terms of a utility function \(u: \mathcal{O} \to \mathbb{R}\), such that \(L \preceq L'\) if and only if \(E(u(L)) \leq E(u(L'))\). i.e. we prefer a lottery if it has a higher expected utility, and are indifferent between two lotteries with the same expected utility.&lt;/p&gt;


&lt;p&gt;It's instructive to sketch the proof of this theorem:&lt;/p&gt;


&lt;p&gt;Because we have a total order over outcomes, there is a best outcome and a worst outcome. Call them \(B\) and \(W\) respectively. If \(B \sim W\) then we're necessarily indifferent between all lotteries, so assume \(W \prec B\).
Now, consider the family of lotteries \(M_p\) such that \(M_p\) is \(B\) with probability \(p\) and \(W\) with probability \(1 - p\). So \(M_1 = B\) and \(M_0 = W\).&lt;/p&gt;


&lt;p&gt;Let \(L\) be some lottery. We must have \(M_0 = W \preceq L \preceq B = M_1\).
Therefore (by the hypotheses of the theorem) we can find some \(p\) in the middle such that \(M_p \sim L\).
Some computation shows that this number \(p\) behaves like an expected value.&lt;/p&gt;


&lt;p&gt;Thus the VNM theorem gives us the expected utility of an arbitrary lottery. Right?&lt;/p&gt;


&lt;p&gt;Except, it doesn't. It guarantees that this expected utility exists, but it does not actually give us a way to compute it.
The hypothesis of the VNM theorem is that there is some point at which the switchover to indifference happens, but we are not given an oracle which computes that probability, only a guarantee of its existence.&lt;/p&gt;


&lt;p&gt;Instead what we have to do is turn our oracle for preferences into a sort of mathematical root finding algorithm to find increasingly fine grained approximations to \(p\). We construct a sequence of approximations \(q_k \leq p \leq r_k\) by setting \(q_0 = 0\), \(r_0 = 1\), then to calculate \(q_{k+1}, r_{k+q}\) we set \(s_k = \frac{q_k + r_k}{2}\) and then asking whether \(M_{s_k} \preceq L\). If yes, we set \(q_{k + 1} = r_k, r_{k + 1} = s_k\), if no we set \(q_{k + 1} = s_k, r_{k + 1} = r_k\).&lt;/p&gt;


&lt;p&gt;There is no guarantee that this process will ever terminate - the numbers for which it terminates are precisely the dyadic rationals, the integers divided by some power of two, so for example if \(p = \frac{1}{3}\) it will go on forever, producing increasingly good approximations. You can design other divide and conquer schemes - there's nothing special about this particular one - but they will all have this problem of there being some utility values that this doesn't converge to.&lt;/p&gt;


&lt;p&gt;(Honesty compels me to admit that this problem goes away if you insist that probabilities and utilities have to be rational numbers, because you can design a divide and conquer scheme that just enumerates all of the rationals between the current bounds. For technical reasons that I will not go into here I find this a fundamentally unsatisfying conclusion, but the short version is that the real numbers are a better approximation to the problems of physical measurement than the rational numbers are)&lt;/p&gt;


&lt;p&gt;Which is fine, in practical terms you will very rarely need to know your utility to the last decimal place, but the interesting thing to note is this: Given access to a procedure that provides these increasingly fine grained approximations to the expected utility, you cannot actually recreate the original decision procedure with a process that is guaranteed to terminate.&lt;/p&gt;


&lt;p&gt;The problem is this: Suppose we have some procedure that calculates a relationship \(\preceq'\) based on such converging approximate utilities. We want \(\preceq'\) to be identical with \(\preceq\), so lets assume we've succeeded and see if this leads to a contradiction.&lt;/p&gt;


&lt;p&gt;Let \(p\) be some number where the process doesn't terminate and ask whether \(M_p \preceq' M_p\) (using the same \(M_p\) as above). If \(\preceq'\) agrees with \(\preceq\) we will get the correct answer that \(M_p \sim' M_p\).&lt;/p&gt;


&lt;p&gt;Now, look at how many steps of the approximation you had to compute in order to get to that point, say \(k\). This gives us an interval \(q_k \leq p \leq r_k\).&lt;/p&gt;


&lt;p&gt;Now at this point we decided that \(M_p \sim' M_p\) based on no information other than that \(p \in [q_k, r_k]\). Therefore for any other \(s \in [q_k, r_k]\) we would also decide that \(M_p \sim M_s\) (because we can't tell based on what we've computed that \(p \neq s\)). But in our original relationship we must have \(M_p \prec M_s\) or \(M_s \prec M_p\), because their utilities are \(p\) and \(s\) respectively. Therefore we have failed to recreate \(\preceq\) from the output of the VNM theorem.&lt;/p&gt;


&lt;p&gt;Why is this interesting?&lt;/p&gt;


&lt;p&gt;Well, mostly it's interesting because this looks exactly like my normal objections to the VNM theorem, but normally I justify it on practical grounds: You run into &lt;a href="https://en.m.wikipedia.org/wiki/Fredkin%27s_paradox"&gt;Fredkin's paradox&lt;/a&gt; (a term I've only recently learned - I've always thought of this as the &lt;a href="https://en.m.wikipedia.org/wiki/Buridan%27s_ass"&gt;Buridan's ass problem&lt;/a&gt;), where the amount of work you have to do to decide goes to infinity as you approach points of indifference.
From a practical viewpoint where you have all sorts of measurement and approximation problems,
this is (to me) obviously a problem with the theory, which is generally handwaved away by the setup:
The VNM theorem basically starts by saying "OK so first assume you're omniscient..." which is what allows it to start by assuming that you have this decision procedure.&lt;/p&gt;


&lt;p&gt;The interesting thing about this result is that the VNM theorem runs into this problem even on its own terms, without introducing any additional assumptions of physical reasonableness (unless you count "cannot run a literally infinite amount of computation"). The VNM theorem starts from an omniscient decision procedure and constructs a utility function, but you cannot run that process backwards from the utility function to such a decision procedure because of precisely the sorts of limitations that meant that you couldn't really have constructed that procedure in the first place. &lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-06-07-06:51.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-06-07-09:03.html</id>
    <title>High level objections to subjective expected utility theory</title>
    <updated>2021-06-07T09:03:00+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;High level objections to subjective expected utility theory&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-06-07&lt;/dd&gt;
&lt;/dl&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-06-07-09:03.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-07-02-09:29.html</id>
    <title>People are bad at defining things</title>
    <updated>2021-07-02T14:25:09+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;People are bad at defining things&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-07-02&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I'm currently reading a philosophy book, and as a result I am cranky about definitions.
Like everyone who is not a mathematician (or culturally basically a mathematician, such as a computer scientist or theoretical physicist),
philosophers are very bad at defining their terms. The key sin of philosophers is that they love doing it anyway, and think they're good at it, but are for the most part terrible about it unless they've lucked into learning how to do it well from some source that wasn't a bunch of philosophers who think "language games" meant you were supposed to play language as a competitive sport where you score points for using words that nobody else understands.&lt;/p&gt;


&lt;p&gt;What I mean by "definition" here, is that when you introduce a new term (or even use an existing term in a specific or unfamilar way), you need to provide people with the ability to understand your usage of it. This doesn't require a precise definition that says exactly what the term means, like a mathematician would, though if you &lt;em&gt;can&lt;/em&gt; provide such a thing that is often helpful, but it does require you to introduce enough precision that people can understand what you're talking about.&lt;/p&gt;


&lt;p&gt;Most people do not put in nearly enough work into defining novel terms in a way that is usable by the reader.
When you introduce a definition without adequate support, you have essentially put a road block in the reader's path until they have sufficiently understood your definition to proceed.
If you do not help them out in doing this, you have probably lost most of them as a reader,
and almost certainly many of those you retain will misunderstand your point.&lt;/p&gt;


&lt;h3&gt;What a good definition is&lt;/h3&gt;


&lt;p&gt;The purpose of a definition is to give people enough to grab on to to be able to understand your usage of the term.
As such, it should explain the term (see &lt;a href="https://www.drmaciver.com/2018/10/how-to-explain-anything-to-anyone/"&gt;How to explain anything to anyone&lt;/a&gt;) and give them some material to work with in developing their understanding on top of that explanation. &lt;/p&gt;


&lt;p&gt;A gold standard for definitions contains all of the following characteristics:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;The term is well separated from surrounding text rather than buried somewhere. The mathematical convention is that we typically have a bolded "&lt;strong&gt;Definition:&lt;/strong&gt;" in front of it, often with a number you can refer back to.&lt;/li&gt;
&lt;li&gt;It provides an explanation of &lt;em&gt;why we need the term&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;It provides an intuitive handle on the term.&lt;/li&gt;
&lt;li&gt;It provides as precise an explanation of what the term means as is reasonable.&lt;/li&gt;
&lt;li&gt;It is accompanied by at least one &lt;em&gt;positive example&lt;/em&gt;, an example of a thing that satisfies the definition, with an explanation of why it is an example.&lt;/li&gt;
&lt;li&gt;It is accompanied by at least one &lt;em&gt;negative example&lt;/em&gt;, an example of a thing that is close to satisfying the definition but doesn't, with an explanation of why it is not an example.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Of these, the examples are in many ways the most important and the most overlooked.&lt;/p&gt;


&lt;h3&gt;An example of a bad definition&lt;/h3&gt;


&lt;p&gt;In contrast the way most people do it is they shove a new term somewhere in the middle of a paragraph, gesture vaguely at what they mean by it, and then go on to use it in ways that completely fail to track the limited sense in which they've defined it.&lt;/p&gt;


&lt;p&gt;The definition that I am currently being cranky about comes from Albert Borgmann's is as follows:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;The early scientific theories of the Western world had both world-articulating and world-explaining significance. To articulate something, i.e., to outline and highlight the crucial features of something is also a kind of explanation. It is the kind of explanation that can satisfy the request for an explanation of a concrete thing or event. I will call it deictic explanation to distinguish it from deductive-nomological or subsumptive explanation.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;If I am being very very charitable, this just about satisfies (2) and (4). It explains why we need the term (it distinguishes a type of explanation that is different from the sort we are considering in the context of a modern scientific explanation), and it more or less defines it as being the sort of explanation that you get by outlining and highlighting th ecrucial features of something. Fair enough.&lt;/p&gt;


&lt;p&gt;Anyway, on the very next page this is how the word deictic is used:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;They do have deictic power in the sense of delimiting a set of possible worlds and ruling out certain impossible worlds. We can observe a similar pattern in the development from alchemy by way of chemistry to nuclear physics. Alchemy reflected in its laws a definite world of a limited number of stuffs and transformative forces and processes. Nuclear physics. being a microtheory, allows for an indefinite number of molar worlds.&lt;/p&gt;
&lt;p&gt;This pattern in the progress of science has no a priori character. It is an empirical fact that the world can be explained in the powerful scientific theories that we now have. The pace of the discoveries of these theories is a matter of historical fact. But given these two facts, it was inevitable that the deictic power of the sciences waned and all but vanished. This is not a failure of science. Nor is it the case that the deictic achievement of the earlier sciences was unquestionable or unique. Art has always been the supreme deictic discipline. Art in turn has sometimes been one with philosophy, religion, and politics; at other times these disciplines have complemented or competed with one another as disciplines of deictic explanation.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;This subsequent usage &lt;em&gt;very&lt;/em&gt; much makes me question whether I have understood his meaning.&lt;/p&gt;


&lt;p&gt;As near as I can tell, "deictic" means something along the lines of "indexical", which means something along the lines of "pointing to a specific thing" (thanks to &lt;a href="https://carcinisation.com/2020/01/27/ignorance-a-skilled-practice/"&gt;a literal banana for explaining this&lt;/a&gt;). Something is deictic to the degree that it concerns a specific thing in all its glory, in contrast to something being apodeictic. An explanation is deictic to the degree that it is focused on explaining &lt;em&gt;a particular thing&lt;/em&gt; and what is interesting about it specifically, while it is apodeictic to the degree that it is focused on explaining &lt;em&gt;a general phenomenon&lt;/em&gt; and the conditions under which it emerges.&lt;/p&gt;


&lt;p&gt;Maybe. I can't tell for sure, because he doesn't provide me with any concrete examples to test this. Certainly I would very much not have thought of philosophy or religion as at all examples of things prone to deictic explanations. Art I will grant, certainly, and politics certainly is full of &lt;a href="https://en.wikipedia.org/wiki/Wicked_problem"&gt;wicked problems&lt;/a&gt;, which by their nature require attention to the specifics of the unique thing involved.&lt;/p&gt;


&lt;p&gt;All of this confusion would easily have been cleared up with some examples and a bit more spelling out of his meaning.&lt;/p&gt;


&lt;h3&gt;An example of a good definition&lt;/h3&gt;


&lt;p&gt;In contrast, I think the following definition from Bernard Suits's "The Grasshopper", is very good:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;To play a game is to attempt to achieve a specific state of affairs [prelusory goal], using only means permitted by rules [lusory means], where the rules prohibit use of more efficient in favour of less efficient means [constitutive rules], and where the rules are accepted just because they make possible such activity [lusory attitude]. I also offer the following simpler and, so to speak, more portable version of the above: playing a game is the voluntary attempt to overcome unnecessary obstacles.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;This has both the precise definition and the intuitive handle on it. It then goes on for several pages more to investigate how this does and doesn't apply to specific examples, which I won't include here. It doesn't have a mathematician's "&lt;strong&gt;Definition:&lt;/strong&gt;" preamble, but that wouldn't really fit the style of the book, and it's still its own paragraph.&lt;/p&gt;


&lt;h3&gt;Appropriate levels of precision&lt;/h3&gt;


&lt;p&gt;In my gold standard for "good definition" I said "It provides as precise an explanation of what the term means as is reasonable".
The reason for the "as is reasonable" clause is that it is rarely the case that one can define something perfectly precisely,
at least outside of the realm of mathematics or constrained rule based environments (e.g. laws, rituals, games).&lt;/p&gt;


&lt;p&gt;In reality many things cannot be this precisely defined, and we need more example-based (philosophers would say "ostensive") and sketch explanations. For example I can't really define "red" for you, but I can tell you that red is a colour (hopefully you don't need me to define "colour", because I'd really struggle with this one) and then point at examples of things that are red and things that are not red until you get the picture.&lt;/p&gt;


&lt;p&gt;This is particularly common when you start talking about people, as people are reliably messy and complicated, and it becomes hard to precisely define a term rigorously. That's fine. We can still talk usefully about things like "love" without answering the question "What is love?", it just requires us to be a bit more careful about how we do it, and how we use examples. It may also require us to talk around a bit more about what we do and don't mean.&lt;/p&gt;


&lt;p&gt;For an example of why one might need to do this, a question that came up in discussion recently is "What does 'unconditional' mean in 'unconditional love'?". It turned out everyone was using it differently (e.g. does it refer to how the love would change if circumstances change, or is it a character of the type of love felt in the moment?). This is the sort of confusion that comes up in the absence of a sufficiently precise definition. We cannot always solve it with that better definition, but we do need to navigate it.&lt;/p&gt;


&lt;p&gt;This ties in to the question of teaching tacit knowledge that I talked about in &lt;a href="https://drmaciver.substack.com/p/how-to-teach-the-local-style"&gt;How to teach the local style&lt;/a&gt;. These definitions cannot perfectly capture the thing you want to convey, but you can get the rough idea, and you can correct any errors along the way.&lt;/p&gt;


&lt;p&gt;One thing that I think is important is that when you've introduced something this way you need to hold people's hands more when you use the term. Every time it comes up in future is an opportunity to refine their understanding of it, and it's worth taking many of those opportunities.&lt;/p&gt;


&lt;h3&gt;How bad definitions thrive&lt;/h3&gt;


&lt;p&gt;Why do so few people meet my standards of good definitions?&lt;/p&gt;


&lt;p&gt;I think, primarily, people write bad definitions for the following reasons:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;Nobody taught them how to write good definitions, because the practice is so rare outside of mathematicians (and also nobody teaches mathematicians to do it well, they just hope they pick it up by example like everything else important in mathematics).&lt;/li&gt;
&lt;li&gt;Writing good definitions is much harder work than defining things badly.&lt;/li&gt;
&lt;li&gt;Writing good definitions will often reveal that the emperor has no clothes and you're using big words for no goddamn good reason to make yourself sound clever.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I'm hoping in this essay I have done at least some small part to offset the first, although it probably needs a longer essay with more worked examples and better explanations of the various moving parts to really work for that.&lt;/p&gt;


&lt;p&gt;The second is, in fact, good. Writing definitions &lt;em&gt;should&lt;/em&gt; be hard, because every time you introduce a new term (even when you introduce it well!) you are asking the reader to do a bunch of work, and significantly harming their ability to understand your text. Often the best way to define something is to figure out a way to not need it at all, and where you do need it it's worth putting in the work to make sure that people can actually use it.&lt;/p&gt;


&lt;p&gt;The third is, I think, the most pernicious. I'm a big fan of Michael Billig's "Learn to write badly: How to succeed in the social sciences". I tried to find some good excerpts to include here, but it's not actually all that well suited to excerpting. In short, he argues that much of the use of jargon and technical terms in the social sciences is of an imprecise sort that &lt;em&gt;cannot&lt;/em&gt; be defined precisely in this way, because that would lose its social function, which is more about marketing your ideas and protecting them from criticism than it is about carefully and helpfully communicating them.&lt;/p&gt;


&lt;p&gt;Charitably, I don't think most people are deliberately leaning into this incentive, but I do think many people are innocently writing in a style that these incentives have shaped, and demanding clear communication is a good way to push back on those incentives.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-07-02-09:29.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-07-03-13:55.html</id>
    <title>No shortage of things to write about</title>
    <updated>2021-07-03T15:04:38+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;No shortage of things to write about&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-07-03&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;Back in &lt;a href="https://drmaciver.substack.com/p/fragments-intellectual-diy"&gt;Intellectual DIY&lt;/a&gt; I recounted a conversation with my father in which I explained that all of my reading and writing was more by way of a vast intellectual DIY project:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Last year I had a conversation with my father in which he said how impressed he was with my memory and general breadth of knowledge and intellectual achievement, and that he could certainly never do anything like that.&lt;/p&gt;
&lt;p&gt;I expressed polite scepticism at this claim and pointed out that his sheer breadth of practical knowledge far outstripped mine. He seems to know everything about DIY, and is generally prepared to wade in and do a good job on just about anything you might expect to do around the home, from heat pump engineering to building furniture. He's been building some stairs recently, as one does. He is finally starting to come around to the idea that maybe he doesn't have to do everything himself, but generally it's not a question of whether he can do it himself, but whether he can do this thing and all of the thousand other tasks he's taken on. This is of course all before we even start on his actual professional skill set (he was a banker before he retired).&lt;/p&gt;
&lt;p&gt;He objected that this was not at all the same thing - he just figured things out as he went and refined his skills over time in the course of doing them. He didn't necessarily remember all these things, he just started with the problems he was trying to solve, had the confidence to try things out and see if they worked, and practiced them until he got the knack of it, and then builds on that prior experience for future problems.&lt;/p&gt;
&lt;p&gt;“Well yes”, I said. “How do you think I do it?”&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;This analogy continues to bear fruit, as I had another conversation with him recently, in which he expressed how he was impressed and surprised by how I never ran out of topics to write about. "Well, would you have trouble coming up with things to do on the Mill?", I replied. "No, I guess not." "Same thing, basically."&lt;/p&gt;


&lt;p&gt;(The Mill is my parents' house. It's a converted water mill. It's lovely but also a significant amount of work)&lt;/p&gt;


&lt;p&gt;I think when people think of writing, people think of it as trying to create something new in the world,
but actually most of my writing has very little creativity to its selection of topics, I'm just responding to a particular concrete thing I've encountered.&lt;/p&gt;


&lt;p&gt;If you look at &lt;a href="https://notebook.drmaciver.com/posts/2021-07-02-09:29.html"&gt;yesterday's piece about defining things&lt;/a&gt;, there was no primal act of creativity where I had to "come up" with a topic. I was experiencing a thing and I was annoyed about it, so I wrote about it.&lt;/p&gt;


&lt;p&gt;Similarly, the day before's piece on the newsletter, &lt;a href="https://drmaciver.substack.com/p/telegraph-your-moves"&gt;Telegraph your moves&lt;/a&gt;, was a specific response to people's responses to some scheduling changes.&lt;/p&gt;


&lt;p&gt;Both of these were very mundane prompts, but based on my own evaluation and other people's responses, they both produce pretty top tier pieces.&lt;/p&gt;


&lt;p&gt;In general, life is full of problems, and even where there are no well defined problems there's a near infinite amount of detail.
Almost all of that detail you can just look at and go "Huh. What's up with that?" and write something about it.&lt;/p&gt;


&lt;p&gt;For me in particular a lot of my reading and writing fits into a sort of broad intellectual program of trying to help people get better at life, for values of "people" that first and foremost includes me. As a result, in order to run out of things to write, I'd have to basically have life solved. I'm uh a little ways away from that. &lt;/p&gt;


&lt;p&gt;Generally the problem I have is the opposite: There is &lt;em&gt;too much&lt;/em&gt; to write about. This leads to two problems:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;How do I decide which bits to write about?&lt;/li&gt;
&lt;li&gt;How do I write a short piece without pulling in everything?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The first part is genuinely hard, and I tend towards the less deliberate: I write about whatever happens to be particularly on my mind when I sit down to write. Sometimes I have to shuffle through a few topics until something clicks, but ultimately (as per &lt;a href="https://notebook.drmaciver.com/posts/2020-02-25-10:39.html"&gt;How to make decisions&lt;/a&gt;) when you've got a lot of things that could work the best way is to just pick something arbitrarily. This does result in my topics jumping around a lot, seemingly at random, but I'm OK with that and mostly don't care if other people aren't (and suspect, by and large, people enjoy the novelty factor).&lt;/p&gt;


&lt;p&gt;For the second: The way I experience this is that I'm reaching into a giant tangle and tugging on a thread. I start somewhere that seems interesting (such as with a concrete problem), and as I pull related thoughts and information come along with it, constructing a linear narrative as I go. Eventually I find somewhere that feels like a natural stopping point, and I stop. Like right now.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-07-03-13:55.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-07-04-12:52.html</id>
    <title>The humour test for expertise</title>
    <updated>2021-07-04T14:05:45+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;The humour test for expertise&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-07-04&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;A heuristic I sometimes use is this: You understand something at a reasonably expert level if and only if you get the jokes at a level where you could find it funny.&lt;/p&gt;


&lt;p&gt;I started thinking about this when I watched this video:&lt;/p&gt;


&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/VD5I4l9oASg" title="YouTube video player" width="560"&gt;&lt;/iframe&gt;


&lt;p&gt;This video is hilarious. If you don't play Slay the Spire much you won't understand why. I can explain why, and I can even explain why well enough that you in some way understand it, but I cannot explain it in a way that allows you to find it funny without teaching you to play Slay the Spire, because the humour derives entirely from an understanding of the game.&lt;/p&gt;


&lt;p&gt;Similarly, &lt;a href="https://github.com/mame/quine-relay"&gt;the quine relay&lt;/a&gt; is hilarious, but if you're not a programmer you absolutely won't find it funny, it's just weird and pointless.&lt;/p&gt;


&lt;p&gt;The reason for this link between expertise and humour is basically &lt;a href="https://en.wikipedia.org/wiki/Theories_of_humor#Incongruous_juxtaposition_theory"&gt;the incongruity theory of humour&lt;/a&gt;: You have a particular set of expectations about how the game is played, and the video repeatedly violates those assumptions by playing the game in slightly unexpected ways. This is not how a Defect build is supposed to work, but it's definitely how &lt;em&gt;this&lt;/em&gt; Defect build works.&lt;/p&gt;


&lt;p&gt;Because humour derives from incongruity, if you can get the jokes in a field then you must have a sense of incongruity. This means that you've internalised the sort of intuition about what the expected normal for the field is - the field's norms of taste, a working knowledge for how everything typically goes and fits together. These are all signs of expertise, that you've progressed to the &lt;a href="https://en.wikipedia.org/wiki/Four_stages_of_competence"&gt;Unconscious Competence stage&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;The humour test for expertise is itself a sort of joke, and the fact that I find it funny is probably an indication that I've at least started to get something of a handle on the expertise literature (or just that I'm weird), but it's also serious and I think probably points in a number of interesting directions.&lt;/p&gt;


&lt;p&gt;In particular, although you can't explain specialist jokes in a way that makes them funny, it may be worth explaining them, as it will often reveal interesting things about expert intuition that might not otherwise be readily accessible.&lt;/p&gt;


&lt;p&gt;Another application of the test is that the jokes you find funny probably tell you something about your areas of expertise that you hadn't realised. This may be an uncomfortable realisation because e.g. it causes you to realise quite how much time you've spent playing Slay the Spire.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-07-04-12:52.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2021-07-05-10:30.html</id>
    <title>Why do we need new terminology?</title>
    <updated>2021-07-05T10:30:00+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Why do we need new terminology?&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2021-07-05&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;This piece is off the back of &lt;a href="https://notebook.drmaciver.com/posts/2021-07-02-09:29.html"&gt;People are bad at defining things&lt;/a&gt;. You should read that piece first. You don't strictly &lt;em&gt;need&lt;/em&gt; to read that piece first, but this post will have a lot more context if you do.&lt;/p&gt;


&lt;p&gt;When you try to write using only easy to understand words, it can be hard to say what you mean because you have to use many words where a smaller number will do. As well as making it harder to write, this can make it harder to read, because every time you want to use the thing that you talked about in many words you have to use all those words again.&lt;/p&gt;


&lt;p&gt;When you do this it helps more people to understand what you have written, but every single person who could understand the shorter way of writing it will find it harder to understand the longer way of writing it because each time they read those many words they have to make sure it is the same thing.&lt;/p&gt;


&lt;p&gt;Sometimes it helps to make up new words that you can use instead of the many words that you would have to write. This makes it easier for people to read what you have written because you can use the word you made up each time you would have written many words and they know it means the same thing each time.&lt;/p&gt;


&lt;p&gt;I wrote that in the &lt;a href="https://splasho.com/upgoer5/"&gt;upgoer five text editor&lt;/a&gt;, which requires you to write using only the ten hundred most common words (thousand is not in the thousand most common words you see), and it was kinda annoying.&lt;/p&gt;


&lt;p&gt;What I wanted to say is this: Using a small vocabulary helps more people understand what you write, but it makes the text much longer. As a result, every person who could understand the larger vocabulary easily will tend to find it easier to understand the version that uses it fully, because each new word replaces a concept that took many words in the smaller vocabulary. Reading that single word each time is faster and doesn't require people to read the longer phrase to understand that you mean the same thing each time. As a result, it's often useful to introduce new terms that represent complicated concepts you're talking about to give people a handle on the concept they can use while reading your text.&lt;/p&gt;


&lt;p&gt;I don't know that the version with the larger vocabulary is &lt;em&gt;that&lt;/em&gt; much better than the version in up goer five. I'm certainly happier with it, but I think for short texts it doesn't matter that much and it's repetition over the course of a longer text that matters.&lt;/p&gt;


&lt;p&gt;Also sometimes you have a thing that you don't totally understand yet and it is nice to have a word that you can use to mean that thing while you talk around it. You can show things that are not what you mean and things that are what you mean, but you can't say exactly what it is you mean. Here it's really important to have a single word for it so you don't have to keep saying "that thing that I mean and am talking around".&lt;/p&gt;


&lt;p&gt;Which, translated out of upgoer five, is to say: Sometimes you have a concept that you don't know how to define precisely and want to talk about, and you can't just keep saying "that concept that I haven't defined yet that I introduced back there", you need some sort of placeholder you can talk about.&lt;/p&gt;


&lt;p&gt;This is why I talked about example-based definitions: You introduce the terminology, and you do your best to convey the rough sense of what you mean and provide examples to help refine it. By introducing a new word, you have given yourself the ability to talk about something that you would previously only have been able to talk around.&lt;/p&gt;


&lt;p&gt;To summarise all of this: We sometimes need new terminology because, used well, it helps you communicate what you mean in a way that people will find easier to understand.&lt;/p&gt;


&lt;p&gt;If this feels like an alien concept, very different to your experience of how people use novel terminology, yes that's right. That's why I'm complaining about people using definitions badly.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2021-07-05-10:30.html" rel="alternate"/>
  </entry>
</feed>
