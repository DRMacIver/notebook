<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://notebook.drmaciver.com/</id>
  <title>DRMacIver's notebook</title>
  <updated>2020-12-28T22:25:08+00:00</updated>
  <author>
    <name>David R. MacIver</name>
    <email>david@drmaciver.com</email>
  </author>
  <link href="https://notebook.drmaciver.com" rel="alternate"/>
  <link href="https://notebook.drmaciver.com/feed.xml" rel="self"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-22-12:49.html</id>
    <title>An Annoyingly Hard Algorithm to Beat</title>
    <updated>2020-07-22T13:25:55+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;An Annoyingly Hard Algorithm to Beat&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-22&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I have a vice, and that vice is clever solutions. I &lt;em&gt;love&lt;/em&gt; it when I can hit a problem with some giant theory hammer that cracks it right open. I don't claim this is a &lt;em&gt;good&lt;/em&gt; way to solve problems, but it sure is aesthetic.&lt;/p&gt;


&lt;p&gt;So let me tell you about an algorithm I resent &lt;em&gt;so&lt;/em&gt; much. It looks like this:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;failures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="c1"&gt;# 10 is arbitrary and the value that works best&lt;/span&gt;
&lt;span class="c1"&gt;# varies by problem.&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;failures&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;try_a_random_thing&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;the_thing_succeeded&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;failures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;the_thing_was_bad&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;failures&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I call these failure/success loops. As far as I know they're idiosyncratic to me but I'm sure it's a reinvention because they're so obvious.&lt;/p&gt;


&lt;p&gt;The basic idea is that you can use them to spend more time on things that have a high chance of succeeding. The parameter value (10 in this case) basically reduces variance at the cost of higher overhead, but either way you end up running things that have a higher chance of working for longer (I haven't bothered to do the maths of the exact distribution, because I inevitably end up using these in situations where success changes the probabiltiy of future successes anyway).&lt;/p&gt;


&lt;p&gt;Why do I resent the failure/success loop algorithm?&lt;/p&gt;


&lt;p&gt;Well because it's so dumb and it keeps beating better options.&lt;/p&gt;


&lt;p&gt;For example suppose you have a multi-armed bandit problem, where you have \(n\) arms and you want to pull arms that succeed and not pull arms that fail. Try the following:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bandit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_pulls&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pulls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;failures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="c1"&gt;# Why three? Who knows! But somehow three&lt;/span&gt;
            &lt;span class="c1"&gt;# often ends up being a good choice here.&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;failures&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pull&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="n"&gt;pulls&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;pulls&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;max_pulls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;return&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;failures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;failures&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This keeps pulling each arm until it's failed three times in a row, then moves on to the next one.&lt;/p&gt;


&lt;p&gt;Is this optimal? No, obviously not. This is a well studied problem with lots of interesting theory about regret bounds. From a theoretical point of view this algorithm is garbage.&lt;/p&gt;


&lt;p&gt;And also every damn time I try to write a multiarmed bandit code and feel super pleased with myself at getting to use some clever theory I find rough edges, go "Hmm... What if I just insert a bit of pregaming to prime the bandit with better information?" and then before I know it I've deleted the bandit code and am left with something that looks like the above.&lt;/p&gt;


&lt;p&gt;Why does this keep happening?&lt;/p&gt;


&lt;p&gt;I think there are a couple of reasons.&lt;/p&gt;


&lt;p&gt;The first is that the situations where they end up beating out clever algorithms for me tend to be ones where &lt;em&gt;nothing&lt;/em&gt; is going to work especially well. For example the above bandit algorithm seems to work pretty well in cases where most of the bandit arms have no chance of working and a few of the arms have a not-terrible chance of working. What ends up happening in these cases is that clever algorithms end up not buying you much, occasionally get stuck in weird ways, and you pay high overhead for their complexity.&lt;/p&gt;


&lt;p&gt;The second is that the virtue of this algorithm is it's incredibly simple and thus it's easy to tinker with - you can fiddle with the parameter on the fly, you can try other things in the middle, and it pretty much all works fine. There's no complicated state to maintain, and there's plenty of room to maneuver as the result of the simplicity, so even if the base algorithm doesn't work something like it will.&lt;/p&gt;


&lt;p&gt;In some sense this is perhaps evidence that the algorithm is less dumb than it seems, but that doesn't mean I resent it any less.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-22-12:49.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-26-10:40.html</id>
    <title>Notes on Reduced Output</title>
    <updated>2020-07-26T11:01:38+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Notes on Reduced Output&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-26&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;You've probably noticed that, despite &lt;a href="https://notebook.drmaciver.com/posts/2020-06-08-10:11.html"&gt;my advocacy for daily writing&lt;/a&gt; I've stopped writing here daily.&lt;/p&gt;


&lt;p&gt;I think there are a couple of things going on.&lt;/p&gt;


&lt;p&gt;The first is, as noted in &lt;a href="https://notebook.drmaciver.com/posts/2020-07-16-09:22.html"&gt;habit overload&lt;/a&gt; I've got just a bit too much going on habit-wise at the moment, and the daily notebook post has been crowded out a bit.&lt;/p&gt;


&lt;p&gt;The second is, a lot of the utility of the notebook has been supplanted for me by other things.
This notebook is a place where I process thoughts and explain things I want to explain, but the former has been taken over by morning pages, and the latter has been taken over by the newsletter.&lt;/p&gt;


&lt;p&gt;As a result, the notebook is left a bit vestigial. It's not clear what it's purpose is, and it's not quite clear where it fits into everything else in my habit schedule.&lt;/p&gt;


&lt;p&gt;This is a shame, because I like the notebook and I'd quite like to stick with it,
so I think for the next week I'm going to double down on it as follows:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;I'm going to write the notebook post immediately after lunch.&lt;/li&gt;
&lt;li&gt;I'm going to use a book prompt every day regardless of whether I feel like writing something else.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This should hopefully help it find a better place in my schedule and use cases.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-26-10:40.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-27-15:30.html</id>
    <title>Thinking Outside Your Head</title>
    <updated>2020-07-27T17:54:47+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Thinking Outside Your Head&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-27&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;As per &lt;a href="https://notebook.drmaciver.com/posts/2020-07-26-10:40.html"&gt;notes on reduced output&lt;/a&gt; I'm trying to do more book post prompts. Right now I think this will be &lt;em&gt;very&lt;/em&gt; rough and ready stuff where I'm just gathering together some related thoughts.&lt;/p&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/2LZNHSF"&gt;Behind Human Error&lt;/a&gt; by Woods et al., page 173:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;flexibility tends to create and proliferate modes of operation which create new cognitive demands on practitioners. Practitioners must know more - both about how the system works in each different mode and about how to manage the new set of options in different operational contexts.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;Long ago I wrote &lt;a href="https://notebook.drmaciver.com/posts/2018-10-31-16:54.html"&gt;The No Genies Conjecture&lt;/a&gt;, in which I proposed that all systems have at most two of the following properties:&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Powerful&lt;/li&gt;
&lt;li&gt;Flexible&lt;/li&gt;
&lt;li&gt;Easy to use&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This prompted a good conversation with &lt;a href="https://twitter.com/chrisamaphone/status/1252591089900572673"&gt;Chris Martens&lt;/a&gt; about what "easy to use" meant, in that most experts presumably find their tools easy to use.
I think this is a good example of why that is not the case: As flexibility goes up, the amount of state that the expert has to manage in their head will usually exceed the capacity of their week fleshy game.&lt;/p&gt;


&lt;p&gt;This is also the limitation of what I suggested in &lt;a href="https://notebook.drmaciver.com/posts/2020-07-10-16:52.html"&gt;learning to use the system&lt;/a&gt;, where I talked about how forming mental models is critical to using a system effectively.
In response to this pozorvlak pointed out how one of the features of good UI is to enable the formation of mental models.&lt;/p&gt;


&lt;p&gt;In particular I thought &lt;a href="https://twitter.com/pozorvlak/status/1282994011440021505"&gt;this point was particularly good&lt;/a&gt;:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;good UIs foster a good mental model of the system, &lt;em&gt;split over the mind of the user and the information available in the world&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;The problem is that this split is rarely well facilitated, especially in systems designed for experts, and this gets only worse as the flexibility of the system grows.&lt;/p&gt;


&lt;p&gt;One of the big problems that comes up a lot in "Behind Human Error" is the problem of &lt;em&gt;modality&lt;/em&gt;. Modal systems are ones where you need different mental models for different states of the system. People commit mode errors when they use a mental model for a different mode.&lt;/p&gt;


&lt;p&gt;This, I think, points to one of the difficulties of externalising parts of your mental model: what you're externalising it as is, itself, something you need a model for.&lt;/p&gt;


&lt;p&gt;I don't know what to do about this, but evidence suggests neither do the people who work on such systems unfortunately.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-27-15:30.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-28-09:52.html</id>
    <title>Seeing Your Working</title>
    <updated>2020-07-28T10:10:03+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Seeing Your Working&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-28&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/2ZBIVTz"&gt;Style: Towards Clarity and Grace&lt;/a&gt; by Joseph M. Williams, page 129:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;When someone is thoroughly at home in thinking through a problem, she can suppress in her prose the metadiscourse that records her thinking, allowing little or no of the intellectual process to reach the surface of her prose, or at least to remain the final draft.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;Coincidentally, &lt;a href="https://drmaciver.substack.com/p/writing-to-understand"&gt;today's newsletter is about the use of writing to understand rather than communicate&lt;/a&gt;. Using writing specifically as a place where all your thinking goes on the page, and is thereby improved.&lt;/p&gt;


&lt;p&gt;This reminds me of a complaint I often have: It's essentially impossible to learn to do mathematics by reading research level mathematics, because research level mathematics is performance of this form. A proof as presented is almost &lt;em&gt;never&lt;/em&gt; a proof as discovered, because the authors have spent a great deal of time refining, generalising, and making the process more elegant. Additionally, it often contains little to no trace of the process that lead to that discovery, and is presented in an entirely different order from that which lead to the underlying insights.&lt;/p&gt;


&lt;p&gt;It occurs to me that I would really enjoy reading a book that was just ethnographic studies and interviews of academics, where they talk about the process that lead to some great papers. Ideally these would be by embedded ethnographers who had spent some time observing the academics' actual research process, to try to offset the tendency to historical revisionism that we're all prone to.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-28-09:52.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-29-14:50.html</id>
    <title>Inclusivity and Onboarding</title>
    <updated>2020-07-29T15:09:42+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Inclusivity and Onboarding&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-29&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/2WYeYew"&gt;Talking About Machines&lt;/a&gt; by Julian Orr, page 70:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;The conversation of a closely cooperating group can be quite cryptic when the members are sharing information about work; they are considering a well-defined field which can be discussed with considerable economy, verging on code.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;A thing I have been thinking about a lot recently is the trade off between making a community accessible and the community being able to serve the needs of its core members.&lt;/p&gt;


&lt;p&gt;In &lt;a href="https://drmaciver.substack.com/p/being-deep-in-an-abstraction-stack"&gt;Being Deep in an Abstraction Stack&lt;/a&gt; I talk about how you need a community of people who share your abstraction stack and there is really no substitute for this no matter how much we want one.&lt;/p&gt;


&lt;p&gt;But unfortunately by creating such a community there is a certain amount of inherent gatekeeping: It is a community for people who have that abstraction stack, therefore if you do not have that abstraction stack you cannot participate in the community as a full member. I don't necessarily mean this as a moral judgement - sometimes gateekeeping is good - but there is nevertheless a certain amount of elitism implicit in the existence of such communities. In order to talk about our &lt;a href="https://drmaciver.substack.com/p/maintaining-niche-interests"&gt;niche interests&lt;/a&gt; we &lt;em&gt;need&lt;/em&gt; this cryptic shared language, and attempts to make the group more inclusive by avoiding that will also make it worse for us.&lt;/p&gt;


&lt;p&gt;I don't really have an answer to that other than "deal with it". "I need other people to talk about my interests with" is a &lt;a href="https://notebook.drmaciver.com/posts/2020-07-15-09:33.html"&gt;nice problem to have&lt;/a&gt;, but it's still a need and it's one that it is reasonable to satisfy, and creating slightly exclusionary communities is pretty much the only way to solve that problem..&lt;/p&gt;


&lt;p&gt;Part of why I've been thinking about this is &lt;a href="https://docs.google.com/document/d/1y3DkPGuAZiGbeNYlwje6jTlFxz9peIGnjSuZTwF4NGM/edit"&gt;Weird CS Theory Discord&lt;/a&gt;. The opening paragraph includes the sentence:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;You don’t have to “be” a computer scientist, or even know much computer science, to be here, but if you’re not interested in computer science you probably won’t enjoy the space.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;This hopefully makes it clear that we're not looking to gatekeep (I originally typoed that as "gategeek" which, well, valid) based on formal qualifications, but nevertheless there is a reasonably high bar to participating in the space. People are welcome to join, and they're welcome to learn to become full participants through their experience in the space, but that is there.&lt;/p&gt;


&lt;p&gt;But, at the same time, we're a tiny free community that I created to serve my particular need for more chatter about weird CS theory. We're absolutely not a computer science institution. We cannot invest a tonne of effort in onboarding people, the best we can possibly hope to do is to be patient and welcoming to people who are still in the process of onboarding themselves.&lt;/p&gt;


&lt;p&gt;One problem that occurs, and is important to avoid, is when this lack of inclusitivity becomes considered a feature rather than an unfortunate trade off. The specialised language becomes not a useful shorthand but a shibboleth.
I have tried to head that off by making it clear that asking us to explain ourselves is always valid:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;[You are encouraged to] Ask questions. (“Can you explain that?” is always fine, though answering “No” is fine too)&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;We have a specific norm that is designed to give people the space to indicate that they need help understanding something, but also a specific norm that we should not exhaust ourselves trying to bring people in.
If people want to bring themselves in, the door is open, but we can only help so much.&lt;/p&gt;


&lt;p&gt;Unfortunately, this too is exclusionary: Many people don't have the time or resources to do that work themselves.
I feel bad about this, but ultimately the world is still a better place for the community, and there is only so much I can do about it.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-29-14:50.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-30-16:00.html</id>
    <title>Politics as long periods of boredom punctuated by moments of sheer terror</title>
    <updated>2020-07-30T16:16:22+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Politics as long periods of boredom punctuated by moments of sheer terror&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-30&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/2yvyoOw"&gt;Democracy in Small Groups&lt;/a&gt; by John Gastil, page 145:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Consensus and democracy didn't always work to the satisfaction of the faculty and students at the Meeting School. Even as students obtained ever-greater power, they remaind reluctant to deal with many important issues.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;I think an under-appreciated constraint on the running of human civilisation is just how incredibly boring well-functioning politics is. (2020 politics is of course, sadly, not boring).&lt;/p&gt;


&lt;p&gt;Worse, it manages to be simultaneously:&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Incredibly Boring&lt;/li&gt;
&lt;li&gt;Extremely detailed&lt;/li&gt;
&lt;li&gt;Highly Stressful&lt;/li&gt;
&lt;li&gt;Utterly Vital&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I feel like this is a combination of factors that a democratic process is almost uniquely poorly suited for, which is I guess why we end up delegating so much of the running of a country to its civil service.&lt;/p&gt;


&lt;p&gt;But there end up being all sorts of things which have these properties and you &lt;em&gt;can't&lt;/em&gt; delegate to a civil service, because they require actual policy decisions, and I feel like we don't really have a good handle on how those work (or, at least &lt;em&gt;I&lt;/em&gt; don't have a good handle on how those work. Is there some sort of ethnographic study of the UK parliament I could read?).&lt;/p&gt;


&lt;p&gt;The worst combination seems to happen when a decision &lt;em&gt;requires&lt;/em&gt; an understanding of all these boring details, but people who are not willing to believe that have democratic oversight over the decision anyway.
This is basically how you end up with Brexit.&lt;/p&gt;


&lt;p&gt;On the other hand, I know enough about how the sausage is made that I wouldn't want a country run entirely by the civil service either (I admit I might still prefer this to the current lot). Experts are, ultimately, still people, and those people &lt;em&gt;do&lt;/em&gt; in some way need to be aligned with the wishes of the populace, at least to the degree that those wishes are reality-based and not actively murderous.&lt;/p&gt;


&lt;p&gt;(OK, maybe I &lt;em&gt;don't&lt;/em&gt; want much democracy in the UK, as we seem to be pretty bad at both halves of that)&lt;/p&gt;


&lt;p&gt;I don't have any sort of solution here. I often describe the problem of how to coordinate large groups of people whose interests are only weakly mutually aligned as the fundamental problem of civilisation, so it's not surprising that I'm not going to solve it in a dashed off notebook post, but I do feel like managing this boredom aspect is more important than I had previously realised.&lt;/p&gt;


&lt;p&gt;Unfortunately, currently we manage the boredom of politics by getting increasingly frustrated with it and, as a result, eventually deciding to make politics exciting, and that doesn't go so well. &lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-30-16:00.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-07-31-13:14.html</id>
    <title>Standardised Facts</title>
    <updated>2020-07-31T13:19:12+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Standardised Facts&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-07-31&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/2ZB2wmE"&gt;Seeing Like a State&lt;/a&gt; by James C. Scott, page 80:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;State simplifications have at least five characteristics that deserve emphasis. Most obviously, state simplifications are observations of only those aspects of social life that are of official interest. They are &lt;em&gt;interested&lt;/em&gt;, utilitarian facts. Second, they are also nearly always written (verbal or numerical) &lt;em&gt;documentary&lt;/em&gt; facts. Third, they are typically &lt;em&gt;static&lt;/em&gt; facts. Fourth, most stylized state facts are also &lt;em&gt;aggregate&lt;/em&gt; facts. Aggregate facts may be impersonal (the density of transportation networks) or simply a collection of facts about individuals (employment rates, literacy rates, residence patterns). Finally, for most purposes, stateofficials need to group citizens in ways that permit them to make a collective assessment. Facts that can be aggregated and presented as averages or distributions must therefore by &lt;em&gt;standardized&lt;/em&gt; facts. However unique the actual circumstances of the various individuals who make up the aggregate, it is their sameness or, more precisely, their differences along a standardized scale or continuum that are of interest.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;This is a very interesting passage that I think I probably just nodded and skimmed over when first reading this book. I find that often the random page approach is a good way of highlighting such passages.&lt;/p&gt;


&lt;p&gt;I don't have a huge amount to say about this passage right now (because I'm ill and it's hot and as a result I am quite stupid today) but I feel like it's especially relevant right now during the COVID-19 epidemic, where we're seeing a lot of this sort of thing. Death rates, hospitalisation rates, etc. are very much an example of standardised facts, and are eliding a lot of the weird and very unpleasant ways in which people's COVID-19 symptoms differ.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-07-31-13:14.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-08-01-15:39.html</id>
    <title>From a Certain Point of View</title>
    <updated>2020-08-01T16:03:44+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;From a Certain Point of View&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-08-01&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;From &lt;a href="https://amzn.to/3fiWsnC"&gt;Jonathan Strange &amp;amp; Mr Norrell&lt;/a&gt; by Susanna Clarke, page 480:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;"Artists are tricky fellows, sir, forever reshaping the world according to some design of their own," said Strange. "Indeed they are not unlike magicians in that."&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;A thing I often think about is the blurry line between true and useful, communication and performance.&lt;/p&gt;


&lt;p&gt;In the context of the story, this is about visual art, but the same happens with the written or spoken word: When we attempt to convey facts, we end up summarising, simplifying, focusing on what we think is important about it, and often when we do this we end up saying things that are not, in the strictest sense, true.&lt;/p&gt;


&lt;p&gt;(In context, Norrell is complaining about a painting that has a mirror where there is none).&lt;/p&gt;


&lt;p&gt;Sometimes this is because we elide crucial details. Sometimes it's a &lt;a href="https://en.wikipedia.org/wiki/Lie-to-children"&gt;lie-to-children&lt;/a&gt;. Sometimes it's to give the story the true aesthetic without the elaborate scaffolding needed to convey that aesthetic accurately (the mirror).
Sometimes, of course, it's just a lie.&lt;/p&gt;


&lt;p&gt;I think we end up doing this more than we think, because memory is more fallible than we like to treat it as, and we tend to think more in terms of aesthetic and narrative than detailed recollection of facts. In this sense we are all artists.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-08-01-15:39.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-08-07-12:27.html</id>
    <title>Ensuring Downward Paths</title>
    <updated>2020-08-07T15:41:49+01:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Ensuring Downward Paths&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-08-07&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;I had some half-formed thoughts about test-case reduction that I wanted to work through, and this notebook is a place for working through half-formed thoughts, so here we go...&lt;/p&gt;


&lt;p&gt;The thing I've been working on on and off for about a month now is &lt;a href="https://notebook.drmaciver.com/posts/2020-07-05-11:29.html"&gt;using language inference to figure out new reduction passes&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;It can be thought of as a way of taking an idea that doesn't work (using L* for test case reduction) and combining it with an idea that works but doesn't generalise well (learning a table of small string substitutions to make),
and getting an idea that does work and does generalise.&lt;/p&gt;


&lt;p&gt;This has me thinking: What, exactly, does it mean for an approach to test-case reduction to generalise?&lt;/p&gt;


&lt;p&gt;Also &lt;em&gt;does&lt;/em&gt; this approach generalise well? The big question with it is basically whether it works well when you change the interestingness test. I think it will, but it probably depends a lot on the choice of interestingness test and how close your reducer is to normalising it already.&lt;/p&gt;


&lt;p&gt;In the approach to test-case reduction I favour, test-case reduction starts from set of test cases \(X\) and a total order \(\preceq\) over \(X\) called the &lt;em&gt;reduction order&lt;/em&gt;,
and a test-case reducer is a function \(r(U, x)\) where \(x \in U \subseteq X\), \(r(U, x) \in U\), and \(r(U, x) \preceq x\). For me usually \(X\) is a set of strings and \(\preceq\) is the shortlex order where \(x \preceq y\) if \(|x| &amp;lt; |y|\) or \(|x| = |y|\) and \(x \leq y\) lexicographically.&lt;/p&gt;


&lt;p&gt;Your prototypical test-case reducer starts from a bunch of small-step reductions, which gives you a function \(t: X \to X^{&amp;lt;\omega}\), where \(t(x)_i \prec x\),
and the reducer greedily tries each of \(t(x)_i\) in order. If none of them are in \(U\) it returns \(x\), otherwise it it recurses and returns \(r(U, t(x)_i)\) for the first \(i\) with \(t(x)_i \in U\).&lt;/p&gt;


&lt;p&gt;More generally given such a function we might say that a reducer is \(t\)-local if it satisfies the following conditions:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;\(t(U, x) = x\) if and only if \(t(x)_i \not\in U\) for all \(i\).&lt;/li&gt;
&lt;li&gt;For all \(U, x\) is some sequence \(x = x_0, \ldots, x_n = r(U, x)\) with \(x_{i + 1} \in t(x)_i\) (note that we don't necessarily require that \(x_i \in U\)).&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;That is, basically a \(t\)-local reducer is one that looks like the prototypical reducer but may have some optimisations and fiddly bits. e.g. delta debugging is a \(t\) local reducer where \(t\) is maps \(x\) to all test cases that can be formed by deleting a single element of \(x\). Delta debugging starts by trying large bulk combinations of these deletions and gradually narrows it down until it's only trying the small ones. Another approach which uses the same \(t\) but a different algorithm is what I've called &lt;a href="https://www.drmaciver.com/2017/06/adaptive-delta-debugging/"&gt;adaptive delta debugging&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;Most reducers implemented in practice are roughly \(t\)-local for some \(t\) (often they fail to be \(t\)-local in some cases because they hit some internal limit and stop when they could be making more progress, and rerunning the reducer may produce further redctions) and I think this is good because it allows you to reason in terms of what transformations they are capable of performing.&lt;/p&gt;


&lt;p&gt;So lets assume we have a \(t\)-local reducer. What makes it good?&lt;/p&gt;


&lt;p&gt;Basically the tradeoff is this:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;The larger \(|t(x)|\) tends to be the better you will reduce \(x\).&lt;/li&gt;
&lt;li&gt;The smaller \(|t(x)|\) is the better your reducer tends to perform.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The ideal seems to be that you want \(|t(x)| = O(|x|)\), as per delta debugging, and the trick is to tweak the set within that constraints to try to get good results.&lt;/p&gt;


&lt;p&gt;(Note that having \(|t(x)| = O(|x|)\) doesn't mean that the overall reducer runs in \(O(|x|)\) - even if you reduce the size at every step you can easily be made to make \(O(|x|^2)\) membership checks, and if you just make lexicographic transformations it's easy for performance to get exponentially bad)&lt;/p&gt;


&lt;p&gt;The problem that you run into is that basically what you really want is not just for \(|t(x)|\) to be large, but for it to contain values that are in some sense "likely to be in \(U\)". One way this can fail to happen is when there is some sort of precondition on the range of \(U\) we are interested in. The biggest example of this is when interesting test cases have to be syntactically valid according to some grammar. When this happens, effectively you are censoring \(t\) by replacing it with its syntactically valid subset. Often when something like delta debugging gets stuck it's not because it's not because of any intrinsic limitation related to the interestingness test, it's just ended up at a point where \(t(x)\) contains no syntactically valid test-cases.&lt;/p&gt;


&lt;p&gt;So, essentially what we want in order to get a good reducer for a format is to try to ensure that \(t(x)\) is always well populated with &lt;em&gt;syntactically valid&lt;/em&gt; reductions of \(x\). &lt;/p&gt;


&lt;p&gt;One way to do this might be to artificially construct interestingness tests that are basically designed to expose small reductions. I'm not quite sure what those would be. One thing I was thinking of is having interestingness tests of where \(x\) is interesting if \(y \preceq x\) for some fixed \(y\), then once those are normalised gradually "censor" the tests by removing all intermediate parts, stopping when you can't learn anything other than the direct jump to \(y\).&lt;/p&gt;


&lt;p&gt;This can be thought of as ensuring that whenever the final reduced result is \(y\) you have as rich a set of paths leading to it as the learning process is possibly able to find.&lt;/p&gt;


&lt;p&gt;I don't yet know how likely this is to work. I'm planning to do more experiments soon on getting this running on hypothesis-csmith and we'll see how well it can learn that, but I think there's some interesting stuff to investigate here.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-08-07-12:27.html" rel="alternate"/>
  </entry>
  <entry>
    <id>https://notebook.drmaciver.com/posts/2020-12-28-16:19.html</id>
    <title>Reducing Weird Tests</title>
    <updated>2020-12-28T22:25:08+00:00</updated>
    <content type="html">

&lt;p class="subtitle"&gt;Reducing Weird Tests&lt;/p&gt;


&lt;dl class="metadata"&gt;
&lt;dt&gt;Published&lt;/dt&gt;
&lt;dd class="post-date"&gt;2020-12-28&lt;/dd&gt;
&lt;/dl&gt;


&lt;p&gt;Hillel recently wrote about &lt;a href="https://buttondown.email/hillelwayne/archive/fd1f0758-ae31-4e83-9138-33721cbd5ce3"&gt;cross-branch testing&lt;/a&gt; - comparing implementations of the same API in two different branches as a form of differential testing to see whether a change introduced any functional differences.
As part of his example he ended up with what was functionally the following interestingness test:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;is_interesting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;He mentions in the article that this gives nondeterministic results for Hypothesis:&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Running this gives us an erroneous input:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Falsifying example: test_f(a=5, b=6, c=5,)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is nondeterministic: I’ve also gotten &lt;code&gt;(0, 4, 0)&lt;/code&gt; and &lt;code&gt;(-114, -114, -26354)&lt;/code&gt; as error cases.&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;&lt;code&gt;(0, 4, 0)&lt;/code&gt; is the (probably globally) correct answer of all of these according to Hypothesis's reduction logic,
but Hypothesis's test-case reduction is unable to reliably transform all of the failing examples to it.&lt;/p&gt;


&lt;p&gt;I reacted to this as I always do to such things: "Oh, I wonder if I can make the test-case reducer better". But the answer is no, no I cannot, and the reason is that this test contains what is basically a hash function (not a very good hash function, but good enough), and as a result the test cases are pretty much uniformly distributed throughout the set of test cases - if you generate test cases where the values are between &lt;code&gt;-10&lt;/code&gt; and &lt;code&gt;10&lt;/code&gt;, then of the &lt;code&gt;9261&lt;/code&gt; possible test cases, &lt;code&gt;129&lt;/code&gt; are interesting, i.e. about &lt;code&gt;1.4%&lt;/code&gt; which is about what you'd expect if the computation of &lt;code&gt;abs(out) % 100&lt;/code&gt; were roughly randomly distributed.&lt;/p&gt;


&lt;p&gt;For such functions we end up with the slightly awkward situation where:&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;It's not that hard to find test cases (&lt;code&gt;1%&lt;/code&gt; isn't that infrequent! We can just exhaustively enumerate to find the globally minimal test case).&lt;/li&gt;
&lt;li&gt;It's hard enoguh to find test cases that most test-cases are locally minimal.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Suppose for a typical triple of integers we have, say, &lt;code&gt;10&lt;/code&gt; reductions we can perform, and those reductions are no more likely than chance to work, then &lt;code&gt;90%&lt;/code&gt; of interesting test cases are locally minimal, which means that we'll typically end the reduction process within a few steps.&lt;/p&gt;


&lt;p&gt;In order to explore this, lets consider the following toy model: Our space of interesting test cases are triples of integers between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;10&lt;/code&gt;, and we have a local reducer that makes the following transformations:&lt;/p&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reduce_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# First try sorting the tuple&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;reduced&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Now try lowering each component&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;reduced&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# If we can raise the next component then try that&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduced&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(if this doesn't make sense to you don't worry about it too much, it's just the first local reducer that came to mind for triples of integers)&lt;/p&gt;


&lt;p&gt;Suppose we're reducing subject to an interesting test where the maximal value &lt;code&gt;(10, 10, 10)&lt;/code&gt; is always interesting and every other test case is interesting with probability &lt;code&gt;p&lt;/code&gt;. What is the probability that this local reducer finds the globally optimal test case?&lt;/p&gt;


&lt;p&gt;Based on some fairly crude simulations, the local reducer starts finding the globally optimal test-case more than half the time around &lt;code&gt;p = 0.93&lt;/code&gt;, which has the amusing property that the local reducer is reliably worse (in expectation) than the reducer that just always tries &lt;code&gt;(0, 0, 0)&lt;/code&gt; and gives up if that doesn't work (which finds the globally minimal test case with probability &lt;code&gt;p&lt;/code&gt;). Some equally crude simulation suggests that an approach that just enumerates the smallest test cases for a while returns at least as good a result in the same number of calls to the interestingness test in at least 80% of cases, regardless of the value of &lt;code&gt;p&lt;/code&gt;.&lt;/p&gt;


&lt;p&gt;I'm not sure if there's any sort of general lesson to derive here - most interestingness tests are not quite this adversarial - but I do feel like there is often something like this going on, where the reduction problem isn't that hard despite being in a local minimum, and I wonder if something loosely inspired by this that estimates the density of interesting test cases below the current interesting test case, but I'm not sure if that's entirely useful.&lt;/p&gt;


&lt;p&gt;If one &lt;em&gt;did&lt;/em&gt; have some sort of easy way to sample from test cases that are strictly smaller than the current interesting test case, it might be worth running that for longer than it would be intuitively obviously useful as a sort of crude but perhaps surprisingly effective reducer? It's certainly more robust to this sort of problem than any kind of local transformation based reducer would be. &lt;/p&gt;


&lt;p&gt;This also suggests that possibly test-case reducers should try a bit more bounded exploration of the small test cases than they currently do, but I think this is probably misleading due to the relatively small number of test cases in this example - for most formats of interest the number of test cases of size &lt;code&gt;n&lt;/code&gt; is exponential in &lt;code&gt;n&lt;/code&gt;, so any amount of enumeration other than trying the globally minimal test case (which &lt;em&gt;is&lt;/em&gt; usually worth doing) is probably not worth it.&lt;/p&gt;

</content>
    <link href="https://notebook.drmaciver.com/posts/2020-12-28-16:19.html" rel="alternate"/>
  </entry>
</feed>
