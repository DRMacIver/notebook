<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>
DRMacIver's Notebook: Thoughts from David R. MacIver
    </title>
    <link rel="stylesheet" href="/pygments.css"/>
    <link rel="stylesheet" href="/tufte.css"/>
    <link rel="stylesheet" href="/latex.css"/>
    <link rel="stylesheet" href="/drmnotes.css"/>
    <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/feed.xml" />

    <meta name="viewport" content="width=device-width, initial-scale=1">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(', '\\)']]},
  multiLine: true,
  "HTML-CSS": { 
       linebreaks: { automatic: true }
  },
  SVG: { 
       linebreaks: { automatic: true } 
  }
});
</script>

  </head>

  <body>
    <article>
        <h1><a href="/">DRMacIver's Notebook</a></h1>
        <p class=subtitle>Thoughts from David R. MacIver</p>

        

<section>
<h2><a href="/posts/2018-09-07-12:32.html">2018-09-07</a></h2>


<p class="subtitle">When come back bring pie(s)</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-07</dd>
</dl>


<p>
 There's a metaphor people use: Some people fight for a larger slice of the pie, others see that it's better to enlarge the pie.</p>


<p>
 I've seen this metaphor used for everything from intersectional feminism to the Patrician of Ankh-Morpork's extremely libertarian brand despotism.
Broadly the point is this: It's better to build a positive sum game where everyone benefits than it is to compete in a zero or negative sum game.</p>


<p>
 The relationship between this point and the metaphor is interesting.
I agree with the thing that I am claiming to be the underlying point (but then I would), but I think what the actual metaphor demonstrates is also interesting:
People don't understand how pies work.</p>


<p>
 What happens when you build a bigger pie?</p>


<ol>
<li>
  You run into scaling issues, limited both by the size of your oven and also (if you build a better oven) the square-cube law (actually I'm not sure if this is the square-cube law at work, as pies tend to be scaled horizontally faster than they are scaled vertically, but either way once your pie gets big enough it's very hard to ensure it's cooked all the way through - you end up with overdone outsides and and underdone middle).</li>
<li>
  The same people who couldn't eat your smaller pie still can't eat your larger one.6</li></ol>


<p>
 The correct solution is not to enlarge the pie. It's to
 <em>
  bake more pies</em>
 , and also the provide tasty food that is not pie because not everyone likes pie.</p>


<p>
 If you've ever tried catering to a diverse group of dietary requirements, at some point you hit the point where you realise that it's much much easier to make multiple dishes than it is to try to create a single dish that can feed everyone.
A vegan gluten free nut free diabetic friendly pie is certainly possible, but it is a pie that basically nobody is going to
 <em>
  want</em>
 to eat.
In contrast, a wide variety of desserts that can cater to each particular restriction that your group encounters,
without attempting to shoehorn everyone into a one size fits all badly model.</p>


<p>
 The Unit of Caring has a notion she uses a lot of
 <em>
  competing access needs</em>.
She
 <a href="https://theunitofcaring.tumblr.com/post/135162290121/hi-i-have-a-quick-question-i-tried-googling-but">
  explains it well here</a>
 ,
but the important quote (to save you from Tumblr's giant GDPR screen) is:</p>


<blockquote>
<p>
  Competing access needs is the idea that some people, in order to be able to participate in a community, need one thing, and other people need a conflicting thing, and instead of figuring out which need is ‘real’ we have to acknowledge that we can’t accommodate all valid needs.
I originally encountered it in disability community conversations: for example, one person might need a space where they can verbally stim, and another person might need a space where there’s never multiple people talking at once. Both of these are valid, but you can’t accommodate them both in the same space.</p></blockquote>


<p>
 Trying to build a space that works for everyone is more or less impossible, and what you will end up with is a space that works badly for everyone.
Instead we need the ability to have multiple spaces which we acknowledge as valid and allow people to freely move between these spaces as long as they are prepared to accept the local rules.</p>


<p>
 In an interesting coincidence, this came up in a completely different context recently.
A while back
 <a href="https://www.drmaciver.com/2016/05/randomizing-lean-coffee/">
  I sketched out a way of using randomization to improve the design of Lean Coffee meetups</a>.
 <a href="https://twitter.com/georgesdubus/status/1037957014599749632">
  This morning a friend reported</a>
 :</p>


<blockquote>
<p>
  I used to organize David-Style lean coffees at my previous job. (...)
The interesting limitation we ran into is that toward the end, the attendence was two groups with mostly disjoint interests.</p></blockquote>


<p>
 The nice thing about small-scale democratic processes like this is that splitting the union is a completely legitimate move.
If you have two groups with disjoint interests, why not run them as two groups? Ideally at different times so that people who really
 <em>
  are</em>
 interested in both can attend both.</p>


<p>
 How to do this sort of thing at a larger scale seems to be one of the great unsolved problems of society.</p>


</section>
<section>
<h2><a href="/posts/2018-09-06-10:14.html">2018-09-06</a></h2>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-06</dd>
</dl>


<p>
 Follow on to
 <a href="https://notebook.drmaciver.com/posts/2018-09-05-13:24.html">
  misc thoughts about voting design for talk scheduling</a>.</p>


<p>
 Here's how a system that is much closer to classic STV could work.
Assume everyone has a ranking of all the talks they wish to attend (this isn't actually reasonable to ask for, but you could get people to score talks according to some ordinal scores and then randomly tie break, or tie break in organiser preferred order or something).</p>


<p>
 The system has the following three parameters:</p>


<ol>
<li>
  The number of time slots.</li>
<li>
  The number of talks per time slot.</li>
<li>
  The minimum number of attendees required for a talk to be worthwhile (should be at least one). Callt his the threshold.</li></ol>


<p>
 You also need to pick a quota system. Either the
 <a href="https://en.wikipedia.org/wiki/Comparison_of_the_Hare_and_Droop_quotas">
  Droop or the Hare quota</a>
 are the obvious choices.
My natural bias is to use the Hare quota, as it's better for minority interests and I think that's a nice feature to have in your conference talk selection (conferences have a tendency to have the same talks over and over again and I think this would help offset that).</p>


<p>
 The system could easily be adapted to more complicated constraints in which not all talk/time slot combinations are valid, but I'm going to ignore that.</p>


<p>
 Conceptually what happens is everyone is given one voting-buck,
and a talk slot "costs" an amount of voting-bucks equal to the quota.
People band together to form buying blocs and each spend the same percentage of their remaining pool of voting money to buy a slot (this is basically how normal STV works too).</p>


<p>
 The system involves running the following process to a fixed point:</p>


<ol>
<li>
  Set the list of eligible talks to all talks which have at least the threshold number of people voted for them.</li>
<li>
  Give everyone exactly one vote (note: as the process evolves, people will have fractional votes).</li>
<li>
  People vote for (talk, slot) pairs, where the slot has not already been filled and the talk is both eligible and not yet scheduled.
   They will vote for a pair if:
  <ol>
<li>
    The talk their highest ranked talk among the available talks.</li>
<li>
    If there are slots which have no talks they want to see in them, they will only vote for pairs in those slots.
   Otherwise they will vote for pairs where they prefer the talk to the one currently scheduled there.
   Note that a voter can vote for multiple (talk, slot) pairs.</li></ol></li>
<li>
  If there are no such pairs, we have scheduled all of the talks we can (even if there still unfilled slots). Stop and report this as the schedule.</li>
<li>
  If any of the pairs has a total number of votes exceeding the quota, pick the one with the most votes and schedule that.
   For each voter who voted for it, multiply their remaining vote by \(1 - \frac{q}{r}\), where \(q\) is the quota and \(r\) is the total vote for the elected slot
   (i.e. we've removed \(q\) from their total vote and everybody pays it equally).</li>
<li>
  If no pair was elected, take the talk with the lowest maximum vote over all vote pairs, and remove it from the list of eligible talks.</li>
<li>
  If a pair was elected, now check if any talks can no longer meet the threshold - i.e. if for every slot you could schedule them in,
   count the number of people for whom that is their favourite talk in that slot. If there are no slots where this exceeds the threshold, remove the talk from the eligible list.</li>
<li>
  If we removed any talks from the eligible list, reset all of the state except the list of eligible talks and go back to step 2. Otherwise go back to step 3.</li></ol>


<p>
 Most of this is just variant STV, with some of the specific details owing to specific types of STV.
The main difference is that because the same voter may cast their vote for multiple options simultaneously,
we need to be careful not to elect more than one "candidate" at once,
plus the specialised drop-out rule for talks that fail to meet the threshold.</p>


<p>
 Most of my problems with it are the same as my problems with STV in general: It looks like an iterative optimisation process, but it's not at all clear what it is you are optimising for.
So it might work well, but I'm not really sure how you would measure "well" in this context.
It seems plausibly worth a try though.</p>


</section>
<section>
<h2><a href="/posts/2018-09-05-13:24.html">2018-09-05</a></h2>


<p class="subtitle">Mechanisms for talk scheduling and voting</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-05</dd>
</dl>


<p>
 I've been thinking about mechanism design for conference scheduling again.
I've
 <a href="https://www.youtube.com/watch?v=OkusHEBOhmQ">
  previously argued that conference scheduling should be treated as an optimisation problem</a>
 ,
but I no longer believe that's true.</p>


<p>
 In particular I think the following hold:</p>


<ul>
<li>
  If we treat talk selection as a voting problem, we must employ some mechanism of proportional representation</li>
<li>
  In a multi-track conference, scheduling and selection cannot be separated.</li></ul>


<p>
 Lets see some examples in support of this.</p>


<p>
 Suppose you're running a Python conference, and 60% of the people attending are web developers and 40% are data scientists.
You put together a set of talk proposals, people vote on them, and you take all of the top voted talks.
What you end up with is of course a conference consisting entirely of web development talks.</p>


<p>
 (Note: Despite the running Python example, this post is not actually about
 <a href="https://2018.hq.pyconuk.org/schedule/">
  The PyCon UK Schedule</a>
 , which I've barely looked at.)</p>


<p>
 For some contexts maybe that's OK, but given that a lot of the value in conferences is the hallway track, it's nice to be able to put together heterogenous conferences.
You could fix this by artificially selecting for certain subjects, but proportional representation seems like a much better approach because it doesn't require you to know all the ways in which your audience is heterogenous in advance.
So, in the above example, we would have roughly 60% web dev talks and 40% data science talks,
but also if it turned out that about 10% of the audience were really excited about Flask,
we could have about 10% Flask talks.</p>


<p>
 If the conference is single-track we're more or less done: Pick your favourite (non party-list based, so probably some variant of STV), proportional voting system,
use that to select your talks, and call it a day.</p>


<p>
 I'd like to pause here by saying that I'm increasingly a fan of single track conferences, so I think "do a single track conference and call it a day" might actually be the correct solution.</p>


<p>
 But lets suppose you're less on board with that and want a multi-track conference.</p>


<p>
 For simplicity, lets imagine that our Python conference now has two rooms,
with talks running in the same time slots in each room,
and attendees now have to choose which of the two to attend.
Lets say it's a single day conference and there are five time slots,
so ten talks.</p>


<p>
 According to our above PR argument, we should run six web dev talks,
but does it really make sense for us to do so?
There are only five time slots,
so (by
 <a href="https://en.wikipedia.org/wiki/Pigeonhole_principle">
  the pigeonhole principle</a>
 if you want to get fancy about it) you're inevitably going to put two web dev talks back to back.
That might be OK - maybe you're scheduling a Django and a Flask talk against each other - but maybe there's a strict preference where there are five obviously best web dev talks and the sixth is pretty good (preferable by web devs to any data science talk) but not good enough (will not get any attendees when scheduled against any of the top five talks). What's the point in selecting that talk given that?</p>


<p>
 In the other direction, lets say we have 20% of the audience who are really interested in random forests,
and so we select two random forests talks,
which we then proceed to schedule in the same time slot.
Now despite 20% representation at the talk level,
they only have 10% representation at the time slot level!</p>


<p>
 (I want to draw an analogy to
 <a href="https://en.wikipedia.org/wiki/Gerrymandering">
  gerrymandering</a>
 here but I don't think it quite works)</p>


<p>
 So, tracking creates an upper bound on how much proportional representation is worth doing, and also scheduling within those tracks affects the amount of proportionality you actually get.</p>


<p>
 So what to do about it?</p>


<p>
 Well, I'm not entirely sure. I started designing a whole complex system in support of this that this note was originally supposed to be about, but I decided I didn't like it very much.</p>


<p>
 The basic ideas were:</p>


<ol>
<li>
  Give each participant a "voting currency" - everyone starts with an equal amount, and talk slots effectively get auctioned off, with the proceeds distributed among everyone equally (possibly among everyone who still has any interest in attending remaining talks).</li>
<li>
  Participants will only vote for talks in slots that are strictly better for them than the talks already scheduled in that slot.</li>
<li>
  Define a threshold of "Minimum number of people required to be worth running a talk". Whenever a talk no longer would meet that requirement (because every slot it could be scheduled in has talks people prefer more), it is immediately excluded and the process restarts from the beginning. This is akin to how exclusions work in
  <a href="https://en.wikipedia.org/wiki/Wright_system">
   The Wright System of STV</a>
  , and is designed to avoid "spoiler" talks, where people who preferred them effectively get screened off from voting in the process until the talk is excluded.</li></ol>


<p>
 The details kinda became a weird hybrid of STV and the
 <a href="https://en.wikipedia.org/wiki/Vickrey%E2%80%93Clarke%E2%80%93Groves_mechanism">
  Vickrey-Clarke-Groves mechanism</a>
 and the more I looked at it the less convinced I became that it was the right way to do things or that I actually understood how the VCG mechanism plays out in practice.</p>


<p>
 I do think the above examples are important to consider though.</p>


</section>
<section>
<h2><a href="/posts/2018-09-05-11:08.html">2018-09-05</a></h2>


<p class="subtitle">My parents, Ayn Rand and God</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-05</dd>
</dl>


<p>
<a href="https://twitter.com/bazzalisk/status/1037277763219152897">
  From bazzalisk on Twitter</a>
 :</p>


<blockquote>
<p>
  “You know him better than I” and “You know him better than me” are both grammatically valid but mean different things</p></blockquote>


<p>
 The former means "You know him better than I do", the latter means "You know him better than you know me".</p>


<p>
 The title of this note comes from the following probably-apocryphal book dedication,
used as an argument for the oxford comma:</p>


<blockquote>
<p>
  This book is dedicated to my parents, Ayn Rand and God.</p></blockquote>


<p>
 Without the Oxford comma,
the implication is that the author's parents are Ayn Rand and God,
with the Oxford comma, this is a dedication to four people (the author's parents, and also to Ayn Rand and God).
 <a href="http://mentalfloss.com/article/33637/best-shots-fired-oxford-comma-wars">
  Mental Floss has a bunch of similar ones</a>.</p>


<p>
<a href="http://msgboard.snopes.com/cgi-bin/ultimatebb.cgi?ubb=get_topic;f=95;t=000863;p=0">
  Snopes think this probably never happened</a>
 ,
but OTOH the following is part of their argument:</p>


<blockquote>
<p>
  Since Rand was such an outspoken atheist, I find it hard to believe that anyone would mention both her and God as sources of inspiration.</p></blockquote>


<p>
 And, well, this seems to ignore the existence of Paul Ryan and a significant chunk of the US political right.
Also I'm now amused by the idea of Ayn Rand's atheism being a reaction to God being a deadbeat dad.
Someone should write that fanfic, but it's not going to be me.</p>


<p>
 There is of course
 <a href="https://amzn.to/2LYsLcz">
  an entire book about comedic misinterpretations due to bad grammar</a>
 ,
but that's not exactly what's going on here:
Instead these are interesting grammatically valid examples that are right on the edge of ambiguity.</p>


<p>
 It's unclear to me whether this actually tells us anything useful.
We could probably derive some normative advice about correct use of grammar from it,
but this sort of thinking about things in terms of their edge cases is a very modern-mathematician view of the world,
which doesn't come very naturally to others.</p>


<p>
 The general widely deployed solution to linguistic ambiguity is instead that we just guess or ask,
and frankly that probably works better than trying to remove it.</p>


</section>
<section>
<h2><a href="/posts/2018-09-02-21:22.html">2018-09-02</a></h2>


<p class="subtitle">Fiction for Kristian</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-02</dd>
</dl>


<p>
 This is a small collection of fiction I've written that I like enough to actively recommend and think count as "finished".</p>


<h3>
 Fan fiction</h3>


<p>
 The two pieces of Stargate fan fiction that I've written and would recommend are
 <a href="https://archiveofourown.org/works/3673335">
  Stargate Physics 101</a>
 and
 <a href="https://archiveofourown.org/works/5023654">
  Interview with a System Lord</a>.
Both are not only canon-compatible (more or less. Stargate Physics 101 doesn't quite line up with Stargate Universe, but I don't care about Universe),
but are 100% my headcanons of how the universe works.</p>


<p>
 Completion status: 100% finished standalone pieces. I may write other Stargate fan fiction at some point, and if I do then as part of the universe's canon those will naturally be part of its backstory,
but there will never be sequels per se to these pieces.</p>


<p>
 Recommendation strength: Stargate Physics 101 is one of my most popular pieces and works even if you have never watched Stargate. If you like any of infrastructure science fiction, software testing, or stargate, it's worth reading.
Interview with a System Lord is worth reading if and only if you like Stargate SG1 (and especially if you like Ba'al) and want a moderately amusing story exploring a weird headcanon. Warning: May cause mild sympathy for the devil.</p>


<p>
<a href="https://archiveofourown.org/works/4637439/chapters/10575111">
  The Rules of Wishing</a>
 is a piece of fan fiction of Disney's Aladdin.
Premise:</p>


<blockquote>
<p>
  What if people were good at wishing? The Genie's rules have holes you could drive a herd of camels through, but they don't have to. Aladdin and Jafar's wishes are shallow and limited, and lack the foresight that really effective wishing entails, but wouldn't a battle between effective wishers be much more interesting? And while we're at it, why does Jasmine have so little agency and basically act as a prize to be won in a battle between two men when literally the entire point of her narrative is that she's not that?</p></blockquote>


<p>
 It has been argued to be rational!fic though I'm not sure I agree with the classification.
Jasmine in this is probably my joint favourite character I've ever written.</p>


<p>
 Completion status: Has a mini non-canon sequel
 <a href="https://archiveofourown.org/works/13523703">
  The Consequences of Wishing</a>
 that explains the divergence between this story and the film.
May, but probably won't, spawn another sequel, but the current ending wraps it up entirely to my satisfaction and any sequel would be a new story in the same universe with the same characters rather than a continuation of this story.</p>


<p>
 Trigger warning: Moderately violent.</p>


<p>
 Recommendation strength: Honestly, you should read this if you like my fiction at all and are not put off by the trigger warning.</p>


<p>
<a href="https://archiveofourown.org/works/13354146">
  Counterparts</a>
 is a crossover fic between Lucifer (the TV show) and Old Harry's Game (the radio show).</p>


<p>
 Completion status: Very standalone. It's not impossible I may do a followup involving The Good Place, but it stands on its own regardless of whether I do.</p>


<p>
 Recommendation strength: Well it amuses
 <em>
  me</em>. Based on feedback, if you like Lucifer it will probably also amuse you. Familiarity with the Old Harry's Game is helpful but not strictly required.</p>


<h3>
 Original Fiction</h3>


<p>
<a href="https://archiveofourown.org/works/9233966/chapters/20941043">
  Programmer at Large</a>
 is a story about gender, social anxiety, and legacy code.
It seems to have a lot of fans.</p>


<p>
 Completion status: Abandoned, but it kinda works that way. It's a series of slice of life chapters, and the protagonist's life is never really "finished". However it definitely has some unsatisfying dangling plot threads that will never be resolved. However most of the strength of this story is at the chapter level anyway - it has some of my best writing in it, but as a whole story I do not feel that it works. I intend at some point to take it apart and refactor and modularise it into several smaller stories. I am fully aware of the irony of saying this about a story about legacy code.</p>


<p>
 Recommendation strength: Mixed. There's some stuff in there I really like, and a lot of people seem to love it, but like I said I don't feel that it hangs together in its current incarnation.</p>


<p>
<a href="https://archiveofourown.org/series/754683">
  The Diaries of Vicky Frankenstein</a>
 more normally AKA "The Vicky Stories". Series of short stories about Dr Vicky Frankenstein and her adventures in joining a biotech startup run by the vampire Ada Lovelace.</p>


<p>
 Completion Status: (Hopefully permanently) incomplete in the sense that I fully intend to keep writing Vicky stories (but don't more than about one every six months), but each Vicky story is a complete standalone short story that happens to be set in the same world and use the same characters. There are minimal to no dangling plot threads between the stories.</p>


<p>
 Recommendation Status: I ♥ writing Vicky and think you should read these. Also, contains a (99% SFW) lesbian sex scene between two amoral monsters that reviewers describe as "ridiculously adorable", so there's that.</p>


</section>
<section>
<h2><a href="/posts/2018-09-02-13:13.html">2018-09-02</a></h2>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-02</dd>
</dl>


<p>
 Compare and contrast two interesting links:</p>


<ul>
<li>
  Siderea's
  <a href="https://siderea.livejournal.com/1230660.html">
   The Asshole Filter</a></li>
<li>
  Cormac Herley's
  <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/WhyFromNigeria.pdf">
   Why do Nigerian Scammers Say They are from Nigeria?</a></li>
<li>
  Karl Popper's "The Paradox of Tolerance"</li></ul>


<p>
 (Note that I've not read the latter two and should. I've only read digested versions of them).</p>


<p>
 In general often the right way to judge an action is not actually on its immediate effects,
but on what long-run effect they will have on the sort of people you will surround yourself with.
This can make seemingly good actions harmful and seemingly bad or nonsensical ones quite useful.</p>


<p>
 I think about this a bunch in the context of codes of conduct:
Often the benefit of the code of conduct is not whether it is ever enforced,
but that it filters out people who don't like codes of conduct.</p>


</section>
<section>
<h2><a href="/posts/2018-09-01-17:41.html">2018-09-01</a></h2>


<p class="subtitle">Notation for test-case reducers</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-01</dd>
</dl>


<p>
 A thing I've been noticing recently is that it's really useful to have compact notation for describing things.
Usually this is equivalent to primivitives + some combinators.</p>


<p>
 One thing that I think it would be useful to have such a notation for is (greedy) test-case reduction passes.
They combine pretty well, and it makes it useful to discuss various things.</p>


<p>
 For example, if you have reducers \(A\), \(B\), you can define the reducer \(AB\) which runs \(A\), then
runs \(B\) on its result. You can also define the reducer \(A^+\) which runs \(A\) to a fixed point.</p>


<p>
 Another interesting combinator is \(/\). \(A / B\) runs \(A\), then runs \(B\) if \(A\) didn't do anything..</p>


<p>
 There are a bunch of really basic algebraic relations that hold, like composition and \(/\) are associative,
and \((A^+)^+ = A^+\), but not a huge amount beyond that.</p>


<p>
 A bunch of interesting questions about test-case reduction can be compactly expressed in this notation though.
For example, suppose you want to reduce to something that is a fixed point of both \(A\) and \(B\).
You could do \((AB)^+\), but you could also do \((A^+B^+)^+\), and it's quite natural to do this in some contexts.
My suspicion, which I've yet to verify, is that it's almost never the right thing to do.</p>


<p>
 You can kinda regard the quadratic mode failure of greedy search as an instance of this problem:
If \(\delta_i\) is the operation that deletes the element at position \(i\), the correct pass to run for greedy deletion is \((\delta_0^+ \ldots \delta_n^+)^+\),
but if you start again at the beginning every time you succeed you are running \((\delta_0 / \ldots / \delta_n)^+\).</p>


</section>
<section>
<h2><a href="/posts/2018-09-01-16:08.html">2018-09-01</a></h2>


<p class="subtitle">Modes of writing</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-01</dd>
</dl>


<p>
 Two posts on writing to contrast:</p>


<ul>
<li>
<a href="https://blog.malignat.us/2018-05-12/on-the-creative-merits-of-paper">
   On the creative merits of paper</a></li>
<li>
<a href="http://devonzuegel.com/post/comparison-of-text-editing-methods">
   Comparison of text editing methods</a></li></ul>


<p>
 Devon posted the second on twitter and it reminded me of the first, which I struggled to refind, which is part of why I'm posting it here.</p>


<p>
 I've been finding having a paper journal very useful, but I'm also finding having this new notebook useful in an entirely different way.
The contrast is very interesting.</p>


</section>
<section>
<h2><a href="/posts/2018-09-01-09:17.html">2018-09-01</a></h2>


<p class="subtitle">Can a machine design?</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-01</dd>
</dl>


<p>
<a href="http://echo.iat.sfu.ca/library/cross_01_machine_des.pdf">
  Can a machine design?</a>
 by Nigel Cross is an interesting paper about architecture (the real kind!) and its relation to automation.
I found it via Adam Marshall Smith's PhD thesis
 <a href="https://adamsmith.as/papers/mechanizing_exploratory_game_design_book.pdf">
  Mechanizing exploratory game design</a>
 (truthfully via
 <a href="https://twitter.com/maxkreminski/status/964923822766833664">
  this tweet</a>
 about it from Max Kreminski),
which is an excellent thesis on mechanically assisted creativity (I must admit I skimmed the technical content as less relevant to me - I care about the meta more than I care about game design qua game design).</p>


<p>
 Most relevant quote for me:</p>


<blockquote>
<p>
  Despite this apparently
easy pace of interaction, all of the designers reported that they
found the experiments hard work and stressful. They reported that
the main benefit of using the "computer" was increased work
speed, principally by reducing uncertainty (i.e., they relatively
quickly received answers to queries, which they accepted as reliable
information).
I also tried a few variations from my standard experiments. The most interesting was to reverse the normal set of expectations of the functions of the designer and the "computer."
The "computer" was given the job of having to produce a design to the
satisfaction of the observing designer. It immediately was apparent
that, in this situation, there was no stress on the designer—in fact, it
became quite fun—and it was the "computer" that found the experience
to be hard work.</p></blockquote>


<p>
 i.e. it's much more fun to tweak a computer's output than it is to be critiqued by one.
An important observation for people in correctness research I think!</p>


</section>
<section>
<h2><a href="/posts/2018-08-31-09:43.html">2018-08-31</a></h2>


<p class="subtitle">Some free user experience consulting for Google</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-31</dd>
</dl>


<p>
 I am not a UX expert. I've worked with people who are, and I'm probably a lot better than my otherwise utter incompetence at front-end work would suggest,
but I'm at best OK.</p>


<p>
 Nevertheless, as a user I get to see a lot of the sharp edge of the problems, and I'm good enough at UX that I think I can see what the shape of the solution is.</p>


<p>
 The product I would like to offer Google some free advice on is the following: Google Maps's driving navigation.</p>


<p>
 On a related note, if you can recommend a good driving navigation app to me (iPhone, sadly), that would be delightful.
It would be especially useful if it were one that understood features of English roads like "has roundabouts" and "is verrah verrah smol" that seem alien to people from the US (although given how much of Google maps is in Zurich,
I'm still surprised by its failure to understand these).</p>


<p>
 Anyway, free UX consulting.
User stories are cool I hear, so here are my two user stories for Google maps:</p>


<blockquote>
<p>
  As a driver, I would like to survive my trip.</p></blockquote>


<p>
 and</p>


<blockquote>
<p>
  As a driver, I would like to be able to drive without a constant sense of paranoia.</p></blockquote>


<p>
 Currently Google Maps fails both of these so hard that I have conjectured that I have somehow triggered a special murder-mode for ex-Googlers,
because honestly if Google Maps treats most drivers like it treats me then either not many people can be using it or I would have expected a better publicised death toll from it.
I am not actually being hyperbolic here (or even parabolic).</p>


<p>
 Google maps reliably does everything in its power to destroy my trust in it, which is not ideal in something that I have to use while driving.</p>


<p>
 As the most basic minimum that would be required to restore my trust, I would like to propose the following feature:</p>


<blockquote>
<p>
  Google maps should never, under any circumstances, exit navigation without an audible confirmation that it has done so.</p></blockquote>


<p>
 There is what is almost certainly a bug in Google maps where sometimes it just goes "lol, I'm done here" and exits navigation without telling me.
This is
 <em>
  functionally indistinguishable</em>
 from the sort of confirmation Google maps uses to tell me to just keep going straight.
As a result, whenever Google maps is silent for an extended period of time, I end up feeling a gnawing sense of paranoia that it's just not telling me what to do and I'm going in completely the wrong direction.</p>


<p>
 Almost all of the time this is not the case and the correct thing to do is to keep going straight (although Google maps's notion of what "keep going straight" is is often very funny and involves amusing interpretations of the word "going straight" that include things like "turning left" - it is not very good at actually knowing where the road markings are, and if the road follows around to the right it will often confuse a left turn with keep going straight. However, I will forgive it data problems, particularly on the weird back country roads I often drive),
but this bug triggers just often enough (last incidence: about an hour ago) that the exceedingly common operation of
 <em>
  driving in a straight line</em>
 fills me with deep unease whenever I use Google maps for navigation.</p>


<p>
 Even if this bug were fixed, the damage is done, and I will never believe Google maps is still running if it is silent.</p>


<p>
 On top of that, I would like to propose the following feature:</p>


<blockquote>
<p>
  Google maps should never be silent for an extended period of time.</p></blockquote>


<p>
 I'll grant that if the last instructions were "Keep going for 500 miles" it doesn't need to give me a mile counter every five minutes,
but if it could tell me every half hour or so "Yup, everything is cool, keep going" that would be great.
In normal operation,
every five minutes sounds about right.</p>


<p>
 The second source of paranoia is that Google maps gives absolutely no feedback as to when you have done something wrong.
I know the whole nagging satnav going "Make a U-Turn. Make a U-Turn. Make a- *urk* (noise as satnav is thrown out window)" has a bad reputation,
but there's a happy medium: When you do something Google maps does not expect,
it should say something along the lines of "You missed a turn, I'm going to try to turn you around" or "You missed a turn, finding a new route".</p>


<p>
 Fun instances where it was very useful to have a second person in the car yesterday:</p>


<ol>
<li>
  When Google maps took me 30 miles up the wrong motorway before eventually turning me around.</li>
<li>
  When Google maps was very upset that I didn't drive through the traffic cones blocking the route it wanted me to take and insistently tried to turn me around for another go.</li></ol>


<p>
 Feedback that I had done the wrong thing would have been very helpful on the first, because I would have spent a lot of time confused without it.
Feedback on the second that it was taking me around for another pass would also have been very helpful. I would have probably ignored its instructions even without Luke to assist me,
but I would have felt much less certain about it.</p>


<p>
 Anyway, those is the main sources of paranoia.
Lets talk about the other moderately important feature:
Not dying and/or killing people.</p>


<p>
 This is a very simple issue:
Google maps literally never gives you enough advance warning.
This is especially true in the following two cases:</p>


<ul>
<li>
  with motorway driving. If you tell me "In one mile, take the exit" when I am doing 70 mph (yes, um, definitely 70 mph, that's the speed limit after all) in the right hand lane of a motorway,
  you are saying "In the next 30 seconds, merge across three lanes of possibly quite busy traffic". This is a style of advice that will literally kill people and, worse, make them miss their turning.</li>
<li>
  with roundaboutes and other turnings where there is a lane you need to be in, I need to know what lane that is
  <em>
   before</em>
  reaching the roundabout. It happens all the time that I either exit a roundabout,
  leave a motorway and Google maps is like "tum ti tum, la la, nothing to see here, oh hey there's a roundabout coming up. Atttt... theeee.... rouuuundabout.... taaaake..... the.... third... exit....".
  Often I am
  <em>
   on the fucking roundabout</em>
  before it tells me what lane I need to be in.</li></ul>


<p>
 Giving this sort of last minute instruction is deeply unsafe,
and needs to stop.</p>


<p>
 On top of that there's all sorts of data problems and things where Google maps just clearly doesn't understand UK roads,
but I don't realistically expect those to be fixed, especially with the UK dooming itself to irrelevance next year and the only Google UK presence being in a city where you already have to embrace paranoia and risk loss of life and limb to drive in anyway, so I won't bother venting about those now.</p>


<p>
 In the meantime, I'm serious about that desire for recommendations of less murdery navigation apps. Please?</p>


</section>
<section>
<h2><a href="/posts/2018-08-31-06:57.html">2018-08-31</a></h2>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-31</dd>
</dl>


<p>
 I'm a big fan of the
 <a href="https://en.wikipedia.org/wiki/Brzozowski_derivative">
  Brzozowski derivative</a>
 ,
introduced in "Derivatives of regular expressions" by Janusz A. Brzozowski.</p>


<p>
 The basic idea is that given some language \(L\) over an alphabet \(A\),
and some string \(u\) over \(L\),
you can define the derivative language \(\partial(L, u) = \{v: uv \in L\}\).
We can extend this further (and it will be useful to do so below).
If \(M\) is some other language, we can define \(\partial(L, M) = \{v: \exists u \in M, uv \in L\}\).
I'm not currently sure if the derivative of a regular language by a regular langauge is regular in general. It is in the case we'll see later,
and I suspect it is in general.</p>


<p>
 This seems like a pretty trivial observation until you realise the following three things:</p>


<ol>
<li>
  \(u \in L\) if and only if \(\epsilon \in \partial(L, u)\)</li>
<li>
  \(uv \in L\) if and only if \(v \in \partial(L, u)\)</li>
<li>
  For most common representations of languages, it's actually pretty easy to calculate a representation of their derivative.</li></ol>


<p>
 Putting these together, you can use the Brzozowski derivative to calculate a deterministic (not necessarily finite!) automaton for almost any language that you can easily represent.
You label states with descriptions of languages,
a state is accepting if it matches the empty string,
and transitions to the states labelled by the derivatives.</p>


<p>
<a href="http://www.ccs.neu.edu/home/turon/re-deriv.pdf">
  Regular-expression derivatives reexamined</a>
 by Owens et al. has some nice practical details of doing this in the context of functional programming.</p>


<p>
 To see this in action, consider the standard regular expression operators.
These satisfy the following identifies:</p>


<ol>
<li>
  \(\partial(A | B, u) = \partial(A, u) | \partial(B, u)\)</li>
<li>
  \(\partial(AB, u) = \partial(A, u)B | \nu(A) \partial(B, u)\), where \(\nu(A) = \epsilon\) if \(\epsilon \in A\) or \(\emptyset\) otherwise (i.e. the derivative can skip over \(A\) if and only if \(A\) contains the empty string)</li>
<li>
  \(\partial(A^*, u) = \partial(A, u) A^*\)</li></ol>


<p>
 A result proved in Brzozowski's original paper (apparently. I can't currently seem to access it, and am going off thecite in "Regular-expression derivatives reexamined) is that a small number of reasonable normalisation rules over the representation of the language is enough to ensure that you only get finitely many states in the state machine generated by partial derivatives of regular expressions.
It's certainly true that you only get finitely many if you have full equivalence for the regular languages labelling the states - the derivative automaton is actually the minimal automaton representing a language.</p>


<p>
 There are two very nice things about this representation of the language's automaton though:</p>


<ol>
<li>
  It can be done
  <em>
   lazily</em>. This means that even when your deterministic automaton has exponentially (or infinitely!) many states, you only ever need to explore the states that you walk when matching strings.</li>
<li>
  It is very easy to extend with new operators.</li></ol>


<p>
 An example of (2) is that regular expressions reexamined actually does it for extended regular expressions with intersection and negation, because might as well right? It's no harder than doing it with the normal ones, even though adding these to your regular expression language can cause exponential blowup in the size of the automata compiled from your regex.</p>


<p>
 But there are even more interesting ones if you're prepared to go for more esoteric operations!</p>


<p>
 Have you heard of the
 <a href="https://en.wikipedia.org/wiki/Levenshtein_automaton">
  Levenshtein automaton</a>
 ? The set of strings within some finite edit distance of another string is a regular language and you can define a nice automaton matching it.
But in fact, a stronger result is true: For any regular language \(L\) and natural number \(n\), the set \(E(L, n) = \{u: \exists v \in L, d(u, v) \leq n\}\) is a regular language.
Why?</p>


<p>
 Well, we can calculate its derivative!
The derivative of \(E\) is \(\partial(E(L, n), u) = E(\partial(L, u), n) | E(L, n - 1) | E(\partial(L, \cdot), n - 1) | \partial(E(\partial(L, \cdot), n - 1), u)\).
That is, at each character we can either:</p>


<ol>
<li>
  Continue matching the original language (cost 0).</li>
<li>
  Insert a new character in front of something in the original language (cost 1)</li>
<li>
  Replace a character in the original language with \(u\) (cost 1)</li>
<li>
  Drop a character from the original language and try again (cost 1)</li></ol>


<p>
 In the course of doing this we apply the following rewrite rules:</p>


<ol>
<li>
  \(E(L, 0) = L\)</li>
<li>
  \(E(\emptyset, n) = \emptyset\)</li></ol>


<p>
 As long as the number of reachable representations for the original languages is finite,
so is the number of reachable states in our Levenshtein construction:
Every state is labelled by a set of languages of the form \(E(\partial(L, U), k)\) where \(U\) is a language defined by \(u_1 \ldots u_m\) with each \(u_i\) either a single character or a \(\cdot\),
and \(m + k \leq n\). There are only finitely many such labels as long as there are only finitely many derivatives of \(L\),
although in principle there may be exponentially many.
Because of the laziness of our construction that often won't matter - you can still determine membership for a string of length \(k\) with only \(O(k)\) state traversals (though calculating those states could in principle require up to \(O(nm)\) work, where \(m\) is the number of states in the original automaton).</p>


<p>
 You can also use this to determine the minimum edit distance between two regular languages,
because you can test whether \(E(L, n) \cap L' = \emptyset\) by calculating and walking the generated DFA for the left hand side,
so this gives you a decision procedure for \(d(L, L') \leq n\).</p>


<p>
 Is this a practical algorithm? Not sure. I've played with it a little bit, but I've not really put it to the test,
but I think it's an interesting example of the flexibility of the Brzozowski derivative,
and it was at least mildly surprising to me that the edit ball of a regular language is itself regular.</p>


</section>
<section>
<h2><a href="/posts/2018-08-30-12:39.html">2018-08-30</a></h2>


<p class="subtitle">Mathjax and Python Markdown</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-30</dd>
</dl>


<p>
 I've been having an interesting time of things with this notebook and getting Python markdown and Mathjax to play well with each other.
In particular I have not been enjoying the markdown extension API at
 <em>
  all</em>.</p>


<p>
 Anyway, it turns out that it is easy to do what I need, just slightly undocumented and with some annoyingly silent failure modes.</p>


<p>
 Here is the (slightly simplified) code from this notebook that makes MathJax work correctly:</p>


<div class="codehilite">
<pre><span></span><span class="kn">from</span> <span class="nn">markdown.inlinepatterns</span> <span class="kn">import</span> <span class="n">HtmlPattern</span>

<span class="n">LATEX_BLOCK</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"(</span><span class="se">\\</span><span class="s2">begin{[^}]+}.+?</span><span class="se">\\</span><span class="s2">end{[^}]+})"</span>
<span class="n">LATEX_EXPR</span>  <span class="o">=</span> <span class="sa">r</span><span class="s2">"(</span><span class="se">\\</span><span class="s2">\(.+?</span><span class="se">\\</span><span class="s2">\))"</span>


<span class="k">class</span> <span class="nc">MathJaxAlignExtension</span><span class="p">(</span><span class="n">markdown</span><span class="o">.</span><span class="n">Extension</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">extendMarkdown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">md</span><span class="p">,</span> <span class="n">md_globals</span><span class="p">):</span>
        <span class="c1"># Needs to come before escape so that markdown doesn't break use of \ in LaTeX</span>
        <span class="n">md</span><span class="o">.</span><span class="n">inlinePatterns</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">'mathjaxblocks'</span><span class="p">,</span> <span class="n">HtmlPattern</span><span class="p">(</span><span class="n">LATEX_BLOCK</span><span class="p">,</span> <span class="n">md</span><span class="p">),</span> <span class="s1">'&lt;escape'</span><span class="p">)</span>
        <span class="n">md</span><span class="o">.</span><span class="n">inlinePatterns</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">'mathjaxexprs'</span><span class="p">,</span> <span class="n">HtmlPattern</span><span class="p">(</span><span class="n">LATEX_EXPR</span><span class="p">,</span> <span class="n">md</span><span class="p">),</span> <span class="s1">'&lt;escape'</span><span class="p">)</span></pre></div>


<p>
 The HtmlPattern class takes an expression and treats anything matching that expression as something that the markdown processor should not touch further.</p>


<p>
 Some caveats to note:</p>


<ul>
<li>
  Those brackets around the expression? Those are
  <em>
   important</em>. The way that the regular expression processing works is that it messes with your regex a bit, and then uses capturing group \(2\) as the output (\(1\) will be everything in the current block prior to the start of your regex). This means that if you must use groups in your regex, make them named groups.</li>
<li>
  For reasons I haven't fully understood and have chosen not to bother understanding because the current behaviour is correct for my needs, despite allegedly being an HTML block, this extension does seem to do entity escaping on the contents of your MathJax.</li></ul>


</section>
<section>
<h2><a href="/posts/2018-08-30-07:50.html">2018-08-30</a></h2>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-30</dd>
</dl>


<p>
 I'm going to start trying to port over some contents from
 <a href="https://github.com/DRMacIver/research-notebook">
  my research notebook</a>
 into here,
as this is intended long-term to be a replacement for it.
This will require some figuring out in terms of how to present maths.</p>


<p>
 As a starting point,
here's a theorem:</p>


<p>
 \(H(m) = \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \frac{1}{q}\)</p>


<p>
 Where \(H(m)\) is the m'th harmonic number \(H(m) = \sum\limits_{i}^m \frac{1}{i}\).</p>


<p>
 This came up in "Birthday Paradox, Coupon Collectors, Caching Algorithms and Self-Organizing Search" by Flajolet et al. (which is excellent) where it was stated as "well known". It wasn't well known to
 <em>
  me</em>
 ,
so I set out to prove it.</p>


<p>
 The following is my proof:</p>


<p>
 The main idea is to use a standard tricks of turning sums and integrals into other sums and integrals that happen to be easier to solve.
We use the following standard results:</p>


<ul>
<li>
  \(\frac{1}{n} = \int\limits_0^1 x^{n - 1}dx\)</li>
<li>
  \((1 + x)^m = \sum\limits_{q=1}^m {m \choose q} x^q\)</li>
<li>
  \((1 - x)^{-1} = \sum\limits_{q = 0}^\infty x^q\) for \(|x| &lt; 1\).</li></ul>


<p>
 We then perform the following manipulations (don't worry if some of these are clear as mud. They kinda should be):</p>


<p>
 \begin{align}
\sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \frac{1}{q} &amp;= \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \int\limits_0^1 x^{q - 1} dx\\
&amp;= \int\limits_0^1 \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} x^{q - 1} dx\\
&amp;= \int\limits_0^1 -x^{-1} \sum\limits_{q = 1}^m {m \choose q} {(-x)}^q dx\\
&amp;= \int\limits_0^1 -x^{-1} \left( \sum\limits_{q = 0}^m {m \choose q} {(-x)}^q - 1 \right)dx \\
&amp;= \int\limits_0^1 -x^{-1} \left( {(1 - x)}^m - 1 \right)dx \\
&amp;= \int\limits_0^1 {(1 - x)}^{-1} (x^m - 1) dx \\
&amp;= \int\limits_0^1 \sum\limits_{n = 0}^\infty x^n (x^m - 1) dx \\
&amp;= \sum\limits_{n = 0}^\infty \int\limits_0^1 x^n (x^m - 1) \\
&amp;= \sum\limits_{n = 0}^\infty \frac{1}{n + m} - \frac{1}{n} \\
&amp;= \lim\limits_{k \to \infty}  H(m) - \sum\limits_{n = k}^{m + k} \frac{1}{n + m}\\
&amp;= H(m)\\
\end{align}</p>


<p>
 Notable magic tricks performed:</p>


<ul>
<li>
  \(\int\limits_0^1 -x^{-1} \left( {(1 - x)}^m - 1 \right)dx  \to \int\limits_0^1 {(1 - x)}^{-1} (x^m - 1) dx \) is a change of variables \(x \to 1 - x\).</li>
<li>
  \(\sum\limits_{n = 0}^\infty \frac{1}{n + m} - \frac{1}{n} \to \lim\limits_{k \to \infty}  H(m) - \sum\limits_{n = k}^{m + k} \frac{1}{n + m}\) is because you can use a change of variables \(k \to k - m\),
and then group the terms that cancel out.</li>
<li>
  The final limit is because \(|\sum\limits_{n = k}^{m + k} \frac{1}{n + m}| \leq \frac{m}{k}\).</li></ul>


<p>
 This is a style of calculation I think of as the Feynmann style because
 <del>
  it's very good at seeming more clever than it actually is</del>
 he was fond of smugly boasting about using this sort of trick in preference to contour integration.
Given its prevalence prior to Feynmann, my only defence of the terminology is that it's not really intended as a compliment.</p>


<p>
 I find the Feynmann style completely unenlightening to read - the only way to read a Feynmann style proof is to do it yourself, using the original as a guide when you get stuck.</p>


<p>
 I think that's in some ways its point. It's not a proof technique designed to leverage enlightenment,
but instead it leans heavily on your puzzle solving skills. That can be useful sometimes when you just want to brute force your way through a problem and don't really care about understanding it on any sort of deeper level.</p>


<p>
 I was exposed to the Feynmann style quite early on,
due to reading Schaum's Outlines of Advanced Calculus (an earlier edition. I'm not sure how early. Brown covered one. I sadly gave away my copy, and the 1974 edition one I ordered doesn't seem to be quite it) prior to going to university.
It has quite a lot of exercises using calculations like this,
and afterwards I realised that this is what Feynmann had been talking about in "Surely you're joking, Mr Feynmann" (I didn't understand what a contour integral was until a few years later).</p>


<p>
 Somehow despite this the Feynmann style of brute force problem solving never really integrated into my mathematics,
and it's only some years later I've come to appreciate its merits.
I
 <em>
  still</em>
 prefer to achieve insight and make the problem trivial,
but sometimes the problem isn't worth the insight and you're better off just putting in the hard work and solving it.</p>


<p>
 Putting in the hard work is also useful because sometimes it leads you to the insight you missed and you can throw away most of the work.
This didn't happen here,
but I think that's OK - it's not that interesting a problem,
so I don't really feel upset by the lack of insight into it.</p>


</section>
<section>
<h2><a href="/posts/2018-08-29-09:35.html">2018-08-29</a></h2>


<p class="subtitle">Notes on tiling with polyominoes</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-29</dd>
</dl>


<p>
 Gary Fredericks wrote about
 <a href="https://gfredericks.com/gfrlog/99">
  a backtracking algorithm for tiling a board with polyominoes</a>.</p>


<p>
 His solution is roughly "turn the problem into exact cover and then apply a bunch of interesting optimisations in this context to the naive backtracking algorithm".
The paper
 <a href="https://arxiv.org/pdf/cs/0011047.pdf">
  Dancing Links</a>
 by Donald E. Knuth in fact studies this exact problem as an application of the exact cover algorithm.</p>


<p>
 I think some of the optimisations Gary performs are not ones that would be performed by a modern SAT solver because they are actually too expensive to be worth it if you're good at the SAT problem-e.g.
I know modern SAT solvers tend not to bother decomposing problems into independent problems because the cost is too high-but
it's possible they synergise well enough to be worth it. e.g. the number theory optimisation combined with the independent components may well be worth it,
especially with the heuristic of prioritising moves that disconnect the board.</p>


<p>
 I've been doing a bit of casual reading about this class of problem recently.
I thought I'd use the opportunity of this new notebook to collect some references.
Ideally these would be proper cites,
but I haven't got the citation part of the notebook system working yet.</p>


<p>
<a href="https://www.jstor.org/stable/pdf/2307321.pdf">
  Checker Boards and Polyominoes</a>
 by Solomon W. Golomb is a classic here.
It looks at the question of tiling the chessboard with a single square monomino and 11 tetrominos of various shapes.
In particular it establishes:</p>


<ul>
<li>
  You can do this with right tetrominoes given any placement of the monomino</li>
<li>
  There are only four squares where you can place the monomino if you want to do it with straight tetrominoes.</li></ul>


<p>
<a href="http://chalkdustmagazine.com/blog/polyominoes/">
  How to Tile a Chessboard</a>
 by Trupti Patel is a nice expository piece on this.</p>


<p>
 Golomb also wrote
 <a href="http://publisher-connector.core.ac.uk/resourcesync/data/elsevier/pdf/03f/aHR0cDovL2FwaS5lbHNldmllci5jb20vY29udGVudC9hcnRpY2xlL3BpaS9zMDAyMTk4MDA2NjgwMDMzOQ%3D%3D.pdf">
  Tiling with Polyominoes</a>
 ,
studying much more general questions of how to tile truncated chessboards with polyominoes.</p>


<p>
 A classic version of this is what
 <a href="https://en.wikipedia.org/wiki/Mutilated_chessboard_problem">
  Wikipedia refers to as the mutilated chessboard problem</a>
 (apparently following Max Black):</p>


<blockquote>
<p>
  Suppose a standard 8×8 chessboard has two diagonally opposite corners removed, leaving 62 squares. Is it possible to place 31 dominoes of size 2×1 so as to cover all of these squares?</p></blockquote>


<p>
 The answer is no. In
 <a href="https://www.tandfonline.com/doi/pdf/10.1080/07468342.2004.11922062">
  Tiling with Dominoes</a>
 , N. S. Mendelsohn discusses two proofs:</p>


<blockquote>
<h3>
  First solution</h3>
<p>
  From the checkerboard diagram, the region contains 30 black cells and 32 white cells.
Since each domino covers 1 black and 1 white cell, tiling is impossible.</p>
<h3>
  Second solution</h3>
<p>
  When I was first shown the problem many years ago, it did not occur to me to colour
the cells. The region itself had seven cells in the top and bottom rows and eight cells in
the remaining rows. The same held for the columns. I proceeded to obtain information
on how many dominoes pointed horizontally and how many vertically. The first count
dealt with the vertical dominoes. If the region is tiled, the horizontal dominoes in the
top row occupies an even number of cells. Hence, the cells in the top row that are not
occupied by horizontal dominoes are odd in number. Thus there are an odd number of
vertical dominoes between the first and second rows. Since the second row has eight
cells, and an odd number are occupied by vertical dominoes coming down from the
first row, there remain an odd number of cells in the second row. The same argument
now shows there is an odd number of vertical dominoes from the second row to the
third. Continuing this way, we see that there is an odd number of vertical dominoes
between any pair of consecutive rows. Hence the total number of vertical dominoes is
the sum of seven odd numbers, which is odd. In the same way, using columns instead
of rows, there is an odd number of horizontal dominoes. Hence the total number of
dominoes is even. Since there are 62 cells to cover, the number of dominoes required
is 31, an odd number. Therefore, tiling is impossible.</p></blockquote>


<p>
 He goes on to say:</p>


<blockquote>
<p>
  Why do I produce two solutions to the puzzle? It is because I am interested in
the question of which is the better solution. At first glance, it appears that the first
solution is the better. It is much shorter and is easily understood by many people with
virtually no knowledge of mathematics. But are there considerations that might judge
the second solution to be the better one?</p></blockquote>


<p>
 He then discusses whether the second one is better because it generalises better,
when setting out to prove Gomory's theorem (which I've not been able to find a copy of the original of so far, but I haven't looked very hard):
If you remove two squares of the same colour, you can always tiling the remainder with dominoes.
The proof involves the construction of a hamiltonian circuit on the adjacency graph,
and seems fiddly but interesting.
I've only skimmed it and would like to digest it further.</p>


<p>
 However note that we saw a generalisation in a different direction in the first paper linked! Golomb's proof of the impossibility tiling with straight tetrominoes unless the monomino was in a very specific location was
 <em>
  also</em>
 a colouring argument.</p>


<p>
 The wikipedia page references "Across the board: the mathematics of chessboard problems" by John J. Watkins.
I should probably look up a copy.</p>


</section>
<section>
<h2><a href="/posts/2018-08-28-08:14.html">2018-08-28</a></h2>


<p class="subtitle">First!</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-28</dd>
</dl>


<p>
 This is an experimental new blog intended for notes, thoughts, and whatever else I want to put here.
It will likely be biased towards short notes rather than longform essays.
It's loosely inspired by
 <a href="https://shitpost.plover.com/">
  Mark Jason Dominus's shitposting blog</a>
 and by my frustrations with WordPress, but I'm not really sure where it's going yet.</p>


<p>
 It's also a place where I'll be experimenting with notation,
and generally trying to find a low friction way to express myself in a manner that I like.
As such it's all a bit cobbled together out of spit, bailing wire, and Python.</p>


<h3>
 Notational Highlights</h3>


<p>
 I kinda hate LaTeX, but it's the best typesetting language for mathematics that I know of,
so this notebook supports it using
 <a href="https://www.mathjax.org/">
  mathjax</a>.</p>


<p>
 Testing: \(e^{i\pi} = -1\)</p>


<p>
 A test of code highlighting.</p>


<div class="codehilite">
<pre><span></span><span class="k">class</span> <span class="nc">SomeClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">""""A python class"""</span>

    <span class="k">def</span> <span class="nf">method</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""A method definition"""</span></pre></div>


<p>
 As you've probably noticed, I'm using
 <a href="https://edwardtufte.github.io/tufte-css/">
  Tufte CSS</a>.
I'm not sure it's exactly what I want, but it's a lot closer to what I want than most other things I've tried.
I will likely be messing aroudn with this further.</p>


<p>
 I'm also using
 <a href="http://www.makotemplates.org">
  mako templates</a>
 ,
and fully intend to define a metric tonne of macros to make this usable.</p>


<p>
 In general I expect the actual source code for this site to be totally unusable to anyone who is not me.
If anything,
if it's
 <em>
  not</em>
 then I probably haven't done enough customization for my brain.</p>


</section>

    </article>
<footer>
Copyright David R. MacIver.

CSS mostly due to <a href="https://edwardtufte.github.io/tufte-css/">Tufte CSS</a> by Dave Liepmann.
</footer>
  </body>
</html>
