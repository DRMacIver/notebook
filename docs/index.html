<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>
DRMacIver's Notebook: Thoughts from David R. MacIver
    </title>
    <link rel="stylesheet" href="/pygments.css"/>
    <link rel="stylesheet" href="/tufte.css"/>
    <link rel="stylesheet" href="/latex.css"/>
    <link rel="stylesheet" href="/drmnotes.css"/>
    <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/feed.xml" />

    <meta name="viewport" content="width=device-width, initial-scale=1">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(', '\\)']]},
  multiLine: true,
  "HTML-CSS": { 
       linebreaks: { automatic: true }
  },
  SVG: { 
       linebreaks: { automatic: true } 
  }
});
</script>

  </head>

  <body>
    <article>
        <h1><a href="/">DRMacIver's Notebook</a></h1>
        <p class=subtitle>Thoughts from David R. MacIver</p>

        

<section>
<h2><a href="/posts/2018-09-01-09:17.html">2018-09-01</a></h2>


<p class="subtitle">Can a machine design?</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-09-01</dd>
</dl>


<p>
<a href="http://echo.iat.sfu.ca/library/cross_01_machine_des.pdf">
  Can a machine design?</a>
 by Nigel Cross is an interesting paper about architecture (the real kind!) and its relation to automation.
I found it via Adam Marshall Smith's PhD thesis
 <a href="https://adamsmith.as/papers/mechanizing_exploratory_game_design_book.pdf">
  Mechanizing exploratory game design</a>
 (truthfully via
 <a href="https://twitter.com/maxkreminski/status/964923822766833664">
  this tweet</a>
 about it from Max Kreminski),
which is an excellent thesis on mechanically assisted creativity (I must admit I skimmed the technical content as less relevant to me - I care about the meta more than I care about game design qua game design).</p>


<p>
 Most relevant quote for me:</p>


<blockquote>
<p>
  Despite this apparently
easy pace of interaction, all of the designers reported that they
found the experiments hard work and stressful. They reported that
the main benefit of using the
  <code>
   computer'' was increased work
speed, principally by reducing uncertainty (i.e., they relatively
quickly received answers to queries, which they accepted as reliable
information).
I also tried a few variations from my standard experiments. The most interesting was to reverse the normal set of expectations of the functions of the designer and the</code>
  computer.''
The
  <code>
   computer'' was given the job of having to produce a design to the
satisfaction of the observing designer. It immediately was apparent
that, in this situation, there was no stress on the designer—in fact, it
became quite fun—and it was the</code>
  computer'' that found the experience
to be hard work.</p></blockquote>


<p>
 i.e.\ it's much more fun to tweak a computer's output than it is to be critiqued by one.
An important observation for people in correctness research I think!</p>


</section>
<section>
<h2><a href="/posts/2018-08-31-09:43.html">2018-08-31</a></h2>


<p class="subtitle">Some free user experience consulting for Google</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-31</dd>
</dl>


<p>
 I am not a UX expert. I've worked with people who are, and I'm probably a lot better than my otherwise utter incompetence at front-end work would suggest,
but I'm at best OK.</p>


<p>
 Nevertheless, as a user I get to see a lot of the sharp edge of the problems, and I'm good enough at UX that I think I can see what the shape of the solution is.</p>


<p>
 The product I would like to offer Google some free advice on is the following: Google Maps's driving navigation.</p>


<p>
 On a related note, if you can recommend a good driving navigation app to me (iPhone, sadly), that would be delightful.
It would be especially useful if it were one that understood features of English roads like "has roundabouts" and "is verrah verrah smol" that seem alien to people from the US (although given how much of Google maps is in Zurich,
I'm still surprised by its failure to understand these).</p>


<p>
 Anyway, free UX consulting.
User stories are cool I hear, so here are my two user stories for Google maps:</p>


<blockquote>
<p>
  As a driver, I would like to survive my trip.</p></blockquote>


<p>
 and</p>


<blockquote>
<p>
  As a driver, I would like to be able to drive without a constant sense of paranoia.</p></blockquote>


<p>
 Currently Google Maps fails both of these so hard that I have conjectured that I have somehow triggered a special murder-mode for ex-Googlers,
because honestly if Google Maps treats most drivers like it treats me then either not many people can be using it or I would have expected a better publicised death toll from it.
I am not actually being hyperbolic here (or even parabolic).</p>


<p>
 Google maps reliably does everything in its power to destroy my trust in it, which is not ideal in something that I have to use while driving.</p>


<p>
 As the most basic minimum that would be required to restore my trust, I would like to propose the following feature:</p>


<blockquote>
<p>
  Google maps should never, under any circumstances, exit navigation without an audible confirmation that it has done so.</p></blockquote>


<p>
 There is what is almost certainly a bug in Google maps where sometimes it just goes "lol, I'm done here" and exits navigation without telling me.
This is
 <em>
  functionally indistinguishable</em>
 from the sort of confirmation Google maps uses to tell me to just keep going straight.
As a result, whenever Google maps is silent for an extended period of time, I end up feeling a gnawing sense of paranoia that it's just not telling me what to do and I'm going in completely the wrong direction.</p>


<p>
 Almost all of the time this is not the case and the correct thing to do is to keep going straight (although Google maps's notion of what "keep going straight" is is often very funny and involves amusing interpretations of the word "going straight" that include things like "turning left" - it is not very good at actually knowing where the road markings are, and if the road follows around to the right it will often confuse a left turn with keep going straight. However, I will forgive it data problems, particularly on the weird back country roads I often drive),
but this bug triggers just often enough (last incidence: about an hour ago) that the exceedingly common operation of
 <em>
  driving in a straight line</em>
 fills me with deep unease whenever I use Google maps for navigation.</p>


<p>
 Even if this bug were fixed, the damage is done, and I will never believe Google maps is still running if it is silent.</p>


<p>
 On top of that, I would like to propose the following feature:</p>


<blockquote>
<p>
  Google maps should never be silent for an extended period of time.</p></blockquote>


<p>
 I'll grant that if the last instructions were "Keep going for 500 miles" it doesn't need to give me a mile counter every five minutes,
but if it could tell me every half hour or so "Yup, everything is cool, keep going" that would be great.
In normal operation,
every five minutes sounds about right.</p>


<p>
 The second source of paranoia is that Google maps gives absolutely no feedback as to when you have done something wrong.
I know the whole nagging satnav going "Make a U-Turn. Make a U-Turn. Make a- *urk* (noise as satnav is thrown out window)" has a bad reputation,
but there's a happy medium: When you do something Google maps does not expect,
it should say something along the lines of "You missed a turn, I'm going to try to turn you around" or "You missed a turn, finding a new route".</p>


<p>
 Fun instances where it was very useful to have a second person in the car yesterday:</p>


<ol>
<li>
  When Google maps took me 30 miles up the wrong motorway before eventually turning me around.</li>
<li>
  When Google maps was very upset that I didn't drive through the traffic cones blocking the route it wanted me to take and insistently tried to turn me around for another go.</li></ol>


<p>
 Feedback that I had done the wrong thing would have been very helpful on the first, because I would have spent a lot of time confused without it.
Feedback on the second that it was taking me around for another pass would also have been very helpful. I would have probably ignored its instructions even without Luke to assist me,
but I would have felt much less certain about it.</p>


<p>
 Anyway, those is the main sources of paranoia.
Lets talk about the other moderately important feature:
Not dying and/or killing people.</p>


<p>
 This is a very simple issue:
Google maps literally never gives you enough advance warning.
This is especially true in the following two cases:</p>


<ul>
<li>
  with motorway driving. If you tell me "In one mile, take the exit" when I am doing 70 mph (yes, um, definitely 70 mph, that's the speed limit after all) in the right hand lane of a motorway,
  you are saying "In the next 30 seconds, merge across three lanes of possibly quite busy traffic". This is a style of advice that will literally kill people and, worse, make them miss their turning.</li>
<li>
  with roundaboutes and other turnings where there is a lane you need to be in, I need to know what lane that is
  <em>
   before</em>
  reaching the roundabout. It happens all the time that I either exit a roundabout,
  leave a motorway and Google maps is like "tum ti tum, la la, nothing to see here, oh hey there's a roundabout coming up. Atttt... theeee.... rouuuundabout.... taaaake..... the.... third... exit....".
  Often I am
  <em>
   on the fucking roundabout</em>
  before it tells me what lane I need to be in.</li></ul>


<p>
 Giving this sort of last minute instruction is deeply unsafe,
and needs to stop.</p>


<p>
 On top of that there's all sorts of data problems and things where Google maps just clearly doesn't understand UK roads,
but I don't realistically expect those to be fixed, especially with the UK dooming itself to irrelevance next year and the only Google UK presence being in a city where you already have to embrace paranoia and risk loss of life and limb to drive in anyway, so I won't bother venting about those now.</p>


<p>
 In the meantime, I'm serious about that desire for recommendations of less murdery navigation apps. Please?</p>


</section>
<section>
<h2><a href="/posts/2018-08-31-06:57.html">2018-08-31</a></h2>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-31</dd>
</dl>


<p>
 I'm a big fan of the
 <a href="https://en.wikipedia.org/wiki/Brzozowski_derivative">
  Brzozowski derivative</a>
 ,
introduced in "Derivatives of regular expressions" by Janusz A. Brzozowski.</p>


<p>
 The basic idea is that given some language \(L\) over an alphabet \(A\),
and some string \(u\) over \(L\),
you can define the derivative language \(\partial(L, u) = \{v: uv \in L\}\).
We can extend this further (and it will be useful to do so below).
If \(M\) is some other language, we can define \(\partial(L, M) = \{v: \exists u \in M, uv \in L\}\).
I'm not currently sure if the derivative of a regular language by a regular langauge is regular in general. It is in the case we'll see later,
and I suspect it is in general.</p>


<p>
 This seems like a pretty trivial observation until you realise the following three things:</p>


<ol>
<li>
  \(u \in L\) if and only if \(\epsilon \in \partial(L, u)\)</li>
<li>
  \(uv \in L\) if and only if \(v \in \partial(L, u)\)</li>
<li>
  For most common representations of languages, it's actually pretty easy to calculate a representation of their derivative.</li></ol>


<p>
 Putting these together, you can use the Brzozowski derivative to calculate a deterministic (not necessarily finite!) automaton for almost any language that you can easily represent.
You label states with descriptions of languages,
a state is accepting if it matches the empty string,
and transitions to the states labelled by the derivatives.</p>


<p>
<a href="http://www.ccs.neu.edu/home/turon/re-deriv.pdf">
  Regular-expression derivatives reexamined</a>
 by Owens et al. has some nice practical details of doing this in the context of functional programming.</p>


<p>
 To see this in action, consider the standard regular expression operators.
These satisfy the following identifies:</p>


<ol>
<li>
  \(\partial(A | B, u) = \partial(A, u) | \partial(B, u)\)</li>
<li>
  \(\partial(AB, u) = \partial(A, u)B | \nu(A) \partial(B, u)\), where \(\nu(A) = \epsilon\) if \(\epsilon \in A\) or \(\emptyset\) otherwise (i.e. the derivative can skip over \(A\) if and only if \(A\) contains the empty string)</li>
<li>
  \(\partial(A^*, u) = \partial(A, u) A^*\)</li></ol>


<p>
 A result proved in Brzozowski's original paper (apparently. I can't currently seem to access it, and am going off thecite in "Regular-expression derivatives reexamined) is that a small number of reasonable normalisation rules over the representation of the language is enough to ensure that you only get finitely many states in the state machine generated by partial derivatives of regular expressions.
It's certainly true that you only get finitely many if you have full equivalence for the regular languages labelling the states - the derivative automaton is actually the minimal automaton representing a language.</p>


<p>
 There are two very nice things about this representation of the language's automaton though:</p>


<ol>
<li>
  It can be done
  <em>
   lazily</em>. This means that even when your deterministic automaton has exponentially (or infinitely!) many states, you only ever need to explore the states that you walk when matching strings.</li>
<li>
  It is very easy to extend with new operators.</li></ol>


<p>
 An example of (2) is that regular expressions reexamined actually does it for extended regular expressions with intersection and negation, because might as well right? It's no harder than doing it with the normal ones, even though adding these to your regular expression language can cause exponential blowup in the size of the automata compiled from your regex.</p>


<p>
 But there are even more interesting ones if you're prepared to go for more esoteric operations!</p>


<p>
 Have you heard of the
 <a href="https://en.wikipedia.org/wiki/Levenshtein_automaton">
  Levenshtein automaton</a>
 ? The set of strings within some finite edit distance of another string is a regular language and you can define a nice automaton matching it.
But in fact, a stronger result is true: For any regular language \(L\) and natural number \(n\), the set \(E(L, n) = \{u: \exists v \in L, d(u, v) \leq n\}\) is a regular language.
Why?</p>


<p>
 Well, we can calculate its derivative!
The derivative of \(E\) is \(\partial(E(L, n), u) = E(\partial(L, u), n) | E(L, n - 1) | E(\partial(L, \cdot), n - 1) | \partial(E(\partial(L, \cdot), n - 1), u)\).
That is, at each character we can either:</p>


<ol>
<li>
  Continue matching the original language (cost 0).</li>
<li>
  Insert a new character in front of something in the original language (cost 1)</li>
<li>
  Replace a character in the original language with \(u\) (cost 1)</li>
<li>
  Drop a character from the original language and try again (cost 1)</li></ol>


<p>
 In the course of doing this we apply the following rewrite rules:</p>


<ol>
<li>
  \(E(L, 0) = L\)</li>
<li>
  \(E(\emptyset, n) = \emptyset\)</li></ol>


<p>
 As long as the number of reachable representations for the original languages is finite,
so is the number of reachable states in our Levenshtein construction:
Every state is labelled by a set of languages of the form \(E(\partial(L, U), k)\) where \(U\) is a language defined by \(u_1 \ldots u_m\) with each \(u_i\) either a single character or a \(\cdot\),
and \(m + k \leq n\). There are only finitely many such labels as long as there are only finitely many derivatives of \(L\),
although in principle there may be exponentially many.
Because of the laziness of our construction that often won't matter - you can still determine membership for a string of length \(k\) with only \(O(k)\) state traversals (though calculating those states could in principle require up to \(O(nm)\) work, where \(m\) is the number of states in the original automaton).</p>


<p>
 You can also use this to determine the minimum edit distance between two regular languages,
because you can test whether \(E(L, n) \cap L' = \emptyset\) by calculating and walking the generated DFA for the left hand side,
so this gives you a decision procedure for \(d(L, L') \leq n\).</p>


<p>
 Is this a practical algorithm? Not sure. I've played with it a little bit, but I've not really put it to the test,
but I think it's an interesting example of the flexibility of the Brzozowski derivative,
and it was at least mildly surprising to me that the edit ball of a regular language is itself regular.</p>


</section>
<section>
<h2><a href="/posts/2018-08-30-12:39.html">2018-08-30</a></h2>


<p class="subtitle">Mathjax and Python Markdown</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-30</dd>
</dl>


<p>
 I've been having an interesting time of things with this notebook and getting Python markdown and Mathjax to play well with each other.
In particular I have not been enjoying the markdown extension API at
 <em>
  all</em>.</p>


<p>
 Anyway, it turns out that it is easy to do what I need, just slightly undocumented and with some annoyingly silent failure modes.</p>


<p>
 Here is the (slightly simplified) code from this notebook that makes MathJax work correctly:</p>


<div class="codehilite">
<pre><span></span><span class="kn">from</span> <span class="nn">markdown.inlinepatterns</span> <span class="kn">import</span> <span class="n">HtmlPattern</span>

<span class="n">LATEX_BLOCK</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"(</span><span class="se">\\</span><span class="s2">begin{[^}]+}.+?</span><span class="se">\\</span><span class="s2">end{[^}]+})"</span>
<span class="n">LATEX_EXPR</span>  <span class="o">=</span> <span class="sa">r</span><span class="s2">"(</span><span class="se">\\</span><span class="s2">\(.+?</span><span class="se">\\</span><span class="s2">\))"</span>


<span class="k">class</span> <span class="nc">MathJaxAlignExtension</span><span class="p">(</span><span class="n">markdown</span><span class="o">.</span><span class="n">Extension</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">extendMarkdown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">md</span><span class="p">,</span> <span class="n">md_globals</span><span class="p">):</span>
        <span class="c1"># Needs to come before escape so that markdown doesn't break use of \ in LaTeX</span>
        <span class="n">md</span><span class="o">.</span><span class="n">inlinePatterns</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">'mathjaxblocks'</span><span class="p">,</span> <span class="n">HtmlPattern</span><span class="p">(</span><span class="n">LATEX_BLOCK</span><span class="p">,</span> <span class="n">md</span><span class="p">),</span> <span class="s1">'&lt;escape'</span><span class="p">)</span>
        <span class="n">md</span><span class="o">.</span><span class="n">inlinePatterns</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">'mathjaxexprs'</span><span class="p">,</span> <span class="n">HtmlPattern</span><span class="p">(</span><span class="n">LATEX_EXPR</span><span class="p">,</span> <span class="n">md</span><span class="p">),</span> <span class="s1">'&lt;escape'</span><span class="p">)</span></pre></div>


<p>
 The HtmlPattern class takes an expression and treats anything matching that expression as something that the markdown processor should not touch further.</p>


<p>
 Some caveats to note:</p>


<ul>
<li>
  Those brackets around the expression? Those are
  <em>
   important</em>. The way that the regular expression processing works is that it messes with your regex a bit, and then uses capturing group \(2\) as the output (\(1\) will be everything in the current block prior to the start of your regex). This means that if you must use groups in your regex, make them named groups.</li>
<li>
  For reasons I haven't fully understood and have chosen not to bother understanding because the current behaviour is correct for my needs, despite allegedly being an HTML block, this extension does seem to do entity escaping on the contents of your MathJax.</li></ul>


</section>
<section>
<h2><a href="/posts/2018-08-30-07:50.html">2018-08-30</a></h2>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-30</dd>
</dl>


<p>
 I'm going to start trying to port over some contents from
 <a href="https://github.com/DRMacIver/research-notebook">
  my research notebook</a>
 into here,
as this is intended long-term to be a replacement for it.
This will require some figuring out in terms of how to present maths.</p>


<p>
 As a starting point,
here's a theorem:</p>


<p>
 \(H(m) = \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \frac{1}{q}\)</p>


<p>
 Where \(H(m)\) is the m'th harmonic number \(H(m) = \sum\limits_{i}^m \frac{1}{i}\).</p>


<p>
 This came up in "Birthday Paradox, Coupon Collectors, Caching Algorithms and Self-Organizing Search" by Flajolet et al. (which is excellent) where it was stated as "well known". It wasn't well known to
 <em>
  me</em>
 ,
so I set out to prove it.</p>


<p>
 The following is my proof:</p>


<p>
 The main idea is to use a standard tricks of turning sums and integrals into other sums and integrals that happen to be easier to solve.
We use the following standard results:</p>


<ul>
<li>
  \(\frac{1}{n} = \int\limits_0^1 x^{n - 1}dx\)</li>
<li>
  \((1 + x)^m = \sum\limits_{q=1}^m {m \choose q} x^q\)</li>
<li>
  \((1 - x)^{-1} = \sum\limits_{q = 0}^\infty x^q\) for \(|x| &lt; 1\).</li></ul>


<p>
 We then perform the following manipulations (don't worry if some of these are clear as mud. They kinda should be):</p>


<p>
 \begin{align}
\sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \frac{1}{q} &amp;= \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \int\limits_0^1 x^{q - 1} dx\\
&amp;= \int\limits_0^1 \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} x^{q - 1} dx\\
&amp;= \int\limits_0^1 -x^{-1} \sum\limits_{q = 1}^m {m \choose q} {(-x)}^q dx\\
&amp;= \int\limits_0^1 -x^{-1} \left( \sum\limits_{q = 0}^m {m \choose q} {(-x)}^q - 1 \right)dx \\
&amp;= \int\limits_0^1 -x^{-1} \left( {(1 - x)}^m - 1 \right)dx \\
&amp;= \int\limits_0^1 {(1 - x)}^{-1} (x^m - 1) dx \\
&amp;= \int\limits_0^1 \sum\limits_{n = 0}^\infty x^n (x^m - 1) dx \\
&amp;= \sum\limits_{n = 0}^\infty \int\limits_0^1 x^n (x^m - 1) \\
&amp;= \sum\limits_{n = 0}^\infty \frac{1}{n + m} - \frac{1}{n} \\
&amp;= \lim\limits_{k \to \infty}  H(m) - \sum\limits_{n = k}^{m + k} \frac{1}{n + m}\\
&amp;= H(m)\\
\end{align}</p>


<p>
 Notable magic tricks performed:</p>


<ul>
<li>
  \(\int\limits_0^1 -x^{-1} \left( {(1 - x)}^m - 1 \right)dx  \to \int\limits_0^1 {(1 - x)}^{-1} (x^m - 1) dx \) is a change of variables \(x \to 1 - x\).</li>
<li>
  \(\sum\limits_{n = 0}^\infty \frac{1}{n + m} - \frac{1}{n} \to \lim\limits_{k \to \infty}  H(m) - \sum\limits_{n = k}^{m + k} \frac{1}{n + m}\) is because you can use a change of variables \(k \to k - m\),
and then group the terms that cancel out.</li>
<li>
  The final limit is because \(|\sum\limits_{n = k}^{m + k} \frac{1}{n + m}| \leq \frac{m}{k}\).</li></ul>


<p>
 This is a style of calculation I think of as the Feynmann style because
 <del>
  it's very good at seeming more clever than it actually is</del>
 he was fond of smugly boasting about using this sort of trick in preference to contour integration.
Given its prevalence prior to Feynmann, my only defence of the terminology is that it's not really intended as a compliment.</p>


<p>
 I find the Feynmann style completely unenlightening to read - the only way to read a Feynmann style proof is to do it yourself, using the original as a guide when you get stuck.</p>


<p>
 I think that's in some ways its point. It's not a proof technique designed to leverage enlightenment,
but instead it leans heavily on your puzzle solving skills. That can be useful sometimes when you just want to brute force your way through a problem and don't really care about understanding it on any sort of deeper level.</p>


<p>
 I was exposed to the Feynmann style quite early on,
due to reading Schaum's Outlines of Advanced Calculus (an earlier edition. I'm not sure how early. Brown covered one. I sadly gave away my copy, and the 1974 edition one I ordered doesn't seem to be quite it) prior to going to university.
It has quite a lot of exercises using calculations like this,
and afterwards I realised that this is what Feynmann had been talking about in "Surely you're joking, Mr Feynmann" (I didn't understand what a contour integral was until a few years later).</p>


<p>
 Somehow despite this the Feynmann style of brute force problem solving never really integrated into my mathematics,
and it's only some years later I've come to appreciate its merits.
I
 <em>
  still</em>
 prefer to achieve insight and make the problem trivial,
but sometimes the problem isn't worth the insight and you're better off just putting in the hard work and solving it.</p>


<p>
 Putting in the hard work is also useful because sometimes it leads you to the insight you missed and you can throw away most of the work.
This didn't happen here,
but I think that's OK - it's not that interesting a problem,
so I don't really feel upset by the lack of insight into it.</p>


</section>
<section>
<h2><a href="/posts/2018-08-29-09:35.html">2018-08-29</a></h2>


<p class="subtitle">Notes on tiling with polyominoes</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-29</dd>
</dl>


<p>
 Gary Fredericks wrote about
 <a href="https://gfredericks.com/gfrlog/99">
  a backtracking algorithm for tiling a board with polyominoes</a>.</p>


<p>
 His solution is roughly "turn the problem into exact cover and then apply a bunch of interesting optimisations in this context to the naive backtracking algorithm".
The paper
 <a href="https://arxiv.org/pdf/cs/0011047.pdf">
  Dancing Links</a>
 by Donald E. Knuth in fact studies this exact problem as an application of the exact cover algorithm.</p>


<p>
 I think some of the optimisations Gary performs are not ones that would be performed by a modern SAT solver because they are actually too expensive to be worth it if you're good at the SAT problem-e.g.
I know modern SAT solvers tend not to bother decomposing problems into independent problems because the cost is too high-but
it's possible they synergise well enough to be worth it. e.g. the number theory optimisation combined with the independent components may well be worth it,
especially with the heuristic of prioritising moves that disconnect the board.</p>


<p>
 I've been doing a bit of casual reading about this class of problem recently.
I thought I'd use the opportunity of this new notebook to collect some references.
Ideally these would be proper cites,
but I haven't got the citation part of the notebook system working yet.</p>


<p>
<a href="https://www.jstor.org/stable/pdf/2307321.pdf">
  Checker Boards and Polyominoes</a>
 by Solomon W. Golomb is a classic here.
It looks at the question of tiling the chessboard with a single square monomino and 11 tetrominos of various shapes.
In particular it establishes:</p>


<ul>
<li>
  You can do this with right tetrominoes given any placement of the monomino</li>
<li>
  There are only four squares where you can place the monomino if you want to do it with straight tetrominoes.</li></ul>


<p>
<a href="http://chalkdustmagazine.com/blog/polyominoes/">
  How to Tile a Chessboard</a>
 by Trupti Patel is a nice expository piece on this.</p>


<p>
 Golomb also wrote
 <a href="http://publisher-connector.core.ac.uk/resourcesync/data/elsevier/pdf/03f/aHR0cDovL2FwaS5lbHNldmllci5jb20vY29udGVudC9hcnRpY2xlL3BpaS9zMDAyMTk4MDA2NjgwMDMzOQ%3D%3D.pdf">
  Tiling with Polyominoes</a>
 ,
studying much more general questions of how to tile truncated chessboards with polyominoes.</p>


<p>
 A classic version of this is what
 <a href="https://en.wikipedia.org/wiki/Mutilated_chessboard_problem">
  Wikipedia refers to as the mutilated chessboard problem</a>
 (apparently following Max Black):</p>


<blockquote>
<p>
  Suppose a standard 8×8 chessboard has two diagonally opposite corners removed, leaving 62 squares. Is it possible to place 31 dominoes of size 2×1 so as to cover all of these squares?</p></blockquote>


<p>
 The answer is no. In
 <a href="https://www.tandfonline.com/doi/pdf/10.1080/07468342.2004.11922062">
  Tiling with Dominoes</a>
 , N. S. Mendelsohn discusses two proofs:</p>


<blockquote>
<h3>
  First solution</h3>
<p>
  From the checkerboard diagram, the region contains 30 black cells and 32 white cells.
Since each domino covers 1 black and 1 white cell, tiling is impossible.</p>
<h3>
  Second solution</h3>
<p>
  When I was first shown the problem many years ago, it did not occur to me to colour
the cells. The region itself had seven cells in the top and bottom rows and eight cells in
the remaining rows. The same held for the columns. I proceeded to obtain information
on how many dominoes pointed horizontally and how many vertically. The first count
dealt with the vertical dominoes. If the region is tiled, the horizontal dominoes in the
top row occupies an even number of cells. Hence, the cells in the top row that are not
occupied by horizontal dominoes are odd in number. Thus there are an odd number of
vertical dominoes between the first and second rows. Since the second row has eight
cells, and an odd number are occupied by vertical dominoes coming down from the
first row, there remain an odd number of cells in the second row. The same argument
now shows there is an odd number of vertical dominoes from the second row to the
third. Continuing this way, we see that there is an odd number of vertical dominoes
between any pair of consecutive rows. Hence the total number of vertical dominoes is
the sum of seven odd numbers, which is odd. In the same way, using columns instead
of rows, there is an odd number of horizontal dominoes. Hence the total number of
dominoes is even. Since there are 62 cells to cover, the number of dominoes required
is 31, an odd number. Therefore, tiling is impossible.</p></blockquote>


<p>
 He goes on to say:</p>


<blockquote>
<p>
  Why do I produce two solutions to the puzzle? It is because I am interested in
the question of which is the better solution. At first glance, it appears that the first
solution is the better. It is much shorter and is easily understood by many people with
virtually no knowledge of mathematics. But are there considerations that might judge
the second solution to be the better one?</p></blockquote>


<p>
 He then discusses whether the second one is better because it generalises better,
when setting out to prove Gomory's theorem (which I've not been able to find a copy of the original of so far, but I haven't looked very hard):
If you remove two squares of the same colour, you can always tiling the remainder with dominoes.
The proof involves the construction of a hamiltonian circuit on the adjacency graph,
and seems fiddly but interesting.
I've only skimmed it and would like to digest it further.</p>


<p>
 However note that we saw a generalisation in a different direction in the first paper linked! Golomb's proof of the impossibility tiling with straight tetrominoes unless the monomino was in a very specific location was
 <em>
  also</em>
 a colouring argument.</p>


<p>
 The wikipedia page references "Across the board: the mathematics of chessboard problems" by John J. Watkins.
I should probably look up a copy.</p>


</section>
<section>
<h2><a href="/posts/2018-08-28-08:14.html">2018-08-28</a></h2>


<p class="subtitle">First!</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-28</dd>
</dl>


<p>
 This is an experimental new blog intended for notes, thoughts, and whatever else I want to put here.
It will likely be biased towards short notes rather than longform essays.
It's loosely inspired by
 <a href="https://shitpost.plover.com/">
  Mark Jason Dominus's shitposting blog</a>
 and by my frustrations with WordPress, but I'm not really sure where it's going yet.</p>


<p>
 It's also a place where I'll be experimenting with notation,
and generally trying to find a low friction way to express myself in a manner that I like.
As such it's all a bit cobbled together out of spit, bailing wire, and Python.</p>


<h3>
 Notational Highlights</h3>


<p>
 I kinda hate LaTeX, but it's the best typesetting language for mathematics that I know of,
so this notebook supports it using
 <a href="https://www.mathjax.org/">
  mathjax</a>.</p>


<p>
 Testing: \(e^{i\pi} = -1\)</p>


<p>
 A test of code highlighting.</p>


<div class="codehilite">
<pre><span></span><span class="k">class</span> <span class="nc">SomeClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">""""A python class"""</span>

    <span class="k">def</span> <span class="nf">method</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""A method definition"""</span></pre></div>


<p>
 As you've probably noticed, I'm using
 <a href="https://edwardtufte.github.io/tufte-css/">
  Tufte CSS</a>.
I'm not sure it's exactly what I want, but it's a lot closer to what I want than most other things I've tried.
I will likely be messing aroudn with this further.</p>


<p>
 I'm also using
 <a href="http://www.makotemplates.org">
  mako templates</a>
 ,
and fully intend to define a metric tonne of macros to make this usable.</p>


<p>
 In general I expect the actual source code for this site to be totally unusable to anyone who is not me.
If anything,
if it's
 <em>
  not</em>
 then I probably haven't done enough customization for my brain.</p>


</section>

    </article>
<footer>
Copyright David R. MacIver.

CSS mostly due to <a href="https://edwardtufte.github.io/tufte-css/">Tufte CSS</a> by Dave Liepmann.
</footer>
  </body>
</html>
