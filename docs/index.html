<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>
DRMacIver's Notebook: Thoughts from David R. MacIver
    </title>
    <link rel="stylesheet" href="/pygments.css"/>
    <link rel="stylesheet" href="/tufte.css"/>
    <link rel="stylesheet" href="/latex.css"/>
    <link rel="stylesheet" href="/drmnotes.css"/>
    <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/feed.xml" />

    <meta name="viewport" content="width=device-width, initial-scale=1">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(', '\\)']]},
  multiLine: true,
  "HTML-CSS": { 
       linebreaks: { automatic: true }
  },
  SVG: { 
       linebreaks: { automatic: true } 
  }
});
</script>

  </head>

  <body>
    <article>
        <h1><a href="/">DRMacIver's Notebook</a></h1>
        <p class=subtitle>Thoughts from David R. MacIver</p>

        

<section>
<h2><a href="/posts/2018-08-31-06:57.html">2018-08-31</a></h2>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-31</dd>
</dl>


<p>
 I'm a big fan of the
 <a href="https://en.wikipedia.org/wiki/Brzozowski_derivative">
  Brzozowski derivative</a>
 ,
introduced in "Derivatives of regular expressions" by Janusz A. Brzozowski.</p>


<p>
 The basic idea is that given some language \(L\) over an alphabet \(A\),
and some string \(u\) over \(L\),
you can define the derivative language \(\partial(L, u) = \{v: uv \in L\}\).
We can extend this further (and it will be useful to do so below).
If \(M\) is some other language, we can define \(\partial(L, M) = \{v: \exists u \in M, uv \in L\}\).</p>


<p>
 This seems like a pretty trivial observation until you realise the following three things:</p>


<ol>
<li>
  \(u \in L\) if and only if \(\epsilon \in \partial(L, u)\)</li>
<li>
  \(uv \in L\) if and only if \(v \in \partial(L, u)\)</li>
<li>
  For most common representations of languages, it's actually pretty easy to calculate a representation of their derivative.</li></ol>


<p>
 Putting these together, you can use the Brzozowski derivative to calculate a deterministic (not necessarily finite!) automaton for almost any language that you can easily represent.
You label states with descriptions of languages,
a state is accepting if it matches the empty string,
and transitions to the states labelled by the derivatives.</p>


<p>
<a href="http://www.ccs.neu.edu/home/turon/re-deriv.pdf">
  Regular-expression derivatives reexamined</a>
 by Owens et al. has some nice practical details of doing this in the context of functional programming.</p>


<p>
 To see this in action, consider the standard regular expression operators.
These satisfy the following identifies:</p>


<ol>
<li>
  \(\partial(A | B, u) = \partial(A, u) | \partial(B, u)\)</li>
<li>
  \(\partial(AB, u) = \partial(A, u)B | \nu(A) \partial(B, u)\), where \(\nu(A) = \epsilon\) if \(\epsilon \in A\) or \(\emptyset\) otherwise (i.e. the derivative can skip over \(A\) if and only if \(A\) contains the empty string)</li>
<li>
  \(\partial(A^*, u) = \partial(A, u) A^*\)</li></ol>


<p>
 A result proved in Brzozowski's original paper (apparently. I can't currently seem to access it, and am going off thecite in "Regular-expression derivatives reexamined) is that a small number of reasonable normalisation rules over the representation of the language is enough to ensure that you only get finitely many states in the state machine generated by partial derivatives of regular expressions.
It's certainly true that you only get finitely many if you have full equivalence for the regular languages labelling the states - the derivative automaton is actually the minimal automaton representing a language.</p>


<p>
 There are two very nice things about this representation of the language's automaton though:</p>


<ol>
<li>
  It can be done
  <em>
   lazily</em>. This means that even when your deterministic automaton has exponentially (or infinitely!) many states, you only ever need to explore the states that you walk when matching strings.</li>
<li>
  It is very easy to extend with new operators.</li></ol>


<p>
 An example of (2) is that regular expressions reexamined actually does it for extended regular expressions with intersection and negation, because might as well right? It's no harder than doing it with the normal ones, even though adding these to your regular expression language can cause exponential blowup in the size of the automata compiled from your regex.</p>


<p>
 But there are even more interesting ones if you're prepared to go for more esoteric operations!</p>


<p>
 Have you heard of the
 <a href="https://en.wikipedia.org/wiki/Levenshtein_automaton">
  Levenshtein automaton</a>
 ? The set of strings within some finite edit distance of another string is a regular language and you can define a nice automaton matching it.
But in fact, a stronger result is true: For any regular language \(L\) and natural number \(n\), the set \(E(L, n) = \{u: \exists v \in L, d(u, v) \leq n\}\) is a regular language.
Why?</p>


<p>
 Well, we can calculate its derivative!
The derivative of \(E\) is \(\partial(E(L, n), u) = E(\partial(L, u), n) | E(L, n - 1) | E(\partial(L, \cdot), n - 1) | \partial(E(\partial(L, \cdot), n - 1), u)\).
That is, at each character we can either:</p>


<ol>
<li>
  Continue matching the original language (cost 0).</li>
<li>
  Insert a new character in front of something in the original language (cost 1)</li>
<li>
  Replace a character in the original language with \(u\) (cost 1)</li>
<li>
  Drop a character from the original language and try again (cost 1)</li></ol>


<p>
 In the course of doing this we apply the following rewrite rules:</p>


<ol>
<li>
  \(E(L, 0) = L\)</li>
<li>
  \(E(\emptyset, n) = \emptyset\)</li></ol>


<p>
 As long as the number of reachable representations for the original languages is finite,
so is the number of reachable states in our Levenshtein construction:
Every state is labelled by some set of combinations of \(n\) and \(\partial(L, \cdot^k\)) for some \(0 \leq k \leq n\), so there are only finitely many.
In principle this may be exponentially bad in \(n\),
but because of the laziness of our construction that often won't matter - you can still determine membership for a string of length \(k\) with only \(O(k)\) state traversals (though calculating those states could in principle require up to \(O(nm)\) work, where \(m\) is the number of states in the original automaton).</p>


<p>
 You can also use this to determine the minimum edit distance between two regular languages,
because you can test whether \(E(L, n) \cap L' = \emptyset\) by calculating and walking the generated DFA for the left hand side,
so this gives you a decision procedure for \(d(L, L') \leq n\).</p>


<p>
 Is this a practical algorithm? Not sure. I've played with it a little bit, but I've not really put it to the test,
but I think it's an interesting example of the flexibility of the Brzozowski derivative,
and it was at least mildly surprising to me that the edit ball of a regular language is itself regular.</p>


</section>
<section>
<h2><a href="/posts/2018-08-30-12:39.html">2018-08-30</a></h2>


<p class="subtitle">Mathjax and Python Markdown</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-30</dd>
</dl>


<p>
 I've been having an interesting time of things with this notebook and getting Python markdown and Mathjax to play well with each other.
In particular I have not been enjoying the markdown extension API at
 <em>
  all</em>.</p>


<p>
 Anyway, it turns out that it is easy to do what I need, just slightly undocumented and with some annoyingly silent failure modes.</p>


<p>
 Here is the (slightly simplified) code from this notebook that makes MathJax work correctly:</p>


<div class="codehilite">
<pre><span></span><span class="kn">from</span> <span class="nn">markdown.inlinepatterns</span> <span class="kn">import</span> <span class="n">HtmlPattern</span>

<span class="n">LATEX_BLOCK</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">"(</span><span class="se">\\</span><span class="s2">begin{[^}]+}.+?</span><span class="se">\\</span><span class="s2">end{[^}]+})"</span>
<span class="n">LATEX_EXPR</span>  <span class="o">=</span> <span class="sa">r</span><span class="s2">"(</span><span class="se">\\</span><span class="s2">\(.+?</span><span class="se">\\</span><span class="s2">\))"</span>


<span class="k">class</span> <span class="nc">MathJaxAlignExtension</span><span class="p">(</span><span class="n">markdown</span><span class="o">.</span><span class="n">Extension</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">extendMarkdown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">md</span><span class="p">,</span> <span class="n">md_globals</span><span class="p">):</span>
        <span class="c1"># Needs to come before escape so that markdown doesn't break use of \ in LaTeX</span>
        <span class="n">md</span><span class="o">.</span><span class="n">inlinePatterns</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">'mathjaxblocks'</span><span class="p">,</span> <span class="n">HtmlPattern</span><span class="p">(</span><span class="n">LATEX_BLOCK</span><span class="p">,</span> <span class="n">md</span><span class="p">),</span> <span class="s1">'&lt;escape'</span><span class="p">)</span>
        <span class="n">md</span><span class="o">.</span><span class="n">inlinePatterns</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">'mathjaxexprs'</span><span class="p">,</span> <span class="n">HtmlPattern</span><span class="p">(</span><span class="n">LATEX_EXPR</span><span class="p">,</span> <span class="n">md</span><span class="p">),</span> <span class="s1">'&lt;escape'</span><span class="p">)</span></pre></div>


<p>
 The HtmlPattern class takes an expression and treats anything matching that expression as something that the markdown processor should not touch further.</p>


<p>
 Some caveats to note:</p>


<ul>
<li>
  Those brackets around the expression? Those are
  <em>
   important</em>. The way that the regular expression processing works is that it messes with your regex a bit, and then uses capturing group \(2\) as the output (\(1\) will be everything in the current block prior to the start of your regex). This means that if you must use groups in your regex, make them named groups.</li>
<li>
  For reasons I haven't fully understood and have chosen not to bother understanding because the current behaviour is correct for my needs, despite allegedly being an HTML block, this extension does seem to do entity escaping on the contents of your MathJax.</li></ul>


</section>
<section>
<h2><a href="/posts/2018-08-30-07:50.html">2018-08-30</a></h2>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-30</dd>
</dl>


<p>
 I'm going to start trying to port over some contents from
 <a href="https://github.com/DRMacIver/research-notebook">
  my research notebook</a>
 into here,
as this is intended long-term to be a replacement for it.
This will require some figuring out in terms of how to present maths.</p>


<p>
 As a starting point,
here's a theorem:</p>


<p>
 \(H(m) = \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \frac{1}{q}\)</p>


<p>
 Where \(H(m)\) is the m'th harmonic number \(H(m) = \sum\limits_{i}^m \frac{1}{i}\).</p>


<p>
 This came up in "Birthday Paradox, Coupon Collectors, Caching Algorithms and Self-Organizing Search" by Flajolet et al. (which is excellent) where it was stated as "well known". It wasn't well known to
 <em>
  me</em>
 ,
so I set out to prove it.</p>


<p>
 The following is my proof:</p>


<p>
 The main idea is to use a standard tricks of turning sums and integrals into other sums and integrals that happen to be easier to solve.
We use the following standard results:</p>


<ul>
<li>
  \(\frac{1}{n} = \int\limits_0^1 x^{n - 1}dx\)</li>
<li>
  \((1 + x)^m = \sum\limits_{q=1}^m {m \choose q} x^q\)</li>
<li>
  \((1 - x)^{-1} = \sum\limits_{q = 0}^\infty x^q\) for \(|x| &lt; 1\).</li></ul>


<p>
 We then perform the following manipulations (don't worry if some of these are clear as mud. They kinda should be):</p>


<p>
 \begin{align}
\sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \frac{1}{q} &amp;= \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \int\limits_0^1 x^{q - 1} dx\\
&amp;= \int\limits_0^1 \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} x^{q - 1} dx\\
&amp;= \int\limits_0^1 -x^{-1} \sum\limits_{q = 1}^m {m \choose q} {(-x)}^q dx\\
&amp;= \int\limits_0^1 -x^{-1} \left( \sum\limits_{q = 0}^m {m \choose q} {(-x)}^q - 1 \right)dx \\
&amp;= \int\limits_0^1 -x^{-1} \left( {(1 - x)}^m - 1 \right)dx \\
&amp;= \int\limits_0^1 {(1 - x)}^{-1} (x^m - 1) dx \\
&amp;= \int\limits_0^1 \sum\limits_{n = 0}^\infty x^n (x^m - 1) dx \\
&amp;= \sum\limits_{n = 0}^\infty \int\limits_0^1 x^n (x^m - 1) \\
&amp;= \sum\limits_{n = 0}^\infty \frac{1}{n + m} - \frac{1}{n} \\
&amp;= \lim\limits_{k \to \infty}  H(m) - \sum\limits_{n = k}^{m + k} \frac{1}{n + m}\\
&amp;= H(m)\\
\end{align}</p>


<p>
 Notable magic tricks performed:</p>


<ul>
<li>
  \(\int\limits_0^1 -x^{-1} \left( {(1 - x)}^m - 1 \right)dx  \to \int\limits_0^1 {(1 - x)}^{-1} (x^m - 1) dx \) is a change of variables \(x \to 1 - x\).</li>
<li>
  \(\sum\limits_{n = 0}^\infty \frac{1}{n + m} - \frac{1}{n} \to \lim\limits_{k \to \infty}  H(m) - \sum\limits_{n = k}^{m + k} \frac{1}{n + m}\) is because you can use a change of variables \(k \to k - m\),
and then group the terms that cancel out.</li>
<li>
  The final limit is because \(|\sum\limits_{n = k}^{m + k} \frac{1}{n + m}| \leq \frac{m}{k}\).</li></ul>


<p>
 This is a style of calculation I think of as the Feynmann style because
 <del>
  it's very good at seeming more clever than it actually is</del>
 he was fond of smugly boasting about using this sort of trick in preference to contour integration.
Given its prevalence prior to Feynmann, my only defence of the terminology is that it's not really intended as a compliment.</p>


<p>
 I find the Feynmann style completely unenlightening to read - the only way to read a Feynmann style proof is to do it yourself, using the original as a guide when you get stuck.</p>


<p>
 I think that's in some ways its point. It's not a proof technique designed to leverage enlightenment,
but instead it leans heavily on your puzzle solving skills. That can be useful sometimes when you just want to brute force your way through a problem and don't really care about understanding it on any sort of deeper level.</p>


<p>
 I was exposed to the Feynmann style quite early on,
due to reading Schaum's Outlines of Advanced Calculus (an earlier edition. I'm not sure how early. Brown covered one. I sadly gave away my copy, and the 1974 edition one I ordered doesn't seem to be quite it) prior to going to university.
It has quite a lot of exercises using calculations like this,
and afterwards I realised that this is what Feynmann had been talking about in "Surely you're joking, Mr Feynmann" (I didn't understand what a contour integral was until a few years later).</p>


<p>
 Somehow despite this the Feynmann style of brute force problem solving never really integrated into my mathematics,
and it's only some years later I've come to appreciate its merits.
I
 <em>
  still</em>
 prefer to achieve insight and make the problem trivial,
but sometimes the problem isn't worth the insight and you're better off just putting in the hard work and solving it.</p>


<p>
 Putting in the hard work is also useful because sometimes it leads you to the insight you missed and you can throw away most of the work.
This didn't happen here,
but I think that's OK - it's not that interesting a problem,
so I don't really feel upset by the lack of insight into it.</p>


</section>
<section>
<h2><a href="/posts/2018-08-29-09:35.html">2018-08-29</a></h2>


<p class="subtitle">Notes on tiling with polyominoes</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-29</dd>
</dl>


<p>
 Gary Fredericks wrote about
 <a href="https://gfredericks.com/gfrlog/99">
  a backtracking algorithm for tiling a board with polyominoes</a>.</p>


<p>
 His solution is roughly "turn the problem into exact cover and then apply a bunch of interesting optimisations in this context to the naive backtracking algorithm".
The paper
 <a href="https://arxiv.org/pdf/cs/0011047.pdf">
  Dancing Links</a>
 by Donald E. Knuth in fact studies this exact problem as an application of the exact cover algorithm.</p>


<p>
 I think some of the optimisations Gary performs are not ones that would be performed by a modern SAT solver because they are actually too expensive to be worth it if you're good at the SAT problem-e.g.
I know modern SAT solvers tend not to bother decomposing problems into independent problems because the cost is too high-but
it's possible they synergise well enough to be worth it. e.g. the number theory optimisation combined with the independent components may well be worth it,
especially with the heuristic of prioritising moves that disconnect the board.</p>


<p>
 I've been doing a bit of casual reading about this class of problem recently.
I thought I'd use the opportunity of this new notebook to collect some references.
Ideally these would be proper cites,
but I haven't got the citation part of the notebook system working yet.</p>


<p>
<a href="https://www.jstor.org/stable/pdf/2307321.pdf">
  Checker Boards and Polyominoes</a>
 by Solomon W. Golomb is a classic here.
It looks at the question of tiling the chessboard with a single square monomino and 11 tetrominos of various shapes.
In particular it establishes:</p>


<ul>
<li>
  You can do this with right tetrominoes given any placement of the monomino</li>
<li>
  There are only four squares where you can place the monomino if you want to do it with straight tetrominoes.</li></ul>


<p>
<a href="http://chalkdustmagazine.com/blog/polyominoes/">
  How to Tile a Chessboard</a>
 by Trupti Patel is a nice expository piece on this.</p>


<p>
 Golomb also wrote
 <a href="http://publisher-connector.core.ac.uk/resourcesync/data/elsevier/pdf/03f/aHR0cDovL2FwaS5lbHNldmllci5jb20vY29udGVudC9hcnRpY2xlL3BpaS9zMDAyMTk4MDA2NjgwMDMzOQ%3D%3D.pdf">
  Tiling with Polyominoes</a>
 ,
studying much more general questions of how to tile truncated chessboards with polyominoes.</p>


<p>
 A classic version of this is what
 <a href="https://en.wikipedia.org/wiki/Mutilated_chessboard_problem">
  Wikipedia refers to as the mutilated chessboard problem</a>
 (apparently following Max Black):</p>


<blockquote>
<p>
  Suppose a standard 8×8 chessboard has two diagonally opposite corners removed, leaving 62 squares. Is it possible to place 31 dominoes of size 2×1 so as to cover all of these squares?</p></blockquote>


<p>
 The answer is no. In
 <a href="https://www.tandfonline.com/doi/pdf/10.1080/07468342.2004.11922062">
  Tiling with Dominoes</a>
 , N. S. Mendelsohn discusses two proofs:</p>


<blockquote>
<h3>
  First solution</h3>
<p>
  From the checkerboard diagram, the region contains 30 black cells and 32 white cells.
Since each domino covers 1 black and 1 white cell, tiling is impossible.</p>
<h3>
  Second solution</h3>
<p>
  When I was first shown the problem many years ago, it did not occur to me to colour
the cells. The region itself had seven cells in the top and bottom rows and eight cells in
the remaining rows. The same held for the columns. I proceeded to obtain information
on how many dominoes pointed horizontally and how many vertically. The first count
dealt with the vertical dominoes. If the region is tiled, the horizontal dominoes in the
top row occupies an even number of cells. Hence, the cells in the top row that are not
occupied by horizontal dominoes are odd in number. Thus there are an odd number of
vertical dominoes between the first and second rows. Since the second row has eight
cells, and an odd number are occupied by vertical dominoes coming down from the
first row, there remain an odd number of cells in the second row. The same argument
now shows there is an odd number of vertical dominoes from the second row to the
third. Continuing this way, we see that there is an odd number of vertical dominoes
between any pair of consecutive rows. Hence the total number of vertical dominoes is
the sum of seven odd numbers, which is odd. In the same way, using columns instead
of rows, there is an odd number of horizontal dominoes. Hence the total number of
dominoes is even. Since there are 62 cells to cover, the number of dominoes required
is 31, an odd number. Therefore, tiling is impossible.</p></blockquote>


<p>
 He goes on to say:</p>


<blockquote>
<p>
  Why do I produce two solutions to the puzzle? It is because I am interested in
the question of which is the better solution. At first glance, it appears that the first
solution is the better. It is much shorter and is easily understood by many people with
virtually no knowledge of mathematics. But are there considerations that might judge
the second solution to be the better one?</p></blockquote>


<p>
 He then discusses whether the second one is better because it generalises better,
when setting out to prove Gomory's theorem (which I've not been able to find a copy of the original of so far, but I haven't looked very hard):
If you remove two squares of the same colour, you can always tiling the remainder with dominoes.
The proof involves the construction of a hamiltonian circuit on the adjacency graph,
and seems fiddly but interesting.
I've only skimmed it and would like to digest it further.</p>


<p>
 However note that we saw a generalisation in a different direction in the first paper linked! Golomb's proof of the impossibility tiling with straight tetrominoes unless the monomino was in a very specific location was
 <em>
  also</em>
 a colouring argument.</p>


<p>
 The wikipedia page references "Across the board: the mathematics of chessboard problems" by John J. Watkins.
I should probably look up a copy.</p>


</section>
<section>
<h2><a href="/posts/2018-08-28-08:14.html">2018-08-28</a></h2>


<p class="subtitle">First!</p>


<dl class="metadata">
<dt>Published</dt>
<dd class="post-date">2018-08-28</dd>
</dl>


<p>
 This is an experimental new blog intended for notes, thoughts, and whatever else I want to put here.
It will likely be biased towards short notes rather than longform essays.
It's loosely inspired by
 <a href="https://shitpost.plover.com/">
  Mark Jason Dominus's shitposting blog</a>
 and by my frustrations with WordPress, but I'm not really sure where it's going yet.</p>


<p>
 It's also a place where I'll be experimenting with notation,
and generally trying to find a low friction way to express myself in a manner that I like.
As such it's all a bit cobbled together out of spit, bailing wire, and Python.</p>


<h3>
 Notational Highlights</h3>


<p>
 I kinda hate LaTeX, but it's the best typesetting language for mathematics that I know of,
so this notebook supports it using
 <a href="https://www.mathjax.org/">
  mathjax</a>.</p>


<p>
 Testing: \(e^{i\pi} = -1\)</p>


<p>
 A test of code highlighting.</p>


<div class="codehilite">
<pre><span></span><span class="k">class</span> <span class="nc">SomeClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">""""A python class"""</span>

    <span class="k">def</span> <span class="nf">method</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""A method definition"""</span></pre></div>


<p>
 As you've probably noticed, I'm using
 <a href="https://edwardtufte.github.io/tufte-css/">
  Tufte CSS</a>.
I'm not sure it's exactly what I want, but it's a lot closer to what I want than most other things I've tried.
I will likely be messing aroudn with this further.</p>


<p>
 I'm also using
 <a href="http://www.makotemplates.org">
  mako templates</a>
 ,
and fully intend to define a metric tonne of macros to make this usable.</p>


<p>
 In general I expect the actual source code for this site to be totally unusable to anyone who is not me.
If anything,
if it's
 <em>
  not</em>
 then I probably haven't done enough customization for my brain.</p>


</section>

    </article>
<footer>
Copyright David R. MacIver.

CSS mostly due to <a href="https://edwardtufte.github.io/tufte-css/">Tufte CSS</a> by Dave Liepmann.
</footer>
  </body>
</html>
